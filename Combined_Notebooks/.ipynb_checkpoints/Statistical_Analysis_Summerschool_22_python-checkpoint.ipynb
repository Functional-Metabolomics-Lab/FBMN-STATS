{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Ax4lNbqxcB77",
   "metadata": {
    "id": "Ax4lNbqxcB77"
   },
   "source": [
    "# Performing basic uni- and multivariate statistical analsysis of untargeted metabolomics data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55766981",
   "metadata": {
    "id": "55766981"
   },
   "source": [
    "**Updated on:** 2023-06-20 17:33 CEST\n",
    "\n",
    "In this Jupyter Notebook we perform basic explorative uni- and multivariate statistical analyses of an untargeted metabolomics data set, including data cleaning steps, normalization and batch correction.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Authors**: Abzer Kelminal (abzer.shah@uni-tuebingen.de), Francesco Russo (frru@ssi.dk), Filip Ottosson (faot@ssi.dk), Madaleine Ernst (maet@ssi.dk), Axel Walter (axel.walter@uni-tuebingen.de), Carolina Gonzalez (cgonzalez7@eafit.edu.co), Efi Kontou (eeko@biosustain.dtu.dk), Judith Boldt (judith.boldt@dsmz.de) <br>\n",
    "\n",
    "**Input file format**: .csv files or .txt files <br>\n",
    "**Outputs**: .csv files, .pdf & .svg images  <br>\n",
    "**Dependencies**: pandas, numpy, glob, os, itertools, plotly, matplotlib, scipy, sklearn, pingouin, skbio, ipyfilechooser, ipywidgets, pynmranalysis\n",
    "\n",
    "The session info at the end of this notebook gives info about the versions of all the packages used here.\n",
    "</div>\n",
    "\n",
    "---\n",
    "This Notebook can be run with both Jupyter Notebook & Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3YbAYLwjdumx",
   "metadata": {
    "id": "3YbAYLwjdumx"
   },
   "source": [
    "---\n",
    "\n",
    "<b> Before starting to run this notebook with your own data, remember to save a copy of this notebook in your own Google Drive! Do so by clicking on File &rarr; Save a copy in Drive. You can give whatever meaningful name to your notebook.\n",
    "This file should be located in a new folder of your Google Drive named 'Colab Notebooks'. You can also download this notebook: File &rarr; Download &rarr; Download .ipynb.</b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KT7zBlpIdvUm",
   "metadata": {
    "id": "KT7zBlpIdvUm"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b><font size=3> SPECIAL NOTE: Please read the comments before proceeding with the code and let us know if you run into any errors and if you think it could be commented better. We would highly appreciate your suggestions and comments!!</font> </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0520cb",
   "metadata": {
    "id": "2e0520cb"
   },
   "source": [
    "# <font color ='blue'> 1. Introduction </font>\n",
    "<a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8zlElKRwhzja",
   "metadata": {
    "id": "8zlElKRwhzja"
   },
   "source": [
    "<p style='text-align: justify;'> The example files used in this tutorial are part of a study published by <a href=\"https://doi.org/10.1016/j.chemosphere.2020.129450\">Petras and coworkers (2021)</a>. Here, the authors investigated the coastal environments in northern San Diego, USA, after a major rainfall event in winter 2017/2018 to observe the seawater chemotype. The dataset contains surface seawater samples collected (−10 cm) at 30 sites spaced approximately 300 meters apart and 50–100 meters offshore along the San Diego coastline from Torrey Pines State Beach to Mission Bay (Torrey Pines, La Jolla Shores, La Jolla Reefs, Pacific and Mission Beach) at 3 different time points: Dec 2017, Jan 2018 (After a major rainfall, resulting in decreased salinity of water) and Oct 2018. As a result of the study, a huge shift was observed in the seawater's organic matter chemotype after the rainfall. <br>\n",
    "\n",
    "<p style='text-align: justify;'> Seawater samples collected during October 2018, were not published in the original article, but are added to this tutorial to have increased sample size. The datasets used here can be found in the MassIVE repository: <a href=\"https://massive.ucsd.edu/ProteoSAFe/dataset.jsp?task=8a8139d9248b43e0b0fda17495387756\">MSV000082312</a> and <a href=\"https://massive.ucsd.edu/ProteoSAFe/dataset.jsp?task=c8411b76f30a4f4ca5d3e42ec13998dc\">MSV000085786</a>. The .mzml files\n",
    "were preprocessed using <a href=\"http://mzmine.github.io/\">MZmine3</a> and the <a href=\"https://gnps.ucsd.edu/ProteoSAFe/status.jsp?task=cf6e14abf5604f47b28b467a513d3532\">feature-based molecular networking workflow in GNPS</a>.</p>\n",
    "\n",
    "<p style='text-align: justify;'> We initially clean the feature table by batch correction, blank removal, imputation, normalization, and scaling. Then, we perform univariate and multivariate statistical analyses including unsupervised learning methods (e.g., PCoA and clustering) and supervised classification analysis (Random Forest). Each step is discussed in detail in its own sections.</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75972046",
   "metadata": {
    "id": "75972046"
   },
   "source": [
    "# <font color ='blue'> 2. Preliminary setup for the notebook </font>\n",
    "<a id='Section-2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb66a2-d37d-4a3e-8863-f1312fe15e6d",
   "metadata": {},
   "source": [
    "## Package installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecfa6b",
   "metadata": {
    "id": "47ecfa6b",
    "tags": []
   },
   "source": [
    "### <font color ='darkblue'> Step 1: Installing and loading necessary packages </font>\n",
    "<a id = 'pkg_install'></a>\n",
    "Before we start, we need to install all packages, which we need for our analyses and load the installed packages into our session. Since we have many packages for different sections, we install the packages right before the respective sections to reduce the installation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "NQnoUTaU1s9_",
   "metadata": {
    "id": "NQnoUTaU1s9_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (1.24.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (5.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (1.2.2)\n",
      "Collecting pingouin\n",
      "  Using cached pingouin-0.5.3-py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: kaleido in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (0.2.1)\n",
      "Collecting ipyfilechooser\n",
      "  Using cached ipyfilechooser-0.6.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: nbformat in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (5.7.3)\n",
      "Collecting pynmranalysis\n",
      "  Using cached pynmranalysis-1.1.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: session_info in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting seaborn>=0.11\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Collecting statsmodels>=0.13\n",
      "  Using cached statsmodels-0.14.0-cp39-cp39-win_amd64.whl (9.4 MB)\n",
      "Requirement already satisfied: tabulate in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from pingouin) (0.9.0)\n",
      "Collecting outdated\n",
      "  Using cached outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting matplotlib>=3.0.2\n",
      "  Using cached matplotlib-3.7.2-cp39-cp39-win_amd64.whl (7.5 MB)\n",
      "Collecting pandas-flavor>=0.2.0\n",
      "  Using cached pandas_flavor-0.6.0-py3-none-any.whl (7.2 kB)\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.0-py3-none-any.whl (139 kB)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from nbformat) (5.2.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from nbformat) (5.9.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from nbformat) (4.17.3)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from nbformat) (2.16.3)\n",
      "Requirement already satisfied: stdlib-list in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from session_info) (0.9.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.19.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from jsonschema>=2.6->nbformat) (22.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (23.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (4.41.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (10.0.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (1.1.0)\n",
      "Collecting xarray\n",
      "  Using cached xarray-2023.7.0-py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from statsmodels>=0.13->pingouin) (0.5.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipywidgets->ipyfilechooser) (0.1.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipywidgets->ipyfilechooser) (3.0.8)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipywidgets->ipyfilechooser) (4.0.8)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipywidgets->ipyfilechooser) (8.11.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from jupyter-core->nbformat) (304)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from jupyter-core->nbformat) (3.1.0)\n",
      "Requirement already satisfied: setuptools>=44 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from outdated->pingouin) (67.5.1)\n",
      "Requirement already satisfied: littleutils in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from outdated->pingouin) (0.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from outdated->pingouin) (2.28.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.2->pingouin) (3.15.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser) (0.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser) (0.4.6)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser) (0.18.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser) (2.14.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser) (3.0.38)\n",
      "Requirement already satisfied: stack-data in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser) (0.6.2)\n",
      "Requirement already satisfied: decorator in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->ipyfilechooser) (0.7.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from requests->outdated->pingouin) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from requests->outdated->pingouin) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from requests->outdated->pingouin) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from requests->outdated->pingouin) (2.1.1)\n",
      "  Using cached xarray-2023.6.0-py3-none-any.whl (999 kB)\n",
      "  Using cached xarray-2023.5.0-py3-none-any.whl (994 kB)\n",
      "  Using cached xarray-2023.4.2-py3-none-any.whl (979 kB)\n",
      "  Using cached xarray-2023.4.1-py3-none-any.whl (977 kB)\n",
      "  Using cached xarray-2023.4.0-py3-none-any.whl (977 kB)\n",
      "  Using cached xarray-2023.3.0-py3-none-any.whl (981 kB)\n",
      "  Using cached xarray-2023.2.0-py3-none-any.whl (975 kB)\n",
      "  Using cached xarray-2023.1.0-py3-none-any.whl (973 kB)\n",
      "  Using cached xarray-2022.12.0-py3-none-any.whl (969 kB)\n",
      "  Using cached xarray-2022.11.0-py3-none-any.whl (963 kB)\n",
      "  Using cached xarray-2022.10.0-py3-none-any.whl (947 kB)\n",
      "  Using cached xarray-2022.9.0-py3-none-any.whl (943 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->ipyfilechooser) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets->ipyfilechooser) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ipyfilechooser) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ipyfilechooser) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\abzer\\anaconda3\\envs\\r-environment\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ipyfilechooser) (0.2.2)\n",
      "Installing collected packages: outdated, matplotlib, xarray, statsmodels, seaborn, pynmranalysis, pandas-flavor, ipywidgets, pingouin, ipyfilechooser\n",
      "Successfully installed ipyfilechooser-0.6.0 ipywidgets-8.1.0 matplotlib-3.7.2 outdated-0.2.2 pandas-flavor-0.6.0 pingouin-0.5.3 pynmranalysis-1.1.3 seaborn-0.12.2 statsmodels-0.14.0 xarray-2022.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install libraries that are not preinstalled\n",
    "!pip install pandas numpy plotly scikit-learn pingouin kaleido ipyfilechooser nbformat pynmranalysis session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dda4bb-f6e1-4b5f-9127-16c6c57e770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-bio # Don't import on Windows!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba4c7a",
   "metadata": {
    "id": "47ba4c7a"
   },
   "source": [
    "<font color=\"green\"><b>TIP:</b> # operator refers to comments describing the code function. Codes beginning with # is \"commented out\" and it will not run. To run a commented out code, remove the # and run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Z4eG3cNzmX-x",
   "metadata": {
    "id": "Z4eG3cNzmX-x"
   },
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "import pingouin as pg\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact\n",
    "import warnings\n",
    "from pynmranalysis.normalization import PQN_normalization\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import session_info\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b22ef6-a181-47ba-9294-07cea88aee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skbio # Don't import on Windows!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9d79f3-5b59-4d57-8a82-c59b92c69193",
   "metadata": {
    "id": "5d9d79f3-5b59-4d57-8a82-c59b92c69193"
   },
   "outputs": [],
   "source": [
    "# Disable warnings for cleaner output, comment out for debugging\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a44d4",
   "metadata": {
    "id": "863a44d4",
    "tags": []
   },
   "source": [
    "### <font color ='darkblue'> Step 2: Setting a local working directory </font>\n",
    "<a id = \"set_dir\"></a>\n",
    "\n",
    "<p style='text-align: justify;'> When we set a folder (or directory) as the working directory, we can access the files within the folder just by its names without mentioning the entire file path everytime we use it. Also, all the output files will be saved under the working directory. So, before proceeding with the script further, if you are trying to use your own files for the notebook, then please make sure to include all the necessary input files in one local folder and set it as your working directory. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WHg-QTTuiKTL",
   "metadata": {
    "id": "WHg-QTTuiKTL"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p style='text-align: justify;'> <b>NOTE:</b> When you run the next cell, it will display an output box where you can simply enter the path of the folder containing all your input files in your local computer.\n",
    "directory For example: D:\\User\\Project\\Test_Data.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5JH_tVisidOA",
   "metadata": {
    "id": "5JH_tVisidOA"
   },
   "source": [
    "It will be set as your working directory and you can access all the files within it. <b> Whenever you see an output box asking for user input, please note, the script will not proceed further without your input. Hence, make sure to run the notebook cell-by-cell instead of running all cells in the notebook simultaneously. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cy44oyq2iowr",
   "metadata": {
    "id": "cy44oyq2iowr"
   },
   "source": [
    "<p style='text-align: justify;'>In Google Colab homepage &rarr; there are 3 icons on the upper left corner. Click on the 3 dots to see the contents of the notebook. To create a folder with your input files, click on the folder icon &rarr; Right-click anywher on the empty space within the left section &rarr; Select 'new folder' &rarr; Copy the path and paste in the output box of next cell </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c4151fb",
   "metadata": {
    "id": "3c4151fb"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path of the folder with input files:\n",
      " C:\\Users\\abzer\\OneDrive\\Documents\\GitHub\\Statistical-analysis-of-non-targeted-LC-MSMS-data\\data\n"
     ]
    }
   ],
   "source": [
    "#enter the directory for the data files:\n",
    "data_dir = input(\"Enter the path of the folder with input files:\\n\")\n",
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48f99883",
   "metadata": {
    "id": "48f99883"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path of the folder for all output files:\n",
      " C:\\Users\\abzer\\OneDrive\\Documents\\GitHub\\Statistical-analysis-of-non-targeted-LC-MSMS-data\\outputs_Python_Notebook\n"
     ]
    }
   ],
   "source": [
    "#enter the directory for the results:\n",
    "result_dir = input(\"Enter the path of the folder for all output files:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h6-or5g-i1p9",
   "metadata": {
    "id": "h6-or5g-i1p9"
   },
   "source": [
    "---\n",
    "## Data import\n",
    "<a name='load_ip'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9546ef5-73f8-4623-877d-9c4daf3c18b9",
   "metadata": {
    "id": "e9546ef5-73f8-4623-877d-9c4daf3c18b9"
   },
   "source": [
    "<b><u>Files needed for running the Notebook:</b></u>  \n",
    "\n",
    "1) <b>Feature table:</b> A typical output file of an LC-MS/MS metabolomics experiment, containing all mass spectral features (or peaks) with their corresponding relative intensities across samples. The feature table we use in this tutorial was obtained from MZmine3. (Filetype: .csv file) <br> \n",
    "\n",
    "2) <b>Metadata:</b> An Excel file saved in .txt format that is created by the user, providing additional information about the samples (e.g. sample type, tissue type, species, timepoint of collection etc.) In this tutorial we are using the [metadata format recognized by GNPS workflows](https://ccms-ucsd.github.io/GNPSDocumentation/metadata/). The first column should be named 'filename' and all remaining column headers should be prefixed with ATTRIBUTE_: e.g. ATTRIBUTE_SampleType, ATTRIBUTE_timepoint etc. (Filetype: .txt file) <br>\n",
    "\n",
    "Feature table and metadata used in this tutorial can be accessed at <a href=\"https://github.com/Functional-Metabolomics-Lab/FBMN-STATS/tree/main/data\">our Functional Metabolomics Git Repository.</a>\n",
    "\n",
    "3) <b>OPTIONAL:</b> Annotation files such as GNPS library annotations, GNPS analog annotations, SIRIUS annotations.\n",
    "\n",
    "<b><u>More on Annotation files:</u></b>  \n",
    "<p style='text-align: justify;'> If available, provide the files for molecular annotations such as SIRIUS, CANOPUS and GNPS annotation files. SIRIUS and CANOPUS performs molecular formula prediction and chemical class predictions respectively. GNPS annotation files can be obtained by performing Feature-Based Molecular Networking (FBMN) analysis on the feature table (provided along with its corresponding metadata). <a href=\"https://gnps.ucsd.edu/ProteoSAFe/status.jsp?task=43cf48acbe24401e84a50ab2069b3d26\">The FBMN job is also publicly available.</a> The GNPS annotation files are a result of FBMN.</p>\n",
    "\n",
    "<b><u>To download the FBMN results locally:</u></b>  \n",
    "<p style='text-align: justify;'> Go to your <b>MassIVE</b> or <b>GNPS</b> account &rarr; Jobs &rarr; Click on <b>Status</b> of your FBMN Workflow &rarr; Download Cytoscape Data &rarr; A folder will be downloaded. For Ex: \"ProteoSAFe-FEATURE-BASED-MOLECULAR-NETWORKING-5877234d-download_cytoscape_data\" &rarr; Unzip this folder to access several sub-folders containing files uploaded for FBMN, such as the feature table, metadata table, and graphml file for visualizing the molecular network (found under the folder \"gnps_molecular_network_graphml\"). \n",
    "<br/> FBMN also offers the option to annotate compounds through GNPS spectral library search or an advanced analog search. These annotated files can be found in the sub-folders <b>\"DB_result\"</b> and <b>\"DB_analog_result\"</b> (if analog search is performed). During analog search, the method searches for structurally related molecules within the molecular network using a score threshold, such as a minimum cosine score that MS/MS spectra should achieve in spectral matching with MS/MS spectral libraries to be considered an annotation. In the given example, GNPS annotation annotated 255 compounds, while analog search annotated 1987 compounds. However, it is important to assess the analog annotations by looking at the cosine score and mirror match accuracy. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4Esll1J2jr3T",
   "metadata": {
    "id": "4Esll1J2jr3T"
   },
   "source": [
    "[![More on MZmine](https://img.shields.io/badge/More%20on-MZmine-blue)](https://www.nature.com/articles/s41587-023-01690-2)\n",
    "[![More on GNPS](https://img.shields.io/badge/More%20on-GNPS-informational)](https://www.nature.com/articles/nbt.3597#Abs2)\n",
    "[![More on FBMN](https://img.shields.io/badge/More%20on-FBMN-blue)](https://www.nature.com/articles/s41592-020-0933-6)\n",
    "[![More on SIRIUS](https://img.shields.io/badge/More%20on-SIRIUS-blue)](https://boecker-lab.github.io/docs.sirius.github.io/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4eeee8-1213-4495-a15c-e150dec84883",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'> Step 3: Uploading Files to Google Colab </font>\n",
    "\n",
    "To upload files into Google Colab &rarr; Right-click on the folder you created to 'upload' the necessary files from your computer into the cloud session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31eafc",
   "metadata": {
    "id": "3a31eafc"
   },
   "source": [
    "### <font color ='darkblue'> Step 4 to 6: Data Loading (Choose one method) </font>\n",
    "We can load the data files into the script either from the local working directory or from the web using url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db304ae",
   "metadata": {
    "id": "3db304ae"
   },
   "source": [
    "#### Step 4: Loading files from a local folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZ3oupp4jytJ",
   "metadata": {
    "id": "qZ3oupp4jytJ"
   },
   "source": [
    "Please make sure to include all the necessary input files in the folder you have set as working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d6cd026-e266-447b-9af9-a7b33d2dc40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 ] 20221102_SD_BeachSurvey_batchFile.xml\n",
      "[ 1 ] 20221125_Metadata_SD_Beaches_with_injection_order.txt\n",
      "[ 2 ] GNPS_analog_result_FBMN.tsv\n",
      "[ 3 ] GNPS_result_FBMN.tsv\n",
      "[ 4 ] SD_BeachSurvey_GapFilled_quant.csv\n"
     ]
    }
   ],
   "source": [
    "# List all the files in the current working directory\n",
    "file_names = os.listdir('.')\n",
    "\n",
    "# Loop through file_names and print each filename\n",
    "for i, filename in enumerate(file_names):\n",
    "    print(\"[\", i, \"]\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b6752-4690-4f34-95cf-1a788b1cf862",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> Here, we are importing: the feature table (<b>ft</b>), the metadata table (<b>md</b>), and the annotation tables (<b>an</b>). In our example dataset, we are loading in both gnps and analog results from FBMN as our annotation files. When importing each file, it is important to ensure that the separator used is specified correctly. By default, the pd.read_csv function uses a comma (',') as the separator. However, if you are importing files with different separators, you will need to change the separator parameter accordingly.<br/>\n",
    "<font color=\"red\"> For your own dataset, change the file numbers in the next cell accordingly. </font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8f9d9f8-51a4-489e-bf67-ad028365bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = pd.read_csv(filenames[4])\n",
    "md = pd.read_csv(filenames[0], sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d52d1224-498e-4690-8de3-ea89a65b3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GNPS Annotations:\n",
    "an_gnps = pd.read_csv(filenames[3], sep = \"\\t\") #comment the line if no annotation file is used\n",
    "an_analog = pd.read_csv(filenames[2], sep = \"\\t\") #comment the line if no annotation file is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51775f-585a-4fd1-baa4-8ea2dbb9e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sirius <- pd.read_csv(file_names[3], sep = '\\t') #comment the line if no annotation file is used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c8f1e-7398-4bb6-8d93-c8c9da6dca6b",
   "metadata": {},
   "source": [
    "Once the files are loaded, you can view the files as shown in [step 7](#view_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503bc3e2",
   "metadata": {
    "id": "503bc3e2"
   },
   "source": [
    "#### Step 5: Loading files using URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5p3czbO_kAnv",
   "metadata": {
    "id": "5p3czbO_kAnv"
   },
   "source": [
    "<p style='text-align: justify;'> In this section, we provide an example of how to retrieve data from a URL. Here, we are accessing the same feature table, metadata, and analog result files directly from our Functional Metabolomics GitHub page and importing them into R. If you are working with your own dataset in a Google Colab environment, you can obtain the file URL by first loading the input files into the Colab workspace, right-clicking on the file, selecting \"Copy path\", and then replacing the URL in the subsequent cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7988ffd6",
   "metadata": {
    "id": "7988ffd6"
   },
   "outputs": [],
   "source": [
    "#Reading the input data using URL\n",
    "ft_url = 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/FBMN-STATS/main/data/SD_BeachSurvey_GapFilled_quant.csv'\n",
    "md_url = 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/FBMN-STATS/main/data/20221125_Metadata_SD_Beaches_with_injection_order.txt'\n",
    "an_gnps_url = 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/FBMN-STATS/main/data/DB_result_FBMN.tsv'\n",
    "an_analog_url = 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/FBMN-STATS/main/data/DB_analog_result_FBMN.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca5f2d",
   "metadata": {
    "id": "9fca5f2d"
   },
   "source": [
    "#### Step 6: Loading files directly from GNPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iZUmavTekHsK",
   "metadata": {
    "id": "iZUmavTekHsK"
   },
   "source": [
    "One can also load the files directly from the repositories [MassIVE](https://massive.ucsd.edu/ProteoSAFe/static/massive.jsp) or [GNPS](https://gnps.ucsd.edu/ProteoSAFe/static/gnps-splash.jsp). If one has performed FBMN on their feature table, the files (both, input and output files from FBMN) can be accessed by  providing the task ID in the next cell. Task ID can be found by:  Go to your <b>MassIVE</b> or <b>GNPS</b> account &rarr; Jobs &rarr; unique ID is provided for each job in  'Description' column.\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr><th>Description</th><th>User</th><th>Workflow</th><th>Workflow Version</th><th>Status</th><th>Protected</th><th>Create Time</th><th>Total Size</th><th>Site</th><th>Delete Task</th></tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td><font color=\"red\">ID given here</font></td><td>-</td><td>FBMN</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>GNPS</td><td>-</td></tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eff82d04",
   "metadata": {
    "id": "eff82d04"
   },
   "outputs": [],
   "source": [
    "taskID = \"43cf48acbe24401e84a50ab2069b3d26\" # Enter the task ID here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1137ffc5",
   "metadata": {
    "id": "1137ffc5"
   },
   "outputs": [],
   "source": [
    "ft_url = os.path.join('https://proteomics2.ucsd.edu/ProteoSAFe/DownloadResultFile?task='+taskID+'&file=quantification_table_reformatted/&block=main')\n",
    "md_url = os.path.join('https://proteomics2.ucsd.edu/ProteoSAFe/DownloadResultFile?task='+taskID+'&file=metadata_merged/&block=main')\n",
    "an_gnps_url = os.path.join('https://proteomics2.ucsd.edu/ProteoSAFe/DownloadResultFile?task='+taskID+'&file=DB_result/&block=main')\n",
    "an_analog_url = os.path.join('https://proteomics2.ucsd.edu/ProteoSAFe/DownloadResultFile?task='+taskID+'&file=DB_analogresult/&block=main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45b5bb",
   "metadata": {
    "id": "5b45b5bb"
   },
   "source": [
    "<font color = \"purple\"><b> ▲ CRITICAL: </b></font>Make sure your metadata has enough columns (ATTRIBUTES) describing your data. The metadata given for FBMN might contain only few columns, however for downstream statistical analysis, one might need more attributes. In such cases, load the metadata file from a local folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a37c1c",
   "metadata": {
    "id": "12a37c1c"
   },
   "source": [
    "#### Reading the url from steps 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bad01c55",
   "metadata": {
    "id": "bad01c55"
   },
   "outputs": [],
   "source": [
    "ft = pd.read_csv(ft_url)\n",
    "md = pd.read_csv(md_url, sep = \"\\t\")\n",
    "an_gnps = pd.read_csv(an_gnps_url, sep = \"\\t\")\n",
    "an_analog = pd.read_csv(an_analog_url, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a8ffc",
   "metadata": {
    "id": "670a8ffc"
   },
   "source": [
    "### <font color ='darkblue'>  Step 7: Exploring the Imported Files </font>\n",
    "<a id='view_ip'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JZcdgLslkTlY",
   "metadata": {
    "id": "JZcdgLslkTlY"
   },
   "source": [
    "Lets check how the data looks, the below lines of code show the first 5 rows of the feature and metadata tables as well as their dimensions (numbers of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6gemopL1lV45",
   "metadata": {
    "id": "6gemopL1lV45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:  (11217, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row ID</th>\n",
       "      <th>row m/z</th>\n",
       "      <th>row retention time</th>\n",
       "      <th>row ion mobility</th>\n",
       "      <th>row ion mobility unit</th>\n",
       "      <th>row CCS</th>\n",
       "      <th>correlation group ID</th>\n",
       "      <th>annotation network number</th>\n",
       "      <th>best ion</th>\n",
       "      <th>auto MS2 verify</th>\n",
       "      <th>...</th>\n",
       "      <th>SD_12-2017_15_b.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_15_a.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_27_a.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_29_b.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_21_a.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_30_a.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_28_b.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_29_a.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_28_a.mzXML Peak area</th>\n",
       "      <th>Unnamed: 199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92572</td>\n",
       "      <td>151.035101</td>\n",
       "      <td>13.363672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21385.480</td>\n",
       "      <td>1.138271e+03</td>\n",
       "      <td>1144.8115</td>\n",
       "      <td>12139.16</td>\n",
       "      <td>5.394689e+03</td>\n",
       "      <td>5.270766e+03</td>\n",
       "      <td>1.007839e+03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2513</td>\n",
       "      <td>151.035125</td>\n",
       "      <td>1.129901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27123.893</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>151.035140</td>\n",
       "      <td>0.550724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>212.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1150350.0</td>\n",
       "      <td>1103477.9</td>\n",
       "      <td>2638109.200</td>\n",
       "      <td>1.446267e+06</td>\n",
       "      <td>595216.5000</td>\n",
       "      <td>1225695.20</td>\n",
       "      <td>1.424855e+06</td>\n",
       "      <td>1.557217e+06</td>\n",
       "      <td>1.797692e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1870</td>\n",
       "      <td>151.035199</td>\n",
       "      <td>0.886780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>314371.840</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2127</td>\n",
       "      <td>151.096405</td>\n",
       "      <td>0.986017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   row ID     row m/z  row retention time  row ion mobility  \\\n",
       "0   92572  151.035101           13.363672               NaN   \n",
       "1    2513  151.035125            1.129901               NaN   \n",
       "2      42  151.035140            0.550724               NaN   \n",
       "3    1870  151.035199            0.886780               NaN   \n",
       "4    2127  151.096405            0.986017               NaN   \n",
       "\n",
       "   row ion mobility unit  row CCS  correlation group ID  \\\n",
       "0                    NaN      NaN                   NaN   \n",
       "1                    NaN      NaN                   NaN   \n",
       "2                    NaN      NaN                 212.0   \n",
       "3                    NaN      NaN                   NaN   \n",
       "4                    NaN      NaN                   NaN   \n",
       "\n",
       "   annotation network number best ion  auto MS2 verify  ...  \\\n",
       "0                        NaN      NaN              NaN  ...   \n",
       "1                        NaN      NaN              NaN  ...   \n",
       "2                        NaN      NaN              NaN  ...   \n",
       "3                        NaN      NaN              NaN  ...   \n",
       "4                        NaN      NaN              NaN  ...   \n",
       "\n",
       "   SD_12-2017_15_b.mzXML Peak area SD_12-2017_15_a.mzXML Peak area  \\\n",
       "0                              0.0                             0.0   \n",
       "1                              0.0                             0.0   \n",
       "2                        1150350.0                       1103477.9   \n",
       "3                              0.0                             0.0   \n",
       "4                              0.0                             0.0   \n",
       "\n",
       "   SD_12-2017_27_a.mzXML Peak area  SD_12-2017_29_b.mzXML Peak area  \\\n",
       "0                        21385.480                     1.138271e+03   \n",
       "1                        27123.893                     0.000000e+00   \n",
       "2                      2638109.200                     1.446267e+06   \n",
       "3                       314371.840                     0.000000e+00   \n",
       "4                            0.000                     0.000000e+00   \n",
       "\n",
       "   SD_12-2017_21_a.mzXML Peak area  SD_12-2017_30_a.mzXML Peak area  \\\n",
       "0                        1144.8115                         12139.16   \n",
       "1                           0.0000                             0.00   \n",
       "2                      595216.5000                       1225695.20   \n",
       "3                           0.0000                             0.00   \n",
       "4                           0.0000                             0.00   \n",
       "\n",
       "   SD_12-2017_28_b.mzXML Peak area  SD_12-2017_29_a.mzXML Peak area  \\\n",
       "0                     5.394689e+03                     5.270766e+03   \n",
       "1                     0.000000e+00                     0.000000e+00   \n",
       "2                     1.424855e+06                     1.557217e+06   \n",
       "3                     0.000000e+00                     0.000000e+00   \n",
       "4                     0.000000e+00                     0.000000e+00   \n",
       "\n",
       "   SD_12-2017_28_a.mzXML Peak area  Unnamed: 199  \n",
       "0                     1.007839e+03           NaN  \n",
       "1                     0.000000e+00           NaN  \n",
       "2                     1.797692e+06           NaN  \n",
       "3                     0.000000e+00           NaN  \n",
       "4                     0.000000e+00           NaN  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dimension: ',ft.shape) #gets the dimension (number of rows and columns) of ft\n",
    "ft.head() # gets the first 5 rows of ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8pl29xJVlncw",
   "metadata": {
    "id": "8pl29xJVlncw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:  (186, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>ATTRIBUTE_Sample.Type</th>\n",
       "      <th>ATTRIBUTE_Batch</th>\n",
       "      <th>ATTRIBUTE_Month</th>\n",
       "      <th>ATTRIBUTE_Year</th>\n",
       "      <th>ATTRIBUTE_Sample_Location</th>\n",
       "      <th>ATTRIBUTE_Replicate</th>\n",
       "      <th>ATTRIBUTE_Spot</th>\n",
       "      <th>ATTRIBUTE_Latitude</th>\n",
       "      <th>ATTRIBUTE_Longitude</th>\n",
       "      <th>ATTRIBUTE_Sample_Area</th>\n",
       "      <th>ATTRIBUTE_Spot_Name</th>\n",
       "      <th>ATTRIBUTE_time_run</th>\n",
       "      <th>ATTRIBUTE_Injection_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SD_10_2018_10_a.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>a</td>\n",
       "      <td>10</td>\n",
       "      <td>32.86261</td>\n",
       "      <td>-117.26042</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>SIO_South_Pier</td>\n",
       "      <td>18/07/2020 18:19</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SD_10_2018_10_b.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>b</td>\n",
       "      <td>10</td>\n",
       "      <td>32.86261</td>\n",
       "      <td>-117.26042</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>SIO_South_Pier</td>\n",
       "      <td>18/07/2020 18:35</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SD_10_2018_11_a.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>a</td>\n",
       "      <td>11</td>\n",
       "      <td>32.85601</td>\n",
       "      <td>-117.26253</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>La_Jolla_Shores</td>\n",
       "      <td>18/07/2020 18:51</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SD_10_2018_11_b.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>b</td>\n",
       "      <td>11</td>\n",
       "      <td>32.85601</td>\n",
       "      <td>-117.26253</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>La_Jolla_Shores</td>\n",
       "      <td>18/07/2020 19:07</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SD_10_2018_12_a.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>a</td>\n",
       "      <td>12</td>\n",
       "      <td>32.85161</td>\n",
       "      <td>-117.26965</td>\n",
       "      <td>La_Jolla_Cove</td>\n",
       "      <td>Cove</td>\n",
       "      <td>18/07/2020 19:23</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename ATTRIBUTE_Sample.Type  ATTRIBUTE_Batch  \\\n",
       "0  SD_10_2018_10_a.mzXML                Sample                3   \n",
       "1  SD_10_2018_10_b.mzXML                Sample                3   \n",
       "2  SD_10_2018_11_a.mzXML                Sample                3   \n",
       "3  SD_10_2018_11_b.mzXML                Sample                3   \n",
       "4  SD_10_2018_12_a.mzXML                Sample                3   \n",
       "\n",
       "  ATTRIBUTE_Month  ATTRIBUTE_Year  ATTRIBUTE_Sample_Location  \\\n",
       "0             Oct            2018                         10   \n",
       "1             Oct            2018                         10   \n",
       "2             Oct            2018                         11   \n",
       "3             Oct            2018                         11   \n",
       "4             Oct            2018                         12   \n",
       "\n",
       "  ATTRIBUTE_Replicate  ATTRIBUTE_Spot  ATTRIBUTE_Latitude  \\\n",
       "0                   a              10            32.86261   \n",
       "1                   b              10            32.86261   \n",
       "2                   a              11            32.85601   \n",
       "3                   b              11            32.85601   \n",
       "4                   a              12            32.85161   \n",
       "\n",
       "   ATTRIBUTE_Longitude ATTRIBUTE_Sample_Area ATTRIBUTE_Spot_Name  \\\n",
       "0           -117.26042   SIO_La_Jolla_Shores      SIO_South_Pier   \n",
       "1           -117.26042   SIO_La_Jolla_Shores      SIO_South_Pier   \n",
       "2           -117.26253   SIO_La_Jolla_Shores     La_Jolla_Shores   \n",
       "3           -117.26253   SIO_La_Jolla_Shores     La_Jolla_Shores   \n",
       "4           -117.26965         La_Jolla_Cove                Cove   \n",
       "\n",
       "  ATTRIBUTE_time_run  ATTRIBUTE_Injection_order  \n",
       "0   18/07/2020 18:19                        145  \n",
       "1   18/07/2020 18:35                        146  \n",
       "2   18/07/2020 18:51                        147  \n",
       "3   18/07/2020 19:07                        148  \n",
       "4   18/07/2020 19:23                        149  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dimension: ',md.shape)\n",
    "md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b71c62a5",
   "metadata": {
    "id": "b71c62a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpectrumID</th>\n",
       "      <th>Compound_Name</th>\n",
       "      <th>Ion_Source</th>\n",
       "      <th>Instrument</th>\n",
       "      <th>Compound_Source</th>\n",
       "      <th>PI</th>\n",
       "      <th>Data_Collector</th>\n",
       "      <th>Adduct</th>\n",
       "      <th>Precursor_MZ</th>\n",
       "      <th>ExactMass</th>\n",
       "      <th>...</th>\n",
       "      <th>MoleculeExplorerDatasets</th>\n",
       "      <th>MoleculeExplorerFiles</th>\n",
       "      <th>InChIKey</th>\n",
       "      <th>InChIKey-Planar</th>\n",
       "      <th>superclass</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>npclassifier_superclass</th>\n",
       "      <th>npclassifier_class</th>\n",
       "      <th>npclassifier_pathway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCMSLIB00005466086</td>\n",
       "      <td>Irgarol</td>\n",
       "      <td>LC-ESI</td>\n",
       "      <td>Orbitrap</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Daniel Petras</td>\n",
       "      <td>Daniel Petras</td>\n",
       "      <td>M+H</td>\n",
       "      <td>254.143</td>\n",
       "      <td>254.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HDHLIWCXDDZUFH-UHFFFAOYSA-N</td>\n",
       "      <td>HDHLIWCXDDZUFH</td>\n",
       "      <td>Organoheterocyclic compounds</td>\n",
       "      <td>Triazines</td>\n",
       "      <td>1,3,5-triazines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alkaloids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCMSLIB00003136915</td>\n",
       "      <td>Spectral Match to N-Lauroylsarcosine from NIST14</td>\n",
       "      <td>ESI</td>\n",
       "      <td>QqQ</td>\n",
       "      <td>Isolated</td>\n",
       "      <td>Data from Nediljko Budisa</td>\n",
       "      <td>Data deposited by daniel</td>\n",
       "      <td>M+H</td>\n",
       "      <td>272.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>3116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SpectrumID                                     Compound_Name  \\\n",
       "0  CCMSLIB00005466086                                           Irgarol   \n",
       "1  CCMSLIB00003136915  Spectral Match to N-Lauroylsarcosine from NIST14   \n",
       "\n",
       "  Ion_Source Instrument Compound_Source                         PI  \\\n",
       "0     LC-ESI   Orbitrap      Commercial              Daniel Petras   \n",
       "1        ESI        QqQ        Isolated  Data from Nediljko Budisa   \n",
       "\n",
       "             Data_Collector Adduct  Precursor_MZ  ExactMass  ...  \\\n",
       "0             Daniel Petras    M+H       254.143    254.143  ...   \n",
       "1  Data deposited by daniel    M+H       272.222      0.000  ...   \n",
       "\n",
       "   MoleculeExplorerDatasets MoleculeExplorerFiles  \\\n",
       "0                         0                     0   \n",
       "1                        60                  3116   \n",
       "\n",
       "                      InChIKey InChIKey-Planar                    superclass  \\\n",
       "0  HDHLIWCXDDZUFH-UHFFFAOYSA-N  HDHLIWCXDDZUFH  Organoheterocyclic compounds   \n",
       "1                          NaN             NaN                           NaN   \n",
       "\n",
       "       class         subclass npclassifier_superclass npclassifier_class  \\\n",
       "0  Triazines  1,3,5-triazines                     NaN                NaN   \n",
       "1        NaN              NaN                     NaN                NaN   \n",
       "\n",
       "  npclassifier_pathway  \n",
       "0            Alkaloids  \n",
       "1                  NaN  \n",
       "\n",
       "[2 rows x 46 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_gnps.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3c2b8fbb-7b96-472c-9745-3d70b941b501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpectrumID</th>\n",
       "      <th>Compound_Name</th>\n",
       "      <th>Ion_Source</th>\n",
       "      <th>Instrument</th>\n",
       "      <th>Compound_Source</th>\n",
       "      <th>PI</th>\n",
       "      <th>Data_Collector</th>\n",
       "      <th>Adduct</th>\n",
       "      <th>Precursor_MZ</th>\n",
       "      <th>ExactMass</th>\n",
       "      <th>...</th>\n",
       "      <th>MoleculeExplorerDatasets</th>\n",
       "      <th>MoleculeExplorerFiles</th>\n",
       "      <th>InChIKey</th>\n",
       "      <th>InChIKey-Planar</th>\n",
       "      <th>superclass</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>npclassifier_superclass</th>\n",
       "      <th>npclassifier_class</th>\n",
       "      <th>npclassifier_pathway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCMSLIB00005466086</td>\n",
       "      <td>Irgarol</td>\n",
       "      <td>LC-ESI</td>\n",
       "      <td>Orbitrap</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Daniel Petras</td>\n",
       "      <td>Daniel Petras</td>\n",
       "      <td>M+H</td>\n",
       "      <td>254.143</td>\n",
       "      <td>254.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HDHLIWCXDDZUFH-UHFFFAOYSA-N</td>\n",
       "      <td>HDHLIWCXDDZUFH</td>\n",
       "      <td>Organoheterocyclic compounds</td>\n",
       "      <td>Triazines</td>\n",
       "      <td>1,3,5-triazines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alkaloids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCMSLIB00003136915</td>\n",
       "      <td>Spectral Match to N-Lauroylsarcosine from NIST14</td>\n",
       "      <td>ESI</td>\n",
       "      <td>QqQ</td>\n",
       "      <td>Isolated</td>\n",
       "      <td>Data from Nediljko Budisa</td>\n",
       "      <td>Data deposited by daniel</td>\n",
       "      <td>M+H</td>\n",
       "      <td>272.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>3116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SpectrumID                                     Compound_Name  \\\n",
       "0  CCMSLIB00005466086                                           Irgarol   \n",
       "1  CCMSLIB00003136915  Spectral Match to N-Lauroylsarcosine from NIST14   \n",
       "\n",
       "  Ion_Source Instrument Compound_Source                         PI  \\\n",
       "0     LC-ESI   Orbitrap      Commercial              Daniel Petras   \n",
       "1        ESI        QqQ        Isolated  Data from Nediljko Budisa   \n",
       "\n",
       "             Data_Collector Adduct  Precursor_MZ  ExactMass  ...  \\\n",
       "0             Daniel Petras    M+H       254.143    254.143  ...   \n",
       "1  Data deposited by daniel    M+H       272.222      0.000  ...   \n",
       "\n",
       "   MoleculeExplorerDatasets MoleculeExplorerFiles  \\\n",
       "0                         0                     0   \n",
       "1                        60                  3116   \n",
       "\n",
       "                      InChIKey InChIKey-Planar                    superclass  \\\n",
       "0  HDHLIWCXDDZUFH-UHFFFAOYSA-N  HDHLIWCXDDZUFH  Organoheterocyclic compounds   \n",
       "1                          NaN             NaN                           NaN   \n",
       "\n",
       "       class         subclass npclassifier_superclass npclassifier_class  \\\n",
       "0  Triazines  1,3,5-triazines                     NaN                NaN   \n",
       "1        NaN              NaN                     NaN                NaN   \n",
       "\n",
       "  npclassifier_pathway  \n",
       "0            Alkaloids  \n",
       "1                  NaN  \n",
       "\n",
       "[2 rows x 46 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_analog.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "745b9628-f97d-4baa-9ad0-4ce7bfcc3695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:  (255, 46)\n",
      "Dimension:  (1987, 46)\n"
     ]
    }
   ],
   "source": [
    "print('Dimension: ', an_gnps.shape)\n",
    "print('Dimension: ', an_analog.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda797c-bf9e-43e8-87d8-f7f8840859fd",
   "metadata": {},
   "source": [
    "Out of the 11217 features, we found 255 hits using GNPS library and around 2000 hits using analog search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5af06-1073-414a-b306-c9f5f6bae852",
   "metadata": {
    "id": "7da5af06-1073-414a-b306-c9f5f6bae852"
   },
   "source": [
    "---\n",
    "#### Summarizing the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q6omtph4krb9",
   "metadata": {
    "id": "q6omtph4krb9"
   },
   "source": [
    "<p style='text-align: justify;'>Before starting with our analysis, we take a look at our metadata. For this purpose, we have created a function. A function is a collection of commands, which takes one or multiple input variables and creates a corresponding output. By creating functions, we avoid having to write big code chunks multiple times. Instead, we can call a sequence of code lines by their function name.</p>\n",
    "    \n",
    "<p style='text-align: justify;'><font color=\"red\"> The following cell will not produce any outputs. </font> The outputs will only be produced when we call the function further downstream and give it an input variable. To explore our metadata we define a function called InsideLevels. This function creates a summary table of our metadata, including data types and levels contained in each column.  <font color =\"blue\"> The input is a metadata table and the output consists of a summary dataframe. </font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "Rd1O2VJ1nM1i",
   "metadata": {
    "id": "Rd1O2VJ1nM1i"
   },
   "outputs": [],
   "source": [
    "def inside_levels(df):\n",
    "    # get all the columns (equals all attributes) -> will be number of rows\n",
    "    levels = []\n",
    "    types = []\n",
    "    count = []\n",
    "    for col in df.columns:\n",
    "        types.append(type(df[col][0]))\n",
    "        levels.append(sorted(set(df[col].dropna())))\n",
    "        tmp = df[col].value_counts()\n",
    "        count.append([tmp[levels[-1][i]] for i in range(len(levels[-1]))])\n",
    "    return pd.DataFrame({\"ATTRIBUTES\": df.columns, \"LEVELS\": levels, \"COUNT\":count, \"TYPES\": types}, index=range(1, len(levels)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4502e-3f83-4c02-b5d1-cd661ee1fec1",
   "metadata": {
    "id": "8dc4502e-3f83-4c02-b5d1-cd661ee1fec1"
   },
   "source": [
    "Let's have a look at our metadata, with the above defined function InsideLevels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c4419307",
   "metadata": {
    "id": "c4419307"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "535d67d6-3e74-4eec-885c-06c434381110",
   "metadata": {
    "id": "535d67d6-3e74-4eec-885c-06c434381110"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATTRIBUTES</th>\n",
       "      <th>LEVELS</th>\n",
       "      <th>COUNT</th>\n",
       "      <th>TYPES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>filename</td>\n",
       "      <td>[SD_01-2018_10_a.mzXML, SD_01-2018_10_b.mzXML,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATTRIBUTE_Sample.Type</td>\n",
       "      <td>[Blank, Sample]</td>\n",
       "      <td>[6, 180]</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATTRIBUTE_Batch</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[62, 62, 62]</td>\n",
       "      <td>&lt;class 'numpy.int64'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATTRIBUTE_Month</td>\n",
       "      <td>[Dec, Jan, Oct]</td>\n",
       "      <td>[62, 62, 62]</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ATTRIBUTE_Year</td>\n",
       "      <td>[2017, 2018]</td>\n",
       "      <td>[62, 124]</td>\n",
       "      <td>&lt;class 'numpy.int64'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ATTRIBUTE_Sample_Location</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>&lt;class 'numpy.int64'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ATTRIBUTE_Replicate</td>\n",
       "      <td>[a, b]</td>\n",
       "      <td>[93, 93]</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ATTRIBUTE_Spot</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>&lt;class 'numpy.int64'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ATTRIBUTE_Latitude</td>\n",
       "      <td>[32.75645, 32.75743, 32.75905, 32.76115, 32.76...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>&lt;class 'numpy.float64'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ATTRIBUTE_Longitude</td>\n",
       "      <td>[-117.2872, -117.28664, -117.286, -117.28355, ...</td>\n",
       "      <td>[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...</td>\n",
       "      <td>&lt;class 'numpy.float64'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ATTRIBUTE_Sample_Area</td>\n",
       "      <td>[Blank, La_Jolla Reefs, La_Jolla_Cove, Mission...</td>\n",
       "      <td>[6, 36, 12, 36, 18, 12, 18, 48]</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ATTRIBUTE_Spot_Name</td>\n",
       "      <td>[Big_Rock, Blacks_North_Peak, Blacks_South_Pea...</td>\n",
       "      <td>[6, 6, 6, 6, 12, 6, 6, 6, 6, 6, 6, 12, 18, 6, ...</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ATTRIBUTE_time_run</td>\n",
       "      <td>[11/12/2017 12:12, 11/12/2017 12:28, 11/12/201...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>&lt;class 'str'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ATTRIBUTE_Injection_order</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>&lt;class 'numpy.int64'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ATTRIBUTES  \\\n",
       "1                    filename   \n",
       "2       ATTRIBUTE_Sample.Type   \n",
       "3             ATTRIBUTE_Batch   \n",
       "4             ATTRIBUTE_Month   \n",
       "5              ATTRIBUTE_Year   \n",
       "6   ATTRIBUTE_Sample_Location   \n",
       "7         ATTRIBUTE_Replicate   \n",
       "8              ATTRIBUTE_Spot   \n",
       "9          ATTRIBUTE_Latitude   \n",
       "10        ATTRIBUTE_Longitude   \n",
       "11      ATTRIBUTE_Sample_Area   \n",
       "12        ATTRIBUTE_Spot_Name   \n",
       "13         ATTRIBUTE_time_run   \n",
       "14  ATTRIBUTE_Injection_order   \n",
       "\n",
       "                                               LEVELS  \\\n",
       "1   [SD_01-2018_10_a.mzXML, SD_01-2018_10_b.mzXML,...   \n",
       "2                                     [Blank, Sample]   \n",
       "3                                           [1, 2, 3]   \n",
       "4                                     [Dec, Jan, Oct]   \n",
       "5                                        [2017, 2018]   \n",
       "6   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "7                                              [a, b]   \n",
       "8   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "9   [32.75645, 32.75743, 32.75905, 32.76115, 32.76...   \n",
       "10  [-117.2872, -117.28664, -117.286, -117.28355, ...   \n",
       "11  [Blank, La_Jolla Reefs, La_Jolla_Cove, Mission...   \n",
       "12  [Big_Rock, Blacks_North_Peak, Blacks_South_Pea...   \n",
       "13  [11/12/2017 12:12, 11/12/2017 12:28, 11/12/201...   \n",
       "14  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "\n",
       "                                                COUNT                    TYPES  \n",
       "1   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...            <class 'str'>  \n",
       "2                                            [6, 180]            <class 'str'>  \n",
       "3                                        [62, 62, 62]    <class 'numpy.int64'>  \n",
       "4                                        [62, 62, 62]            <class 'str'>  \n",
       "5                                           [62, 124]    <class 'numpy.int64'>  \n",
       "6   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...    <class 'numpy.int64'>  \n",
       "7                                            [93, 93]            <class 'str'>  \n",
       "8   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...    <class 'numpy.int64'>  \n",
       "9   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  <class 'numpy.float64'>  \n",
       "10  [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...  <class 'numpy.float64'>  \n",
       "11                    [6, 36, 12, 36, 18, 12, 18, 48]            <class 'str'>  \n",
       "12  [6, 6, 6, 6, 12, 6, 6, 6, 6, 6, 6, 12, 18, 6, ...            <class 'str'>  \n",
       "13  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...            <class 'str'>  \n",
       "14  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...    <class 'numpy.int64'>  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inside_levels(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3rHvKfUD9kAw",
   "metadata": {
    "id": "3rHvKfUD9kAw"
   },
   "source": [
    "<p style='text-align: justify;'> The above table is a summary of our metadata tabel. For example, the 1st row says that there are 2 different types of samples under 'ATTRIBUTE_Sample-Type', namely \"Blank\" and \"Sample\". The number of files corresponding to each of these categories is given in the COUNT column. For example, we have 6 files belonging to the \"Blank\" sample type and 180 files to the \"Sample\" sample type. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcf409-e945-480d-9501-a70da21454c9",
   "metadata": {
    "id": "0ebcf409-e945-480d-9501-a70da21454c9"
   },
   "source": [
    "---\n",
    "## Merging annotations with feature table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xkpVQUQhlA19",
   "metadata": {
    "id": "xkpVQUQhlA19"
   },
   "source": [
    "### <font color ='darkblue'>  Step 8: Identifying Appropriate Columns for Merging </font>\n",
    "    \n",
    "The first column in feature table: <b>row ID</b> (the unique ID for each detected feature) is given in different columns in different files: <br>\n",
    "* In DB result and DB analog result files, the row ID is given in the column: <b>#Scan#</b> ('Compound_Name' column has the annotation information)\n",
    "* For SIRIUS and CANOPUS summary files, the row ID of the feature table is given in the column <b>id</b>. A typical feature id would be: \"3_ProjectName_MZmine3_SIRIUS_1_16\", where the last number 16 representing the row ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100bc551-305e-4c00-a302-be3d9f07b922",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'> Step 9: Ensuring Data Compatibility </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JeBxbYzXlCj4",
   "metadata": {
    "id": "JeBxbYzXlCj4"
   },
   "source": [
    "<p style='text-align: justify;'> Annotating features in a feature table is useful for identifying metabolites that correspond to those features. This can help to understand the biological relevance of the features, thereby assisting in the interpretation of our metabolomics data. Here, we will show how to merge ft and an (analog annotation file) based on the above-mentioned columns. Before merging two dataframes based on certain columns, make sure that the classes of both columns are the same. Although the values are same, but if one column is numeric class and the other is of character class, this might cause unwanted error. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a87eaa19",
   "metadata": {
    "id": "a87eaa19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if both columns are of similar class\n",
    "ft[\"row ID\"].dtype== an_gnps[\"#Scan#\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "15e6f654-42a6-42fd-b502-41297bf12646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft[\"row ID\"].dtype== an_analog[\"#Scan#\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8cd0f-5a82-4c02-816b-78011a9b87fa",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'>  Step 10: Merging Annotations </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e2a52-4d72-49ec-9c68-3a4f4ca8b843",
   "metadata": {},
   "source": [
    "<b> Merge the annotations from GNPS: Actual library hits and analogs.</b> This is given in <b>'an_final'</b> dataframe. The compound names of 'an_gnps' and 'an_analog' are combined by a separator ';'. As a result, for each #Scan#, we have a single associated compound name. This result is given in <b>'an_final_single' dataframe.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "304c1a18-4be7-4597-b0e2-aa9b0ba298ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns of 'an_analog' with a prefix 'Analog' (excluding the '#Scan#' column)\n",
    "an_analog.columns = ['Analog_' + col if col != '#Scan#' else col for col in an_analog.columns]\n",
    "\n",
    "# Merge 'an_analog' with 'an_gnps' using a full join on the '#Scan#' column\n",
    "an_final = pd.merge(an_gnps, an_analog, on='#Scan#', how='outer')\n",
    "\n",
    "# Consolidate multiple annotations for a single '#Scan#' into one combined name\n",
    "def combine_names(row):\n",
    "    if row['Compound_Name'] == row['Analog_Compound_Name']:\n",
    "        return row['Compound_Name']\n",
    "    return ';'.join([str(row['Compound_Name']), str(row['Analog_Compound_Name'])])\n",
    "\n",
    "an_final_single = an_final.groupby('#Scan#').apply(lambda group: pd.Series({\n",
    "    'Combined_Name': combine_names(group.iloc[0])\n",
    "})).reset_index()\n",
    "\n",
    "# To get the DataFrame with that exact column name (without automatic renaming)\n",
    "an_final_single.columns = an_final_single.columns.str.replace('.', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a8f49a97-bddb-464c-a8df-a1a3c7b1d8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:  (1987, 91)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpectrumID</th>\n",
       "      <th>Compound_Name</th>\n",
       "      <th>Ion_Source</th>\n",
       "      <th>Instrument</th>\n",
       "      <th>Compound_Source</th>\n",
       "      <th>PI</th>\n",
       "      <th>Data_Collector</th>\n",
       "      <th>Adduct</th>\n",
       "      <th>Precursor_MZ</th>\n",
       "      <th>ExactMass</th>\n",
       "      <th>...</th>\n",
       "      <th>Analog_MoleculeExplorerDatasets</th>\n",
       "      <th>Analog_MoleculeExplorerFiles</th>\n",
       "      <th>Analog_InChIKey</th>\n",
       "      <th>Analog_InChIKey-Planar</th>\n",
       "      <th>Analog_superclass</th>\n",
       "      <th>Analog_class</th>\n",
       "      <th>Analog_subclass</th>\n",
       "      <th>Analog_npclassifier_superclass</th>\n",
       "      <th>Analog_npclassifier_class</th>\n",
       "      <th>Analog_npclassifier_pathway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCMSLIB00005466086</td>\n",
       "      <td>Irgarol</td>\n",
       "      <td>LC-ESI</td>\n",
       "      <td>Orbitrap</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>Daniel Petras</td>\n",
       "      <td>Daniel Petras</td>\n",
       "      <td>M+H</td>\n",
       "      <td>254.143</td>\n",
       "      <td>254.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HDHLIWCXDDZUFH-UHFFFAOYSA-N</td>\n",
       "      <td>HDHLIWCXDDZUFH</td>\n",
       "      <td>Organoheterocyclic compounds</td>\n",
       "      <td>Triazines</td>\n",
       "      <td>1,3,5-triazines</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alkaloids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCMSLIB00003136915</td>\n",
       "      <td>Spectral Match to N-Lauroylsarcosine from NIST14</td>\n",
       "      <td>ESI</td>\n",
       "      <td>QqQ</td>\n",
       "      <td>Isolated</td>\n",
       "      <td>Data from Nediljko Budisa</td>\n",
       "      <td>Data deposited by daniel</td>\n",
       "      <td>M+H</td>\n",
       "      <td>272.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>60</td>\n",
       "      <td>3116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SpectrumID                                     Compound_Name  \\\n",
       "0  CCMSLIB00005466086                                           Irgarol   \n",
       "1  CCMSLIB00003136915  Spectral Match to N-Lauroylsarcosine from NIST14   \n",
       "\n",
       "  Ion_Source Instrument Compound_Source                         PI  \\\n",
       "0     LC-ESI   Orbitrap      Commercial              Daniel Petras   \n",
       "1        ESI        QqQ        Isolated  Data from Nediljko Budisa   \n",
       "\n",
       "             Data_Collector Adduct  Precursor_MZ  ExactMass  ...  \\\n",
       "0             Daniel Petras    M+H       254.143    254.143  ...   \n",
       "1  Data deposited by daniel    M+H       272.222      0.000  ...   \n",
       "\n",
       "   Analog_MoleculeExplorerDatasets Analog_MoleculeExplorerFiles  \\\n",
       "0                                0                            0   \n",
       "1                               60                         3116   \n",
       "\n",
       "               Analog_InChIKey Analog_InChIKey-Planar  \\\n",
       "0  HDHLIWCXDDZUFH-UHFFFAOYSA-N         HDHLIWCXDDZUFH   \n",
       "1                          NaN                    NaN   \n",
       "\n",
       "              Analog_superclass Analog_class  Analog_subclass  \\\n",
       "0  Organoheterocyclic compounds    Triazines  1,3,5-triazines   \n",
       "1                           NaN          NaN              NaN   \n",
       "\n",
       "  Analog_npclassifier_superclass Analog_npclassifier_class  \\\n",
       "0                            NaN                       NaN   \n",
       "1                            NaN                       NaN   \n",
       "\n",
       "  Analog_npclassifier_pathway  \n",
       "0                   Alkaloids  \n",
       "1                         NaN  \n",
       "\n",
       "[2 rows x 91 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dimension: ', an_final.shape)\n",
    "an_final.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "df7fbae4-5228-48b3-b6d5-e694732dee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to CSV, including the index (row names) as a column\n",
    "an_final.to_csv(os.path.join(result_dir, 'GNPS_LibraryHits_AnalogHits_Merged.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cdccb99e-654e-4166-987b-179a87419c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Scan#</th>\n",
       "      <th>Combined_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>nan;Massbank:LU096701 4-Methylumbelliferone|7-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>nan;\"6-methoxypurine, oxamethane CollisionEner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>nan;4-((hydroxyimino)methyl)-1-methylhydroquin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83</td>\n",
       "      <td>nan;\"methyl 3-oxo-2-[(3,4,5-trimethoxyphenyl)m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>nan;\"3,4-Dihydroxymandelic acid CollisionEnerg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>92610</td>\n",
       "      <td>nan;4-((hydroxyimino)methyl)-1-methylhydroquin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>92636</td>\n",
       "      <td>nan;HARMANE CollisionEnergy:102040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>92638</td>\n",
       "      <td>\"methyl 3-oxo-2-[(3,4,5-trimethoxyphenyl)methy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>92649</td>\n",
       "      <td>nan;TRYPTOPHAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>92651</td>\n",
       "      <td>nan;\"6-methoxypurine, oxamethane CollisionEner...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1987 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      #Scan#                                      Combined_Name\n",
       "0          2  nan;Massbank:LU096701 4-Methylumbelliferone|7-...\n",
       "1         11  nan;\"6-methoxypurine, oxamethane CollisionEner...\n",
       "2         27  nan;4-((hydroxyimino)methyl)-1-methylhydroquin...\n",
       "3         83  nan;\"methyl 3-oxo-2-[(3,4,5-trimethoxyphenyl)m...\n",
       "4         94  nan;\"3,4-Dihydroxymandelic acid CollisionEnerg...\n",
       "...      ...                                                ...\n",
       "1982   92610  nan;4-((hydroxyimino)methyl)-1-methylhydroquin...\n",
       "1983   92636                 nan;HARMANE CollisionEnergy:102040\n",
       "1984   92638  \"methyl 3-oxo-2-[(3,4,5-trimethoxyphenyl)methy...\n",
       "1985   92649                                     nan;TRYPTOPHAN\n",
       "1986   92651  nan;\"6-methoxypurine, oxamethane CollisionEner...\n",
       "\n",
       "[1987 rows x 2 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an_final_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0edfdc-2855-4330-bc6f-6929a75df385",
   "metadata": {},
   "source": [
    "This annotation information is then merged with the feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c9146a82",
   "metadata": {
    "id": "c9146a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:  (11217, 202)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row ID</th>\n",
       "      <th>row m/z</th>\n",
       "      <th>row retention time</th>\n",
       "      <th>row ion mobility</th>\n",
       "      <th>row ion mobility unit</th>\n",
       "      <th>row CCS</th>\n",
       "      <th>correlation group ID</th>\n",
       "      <th>annotation network number</th>\n",
       "      <th>best ion</th>\n",
       "      <th>auto MS2 verify</th>\n",
       "      <th>...</th>\n",
       "      <th>SD_12-2017_27_a.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_29_b.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_21_a.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_30_a.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_28_b.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_29_a.mzXML Peak area</th>\n",
       "      <th>SD_12-2017_28_a.mzXML Peak area</th>\n",
       "      <th>Unnamed: 199</th>\n",
       "      <th>#Scan#</th>\n",
       "      <th>Combined_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>391.283813</td>\n",
       "      <td>0.299798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>161.095931</td>\n",
       "      <td>0.235215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nan;Massbank:LU096701 4-Methylumbelliferone|7-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>167.154202</td>\n",
       "      <td>0.474247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>158.961323</td>\n",
       "      <td>0.477149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>23437.6720</td>\n",
       "      <td>22848.000</td>\n",
       "      <td>13511.6570</td>\n",
       "      <td>24267.623</td>\n",
       "      <td>26252.643</td>\n",
       "      <td>23598.620</td>\n",
       "      <td>14783.2270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>172.956270</td>\n",
       "      <td>0.486601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2476.0715</td>\n",
       "      <td>15583.166</td>\n",
       "      <td>3745.3218</td>\n",
       "      <td>18710.676</td>\n",
       "      <td>19134.986</td>\n",
       "      <td>18255.406</td>\n",
       "      <td>4221.2554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>nan;\"6-methoxypurine, oxamethane CollisionEner...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   row ID     row m/z  row retention time  row ion mobility  \\\n",
       "0       1  391.283813            0.299798               NaN   \n",
       "1       2  161.095931            0.235215               NaN   \n",
       "2       4  167.154202            0.474247               NaN   \n",
       "3       5  158.961323            0.477149               NaN   \n",
       "4      11  172.956270            0.486601               NaN   \n",
       "\n",
       "   row ion mobility unit  row CCS  correlation group ID  \\\n",
       "0                    NaN      NaN                   NaN   \n",
       "1                    NaN      NaN                   NaN   \n",
       "2                    NaN      NaN                   NaN   \n",
       "3                    NaN      NaN                   NaN   \n",
       "4                    NaN      NaN                   NaN   \n",
       "\n",
       "   annotation network number best ion  auto MS2 verify  ...  \\\n",
       "0                        NaN      NaN              NaN  ...   \n",
       "1                        NaN      NaN              NaN  ...   \n",
       "2                        NaN      NaN              NaN  ...   \n",
       "3                        NaN      NaN              NaN  ...   \n",
       "4                        NaN      NaN              NaN  ...   \n",
       "\n",
       "   SD_12-2017_27_a.mzXML Peak area SD_12-2017_29_b.mzXML Peak area  \\\n",
       "0                           0.0000                           0.000   \n",
       "1                           0.0000                           0.000   \n",
       "2                           0.0000                           0.000   \n",
       "3                       23437.6720                       22848.000   \n",
       "4                        2476.0715                       15583.166   \n",
       "\n",
       "   SD_12-2017_21_a.mzXML Peak area  SD_12-2017_30_a.mzXML Peak area  \\\n",
       "0                           0.0000                            0.000   \n",
       "1                           0.0000                            0.000   \n",
       "2                           0.0000                            0.000   \n",
       "3                       13511.6570                        24267.623   \n",
       "4                        3745.3218                        18710.676   \n",
       "\n",
       "   SD_12-2017_28_b.mzXML Peak area  SD_12-2017_29_a.mzXML Peak area  \\\n",
       "0                            0.000                            0.000   \n",
       "1                            0.000                            0.000   \n",
       "2                            0.000                            0.000   \n",
       "3                        26252.643                        23598.620   \n",
       "4                        19134.986                        18255.406   \n",
       "\n",
       "   SD_12-2017_28_a.mzXML Peak area  Unnamed: 199  #Scan#  \\\n",
       "0                           0.0000           NaN     NaN   \n",
       "1                           0.0000           NaN     2.0   \n",
       "2                           0.0000           NaN     NaN   \n",
       "3                       14783.2270           NaN     NaN   \n",
       "4                        4221.2554           NaN    11.0   \n",
       "\n",
       "                                       Combined_Name  \n",
       "0                                                NaN  \n",
       "1  nan;Massbank:LU096701 4-Methylumbelliferone|7-...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  nan;\"6-methoxypurine, oxamethane CollisionEner...  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_an = ft.merge(an_final_single, left_on= \"row ID\",  how='left', right_on= \"#Scan#\", sort=True)\n",
    "print('Dimension: ', ft_an.shape)\n",
    "ft_an.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591b6e6-9a09-4e17-a202-bba228bfa019",
   "metadata": {},
   "source": [
    "<b> OPTIONAL: </b>In case, you have SIRIUS annotations as well, the following code cell can be used to get a final output 'merged_data' which contains all the annotations in one table. This can be used later for further data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64774a0-0e8c-4084-8a8c-ea202d379522",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirius['row ID'] = sirius['id'].apply(lambda x: int(re.search(\"(?<=_)[^_]+$\", x).group()))  #Extract 'row ID' from 'id' in 'sirius'\n",
    "sirius = sirius.drop(columns=['id']) # Drop 'id' column from 'sirius'\n",
    "\n",
    "# Rename columns in 'sirius' with a prefix 'SIRIUS_', except for 'row ID'\n",
    "sirius.columns = ['SIRIUS_' + col if col != 'row ID' else col for col in sirius.columns]\n",
    "\n",
    "an_final = an_final.rename(columns={'#Scan#': 'row ID'}) # Rename '#Scan#' column to 'row ID' in 'an_final'\n",
    "merged_data = pd.merge(an_final, sirius, on='row ID', how='outer') # Merge 'an_final' with 'sirius' on 'row ID'\n",
    "merged_data.to_csv(os.path.join(result_dir, 'Merged_Annotations_GNPS_SIRIUS.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4acec47-3171-444e-a551-a63ab87c7f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050440ad",
   "metadata": {
    "id": "050440ad"
   },
   "source": [
    "---\n",
    "## Arranging metadata and feature table in the same order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fd-qLr0XlUW6",
   "metadata": {
    "id": "Fd-qLr0XlUW6"
   },
   "source": [
    "<p style='text-align: justify;'> In the next cells, we bring feature table and metadata in the correct format such that the rownames of the metadata and column names of the feature table are the same. Filenames and the order of files need to correspond in both tables, as we will match metadata attributes to the feature table. In that way, both metadata and feature table, can easily be filtered. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b3266-dee4-411b-9f5b-66b00159ddaa",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'>  Step 11: Creating Backup Files </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "329c00d2-6477-4c85-ab16-710469ad7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_md = md.copy() #storing the files under different names to preserve the original files\n",
    "new_ft = ft.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240199b-bc5d-49f3-a5aa-fcb17c208a61",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'> Step 12: Cleaning the files </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "182a5570-867b-4941-9b7b-d9d4d3ef4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ft.columns = new_ft.columns.str.replace(' Peak area', '') # Removing \" Peak area\" extensions from the column names of new_ft\n",
    "new_ft = new_ft.sort_values(by='row ID') # Arranging the rows of new_ft by ascending order of \"row ID\"\n",
    "\n",
    "new_ft = new_ft.loc[:, new_ft.notna().sum() > 0] # Removing columns in new_ft where all values are NaN\n",
    "new_md = new_md.loc[:, new_md.notna().sum() > 0] # Removing columns in new_md where all values are NaN\n",
    "\n",
    "# Remove leading and trailing spaces from each column of new_md\n",
    "new_md = new_md.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa2f52-2ede-4e7a-a95a-3a5fcef5122f",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'> Step 13: Update the row names of feature table </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b74aed2a-d33a-4dc6-9e29-a57fc98200bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if \"ft_an\" exists and compare \"row ID\" columns if it does\n",
    "if 'ft_an' in globals():\n",
    "    comparison_result = (ft_an['row ID'].values == new_ft['row ID'].values).all()\n",
    "    print(comparison_result)  # Should print True if you have an annotation file\n",
    "\n",
    "# Changing the index (row names) of new_ft into the combined name as \"XID_mz_RT\":\n",
    "new_name = 'X' + new_ft['row ID'].astype(str) + '_' + new_ft['row m/z'].round(3).astype(str) + '_' + new_ft['row retention time'].round(3).astype(str)\n",
    "new_name_values = new_name.values\n",
    "\n",
    "if 'ft_an' in globals():\n",
    "    combined_name_ft = ft_an['Combined_Name'].astype(str).values\n",
    "    underscore_added = ['_' + x for x in combined_name_ft] #add a underscore prefix\n",
    "    new_name_values = np.core.defchararray.add(new_name_values.astype(str), underscore_added)\n",
    "\n",
    "# Set the new index and remove trailing underscore if present\n",
    "new_ft.index = new_name_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c9d13f86-dae4-44c0-9263-e44a2f5d4181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row ID</th>\n",
       "      <th>row m/z</th>\n",
       "      <th>row retention time</th>\n",
       "      <th>correlation group ID</th>\n",
       "      <th>annotation network number</th>\n",
       "      <th>best ion</th>\n",
       "      <th>identified by n=</th>\n",
       "      <th>partners</th>\n",
       "      <th>neutral M mass</th>\n",
       "      <th>SD_01-2018_5_b.mzXML</th>\n",
       "      <th>...</th>\n",
       "      <th>SD_12-2017_23_b.mzXML</th>\n",
       "      <th>SD_12-2017_15_b.mzXML</th>\n",
       "      <th>SD_12-2017_15_a.mzXML</th>\n",
       "      <th>SD_12-2017_27_a.mzXML</th>\n",
       "      <th>SD_12-2017_29_b.mzXML</th>\n",
       "      <th>SD_12-2017_21_a.mzXML</th>\n",
       "      <th>SD_12-2017_30_a.mzXML</th>\n",
       "      <th>SD_12-2017_28_b.mzXML</th>\n",
       "      <th>SD_12-2017_29_a.mzXML</th>\n",
       "      <th>SD_12-2017_28_a.mzXML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X1_391.284_0.3_nan</th>\n",
       "      <td>1</td>\n",
       "      <td>391.283813</td>\n",
       "      <td>0.299798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2_161.096_0.235_nan;Massbank:LU096701 4-Methylumbelliferone|7-hydroxy-4-methylchromen-2-one</th>\n",
       "      <td>2</td>\n",
       "      <td>161.095931</td>\n",
       "      <td>0.235215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    row ID     row m/z  \\\n",
       "X1_391.284_0.3_nan                                       1  391.283813   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...       2  161.095931   \n",
       "\n",
       "                                                    row retention time  \\\n",
       "X1_391.284_0.3_nan                                            0.299798   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...            0.235215   \n",
       "\n",
       "                                                    correlation group ID  \\\n",
       "X1_391.284_0.3_nan                                                   NaN   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   NaN   \n",
       "\n",
       "                                                    annotation network number  \\\n",
       "X1_391.284_0.3_nan                                                        NaN   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                        NaN   \n",
       "\n",
       "                                                   best ion  identified by n=  \\\n",
       "X1_391.284_0.3_nan                                      NaN               NaN   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...      NaN               NaN   \n",
       "\n",
       "                                                   partners  neutral M mass  \\\n",
       "X1_391.284_0.3_nan                                      NaN             NaN   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...      NaN             NaN   \n",
       "\n",
       "                                                    SD_01-2018_5_b.mzXML  ...  \\\n",
       "X1_391.284_0.3_nan                                                   0.0  ...   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0  ...   \n",
       "\n",
       "                                                    SD_12-2017_23_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_15_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_15_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_27_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_29_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_21_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_30_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_28_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_29_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_28_a.mzXML  \n",
       "X1_391.284_0.3_nan                                                    0.0  \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0  \n",
       "\n",
       "[2 rows x 195 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ft.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "52089688-29c8-471b-8d43-5b30ac495cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>ATTRIBUTE_Sample.Type</th>\n",
       "      <th>ATTRIBUTE_Batch</th>\n",
       "      <th>ATTRIBUTE_Month</th>\n",
       "      <th>ATTRIBUTE_Year</th>\n",
       "      <th>ATTRIBUTE_Sample_Location</th>\n",
       "      <th>ATTRIBUTE_Replicate</th>\n",
       "      <th>ATTRIBUTE_Spot</th>\n",
       "      <th>ATTRIBUTE_Latitude</th>\n",
       "      <th>ATTRIBUTE_Longitude</th>\n",
       "      <th>ATTRIBUTE_Sample_Area</th>\n",
       "      <th>ATTRIBUTE_Spot_Name</th>\n",
       "      <th>ATTRIBUTE_time_run</th>\n",
       "      <th>ATTRIBUTE_Injection_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SD_10_2018_10_a.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>a</td>\n",
       "      <td>10</td>\n",
       "      <td>32.86261</td>\n",
       "      <td>-117.26042</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>SIO_South_Pier</td>\n",
       "      <td>18/07/2020 18:19</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SD_10_2018_10_b.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>b</td>\n",
       "      <td>10</td>\n",
       "      <td>32.86261</td>\n",
       "      <td>-117.26042</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>SIO_South_Pier</td>\n",
       "      <td>18/07/2020 18:35</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename ATTRIBUTE_Sample.Type  ATTRIBUTE_Batch  \\\n",
       "0  SD_10_2018_10_a.mzXML                Sample                3   \n",
       "1  SD_10_2018_10_b.mzXML                Sample                3   \n",
       "\n",
       "  ATTRIBUTE_Month  ATTRIBUTE_Year  ATTRIBUTE_Sample_Location  \\\n",
       "0             Oct            2018                         10   \n",
       "1             Oct            2018                         10   \n",
       "\n",
       "  ATTRIBUTE_Replicate  ATTRIBUTE_Spot  ATTRIBUTE_Latitude  \\\n",
       "0                   a              10            32.86261   \n",
       "1                   b              10            32.86261   \n",
       "\n",
       "   ATTRIBUTE_Longitude ATTRIBUTE_Sample_Area ATTRIBUTE_Spot_Name  \\\n",
       "0           -117.26042   SIO_La_Jolla_Shores      SIO_South_Pier   \n",
       "1           -117.26042   SIO_La_Jolla_Shores      SIO_South_Pier   \n",
       "\n",
       "  ATTRIBUTE_time_run  ATTRIBUTE_Injection_order  \n",
       "0   18/07/2020 18:19                        145  \n",
       "1   18/07/2020 18:35                        146  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_md.head(n=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PJCjWcjhlwRR",
   "metadata": {
    "id": "PJCjWcjhlwRR"
   },
   "source": [
    "<p style='text-align: justify;'> After adding the annotation information, the feature table \"new_ft\" now includes the annotation in the row names of the features. This can be beneficial for downstream analysis, such as univariate statistics, where significant features can be easily identified if they are annotated. Similarly, in supervised analysis like Random Forest, the annotated compound driving the classification can be quickly assessed. This will be further highlighted in the respective univariate and multivariate sections. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2668d45-c7f6-48c9-802e-84f07ae11a44",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'> Step 14: Selecting Relevant Columns </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0bafca60-5c46-4b05-90bc-b585f7a546ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the columns with names containing 'mzXML' or 'mzML'\n",
    "new_ft = new_ft.loc[:, new_ft.columns.str.contains('\\\\.mzXML$|\\\\.mzML$')]\n",
    "\n",
    "# If either .mzXML or .mzML files are present, print a message\n",
    "if new_ft.columns.str.contains('\\\\.mzXML$').any() and new_ft.columns.str.contains('\\\\.mzML$').any():\n",
    "    print(\"Both .mzXML and .mzML file types are present in the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "63033047-5166-4082-adb9-d76fc48e9268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:  (11217, 186)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SD_01-2018_5_b.mzXML</th>\n",
       "      <th>SD_01-2018_7_b.mzXML</th>\n",
       "      <th>SD_01-2018_7_a.mzXML</th>\n",
       "      <th>SD_01-2018_3_b.mzXML</th>\n",
       "      <th>SD_01-2018_6_a.mzXML</th>\n",
       "      <th>SD_01-2018_8_a.mzXML</th>\n",
       "      <th>SD_01-2018_1_a.mzXML</th>\n",
       "      <th>SD_01-2018_2_b.mzXML</th>\n",
       "      <th>SD_01-2018_4_b.mzXML</th>\n",
       "      <th>SD_01-2018_2_a.mzXML</th>\n",
       "      <th>...</th>\n",
       "      <th>SD_12-2017_23_b.mzXML</th>\n",
       "      <th>SD_12-2017_15_b.mzXML</th>\n",
       "      <th>SD_12-2017_15_a.mzXML</th>\n",
       "      <th>SD_12-2017_27_a.mzXML</th>\n",
       "      <th>SD_12-2017_29_b.mzXML</th>\n",
       "      <th>SD_12-2017_21_a.mzXML</th>\n",
       "      <th>SD_12-2017_30_a.mzXML</th>\n",
       "      <th>SD_12-2017_28_b.mzXML</th>\n",
       "      <th>SD_12-2017_29_a.mzXML</th>\n",
       "      <th>SD_12-2017_28_a.mzXML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X1_391.284_0.3_nan</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2_161.096_0.235_nan;Massbank:LU096701 4-Methylumbelliferone|7-hydroxy-4-methylchromen-2-one</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    SD_01-2018_5_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                   0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0   \n",
       "\n",
       "                                                    SD_01-2018_7_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                   0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0   \n",
       "\n",
       "                                                    SD_01-2018_7_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                   0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0   \n",
       "\n",
       "                                                    SD_01-2018_3_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                   0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0   \n",
       "\n",
       "                                                    SD_01-2018_6_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                   0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0   \n",
       "\n",
       "                                                    SD_01-2018_8_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                   0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0   \n",
       "\n",
       "                                                    SD_01-2018_1_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                   0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0   \n",
       "\n",
       "                                                    SD_01-2018_2_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                   0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0   \n",
       "\n",
       "                                                    SD_01-2018_4_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                   0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0   \n",
       "\n",
       "                                                    SD_01-2018_2_a.mzXML  ...  \\\n",
       "X1_391.284_0.3_nan                                                   0.0  ...   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                   0.0  ...   \n",
       "\n",
       "                                                    SD_12-2017_23_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_15_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_15_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_27_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_29_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_21_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_30_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_28_b.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_29_a.mzXML  \\\n",
       "X1_391.284_0.3_nan                                                    0.0   \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0   \n",
       "\n",
       "                                                    SD_12-2017_28_a.mzXML  \n",
       "X1_391.284_0.3_nan                                                    0.0  \n",
       "X2_161.096_0.235_nan;Massbank:LU096701 4-Methyl...                    0.0  \n",
       "\n",
       "[2 rows x 186 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the files again to see if the above changes have been made:\n",
    "print('Dimension: ', new_ft.shape)\n",
    "new_ft.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "65837a66-c266-40cd-99d2-d89d7f959cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:  (186, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>ATTRIBUTE_Sample.Type</th>\n",
       "      <th>ATTRIBUTE_Batch</th>\n",
       "      <th>ATTRIBUTE_Month</th>\n",
       "      <th>ATTRIBUTE_Year</th>\n",
       "      <th>ATTRIBUTE_Sample_Location</th>\n",
       "      <th>ATTRIBUTE_Replicate</th>\n",
       "      <th>ATTRIBUTE_Spot</th>\n",
       "      <th>ATTRIBUTE_Latitude</th>\n",
       "      <th>ATTRIBUTE_Longitude</th>\n",
       "      <th>ATTRIBUTE_Sample_Area</th>\n",
       "      <th>ATTRIBUTE_Spot_Name</th>\n",
       "      <th>ATTRIBUTE_time_run</th>\n",
       "      <th>ATTRIBUTE_Injection_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SD_10_2018_10_a.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>a</td>\n",
       "      <td>10</td>\n",
       "      <td>32.86261</td>\n",
       "      <td>-117.26042</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>SIO_South_Pier</td>\n",
       "      <td>18/07/2020 18:19</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SD_10_2018_10_b.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>b</td>\n",
       "      <td>10</td>\n",
       "      <td>32.86261</td>\n",
       "      <td>-117.26042</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>SIO_South_Pier</td>\n",
       "      <td>18/07/2020 18:35</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename ATTRIBUTE_Sample.Type  ATTRIBUTE_Batch  \\\n",
       "0  SD_10_2018_10_a.mzXML                Sample                3   \n",
       "1  SD_10_2018_10_b.mzXML                Sample                3   \n",
       "\n",
       "  ATTRIBUTE_Month  ATTRIBUTE_Year  ATTRIBUTE_Sample_Location  \\\n",
       "0             Oct            2018                         10   \n",
       "1             Oct            2018                         10   \n",
       "\n",
       "  ATTRIBUTE_Replicate  ATTRIBUTE_Spot  ATTRIBUTE_Latitude  \\\n",
       "0                   a              10            32.86261   \n",
       "1                   b              10            32.86261   \n",
       "\n",
       "   ATTRIBUTE_Longitude ATTRIBUTE_Sample_Area ATTRIBUTE_Spot_Name  \\\n",
       "0           -117.26042   SIO_La_Jolla_Shores      SIO_South_Pier   \n",
       "1           -117.26042   SIO_La_Jolla_Shores      SIO_South_Pier   \n",
       "\n",
       "  ATTRIBUTE_time_run  ATTRIBUTE_Injection_order  \n",
       "0   18/07/2020 18:19                        145  \n",
       "1   18/07/2020 18:35                        146  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dimension: ', new_md.shape)\n",
    "new_md.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fca785-2b7f-4327-b9fc-2f2ed813f989",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'> Step 15: Verifying File Consistency </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "33a4f08e-621d-44fc-8e4e-a4f04b3a9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ft = new_ft.reindex(columns=sorted(new_ft.columns)) # Ordering the columns of 'new_ft' by their names\n",
    "new_md = new_md.sort_values(by='filename').reset_index(drop=True) #ordering the md by the 1st column filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "07ff3b92-1276-4935-be96-43daaae81032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>ATTRIBUTE_Sample.Type</th>\n",
       "      <th>ATTRIBUTE_Batch</th>\n",
       "      <th>ATTRIBUTE_Month</th>\n",
       "      <th>ATTRIBUTE_Year</th>\n",
       "      <th>ATTRIBUTE_Sample_Location</th>\n",
       "      <th>ATTRIBUTE_Replicate</th>\n",
       "      <th>ATTRIBUTE_Spot</th>\n",
       "      <th>ATTRIBUTE_Latitude</th>\n",
       "      <th>ATTRIBUTE_Longitude</th>\n",
       "      <th>ATTRIBUTE_Sample_Area</th>\n",
       "      <th>ATTRIBUTE_Spot_Name</th>\n",
       "      <th>ATTRIBUTE_time_run</th>\n",
       "      <th>ATTRIBUTE_Injection_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SD_01-2018_10_a.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>2</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>a</td>\n",
       "      <td>10</td>\n",
       "      <td>32.86261</td>\n",
       "      <td>-117.26042</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>SIO_South_Pier</td>\n",
       "      <td>16/01/2018 16:23</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SD_01-2018_10_b.mzXML</td>\n",
       "      <td>Sample</td>\n",
       "      <td>2</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>b</td>\n",
       "      <td>10</td>\n",
       "      <td>32.86261</td>\n",
       "      <td>-117.26042</td>\n",
       "      <td>SIO_La_Jolla_Shores</td>\n",
       "      <td>SIO_South_Pier</td>\n",
       "      <td>16/01/2018 16:39</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename ATTRIBUTE_Sample.Type  ATTRIBUTE_Batch  \\\n",
       "0  SD_01-2018_10_a.mzXML                Sample                2   \n",
       "1  SD_01-2018_10_b.mzXML                Sample                2   \n",
       "\n",
       "  ATTRIBUTE_Month  ATTRIBUTE_Year  ATTRIBUTE_Sample_Location  \\\n",
       "0             Jan            2018                         10   \n",
       "1             Jan            2018                         10   \n",
       "\n",
       "  ATTRIBUTE_Replicate  ATTRIBUTE_Spot  ATTRIBUTE_Latitude  \\\n",
       "0                   a              10            32.86261   \n",
       "1                   b              10            32.86261   \n",
       "\n",
       "   ATTRIBUTE_Longitude ATTRIBUTE_Sample_Area ATTRIBUTE_Spot_Name  \\\n",
       "0           -117.26042   SIO_La_Jolla_Shores      SIO_South_Pier   \n",
       "1           -117.26042   SIO_La_Jolla_Shores      SIO_South_Pier   \n",
       "\n",
       "  ATTRIBUTE_time_run  ATTRIBUTE_Injection_order  \n",
       "0   16/01/2018 16:23                         83  \n",
       "1   16/01/2018 16:39                         84  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_md.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1f8c9d60-fb45-4994-8434-b73c52afc59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    186\n",
       "Name: filename, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many files in the metadata are also present in the feature table\n",
    "new_md['filename'].isin(new_ft.columns).value_counts()  #if this returns False it means some files are missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GDKlhoZ0m2ZQ",
   "metadata": {
    "id": "GDKlhoZ0m2ZQ"
   },
   "source": [
    "The output says that all 186 files are present in both new_md & new_ft. Furthermore, metadata filenames and feature table column names are identical, indicating that they are in the same order. If the above lines returns FALSE, it means some files are missing. To check names of the files that are missing we can run the next cell. If everything went well, the next cell should return no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe30c3a",
   "metadata": {
    "id": "ffe30c3a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check if new_ft column names and md row names are the same\n",
    "if sorted(new_ft.columns) == sorted(new_md['filename']):\n",
    "    print(f\"All {len(new_ft.columns)} files are present in both new_md & new_ft.\")\n",
    "else:\n",
    "    print(\"Not all files are present in both new_md & new_ft.\\n\")\n",
    "    # print the md rows / ft column which are not in ft columns / md rows and remove them\n",
    "    ft_cols_not_in_md = [col for col in new_ft.columns if col not in new_md['filename']]\n",
    "    print(f\"These {len(ft_cols_not_in_md)} columns of feature table are not present in metadata table and will be removed:\\n{', '.join(ft_cols_not_in_md)}\\n\")\n",
    "    new_ft.drop(columns=ft_cols_not_in_md, inplace=True)\n",
    "    md_rows_not_in_ft = [row for row in new_md['filename'] if row not in new_ft.columns]\n",
    "    print(f\"These {len(md_rows_not_in_ft)} rows of metadata table are not present in feature table and will be removed:\\n{', '.join(md_rows_not_in_ft)}\\n\")\n",
    "    new_md.drop(md_rows_not_in_ft, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4a662",
   "metadata": {
    "id": "8ec4a662"
   },
   "source": [
    "When the above cell returns some filenames, check the corresponding column names in the feature table for spelling mistakes, case-sensitive errors. Reupload the files with correct metadata and rerun the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "daa374c3",
   "metadata": {
    "id": "daa374c3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows and columns in our original ft is: (11217, 200) \n",
      "\n",
      "The number of rows and columns in our new ft is: (11217, 186) \n",
      "\n",
      "The number of rows and columns in our new md is: (186, 14)\n"
     ]
    }
   ],
   "source": [
    "#checking the dimensions of our new ft and md:\n",
    "print(\"The number of rows and columns in our original ft is:\", ft.shape,\"\\n\")\n",
    "print(\"The number of rows and columns in our new ft is:\", new_ft.shape,\"\\n\")\n",
    "print(\"The number of rows and columns in our new md is:\", new_md.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rH7zRZIznD9m",
   "metadata": {
    "id": "rH7zRZIznD9m"
   },
   "source": [
    "Notice that the number of columns of the new feature table is the same as the number of rows in our new metadata. Now, we have both our feature table and metadata in the same order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45b589",
   "metadata": {
    "id": "4f45b589"
   },
   "source": [
    "# <font color ='blue'> 3. Data-cleanup </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27396ed",
   "metadata": {
    "id": "f27396ed"
   },
   "outputs": [],
   "source": [
    "ft_t = pd.DataFrame(new_ft).T #transposing the ft\n",
    "ft_t = ft_t.apply(pd.to_numeric) #converting all values to numeric\n",
    "np.array_equal(new_md.index,ft_t.index) #should return True now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa2d36",
   "metadata": {
    "id": "85fa2d36"
   },
   "source": [
    "As a first step of data-cleanup step, lets merge the metadata and feature table (transposed) together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1931d",
   "metadata": {
    "id": "a5e1931d"
   },
   "outputs": [],
   "source": [
    "#merging metadata (new_md) and transposed feature table based on the sample names\n",
    "ft_merged = pd.merge(new_md.reset_index(),ft_t.reset_index(), on= \"index\", left_on=None, right_on=None) #by.x=\"filename\" picks the filename column of new_md, by.y =0 indicates the rownames of ft_t\n",
    "ft_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DZq0h99Na59R",
   "metadata": {
    "id": "DZq0h99Na59R"
   },
   "outputs": [],
   "source": [
    "ft_merged.to_csv(os.path.join(result_dir,\"Ft_md_merged.csv\")) # This file can be used for batch correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076607da",
   "metadata": {
    "id": "076607da"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b><font size=3> Skip the Batch correction section if you do not have multiple batches !! </font> </b> </div>\n",
    "\n",
    "## Batch Correction (Optional)\n",
    "<a id=\"batch_corr\"></a>\n",
    "\n",
    "<p style='text-align: justify;'> A 'Batch' is a group of samples processed and analyzed by the same experimental & instrumental conditions in the same short time period. In general, if we have more samples than the tray size, we might measure them as multiple batches or groups. When arranging samples in a batch for measurement, in order to ensure biological diversity within a batch, in addition to our samples of interest, it is advised to have QCs, blanks, and controls (Wehrens et al., 2016). To merge data from these different batches, we must look for batch-effects, both, between the batches and within each batch and correct these effects. <b>But, prior to batch correction on a dataset, we should evaluate the severity of the batch effect and when it is small, it is best to not perform batch correction as this may result in an incorrect estimation of the biological variance in the data. Instead, we should treat the statistical results with caution (Nygaard et al., 2016). For more details, please read the manuscript </b>.</p>\n",
    "\n",
    "<p style='text-align: justify;'> In this tutorial, the test dataset was utilized to evaluate the chemical impacts of a significant rain event that occurred in northern San Diego, California (USA) during the Winter of 2017/2018. Despite the presence of a \"ATTRIBUTE_Batch\" column in the metadata, the 3 groups mentioned are not considered as batches due to their distinct collection conditions. The \"ATTRIBUTE_time_run\" column clearly indicates that the seawater samples were collected and measured at different times during Dec 2017, Jan 2018 (after rainfall), and Oct 2018, respectively. Also, they were collected 'before' and 'after' rainfall. Therefore, searching for inter-batch effects is not meaningful in our example dataset. In terms of intra-batch effect, since the sample dataset does not have QCs, we cannot correct for the intra-batch effect.</p>\n",
    "\n",
    "Follow the notebook for Batch Correction: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Functional-Metabolomics-Lab/FBMN-STATS/blob/main/Individual_Notebooks/R-Notebooks/Batch_Correction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099574",
   "metadata": {
    "id": "43099574",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Blank removal\n",
    "<a id=\"blank_rem\"></a>\n",
    "\n",
    "<p style='text-align: justify;'> Blank samples contain no analytes of interest and consist, for example, of the solvent, matrix, tissue or growth media that was used to prepare or dissolve the samples and analytes of interest. Such as the analytes, the mass spectral features deriving from blank samples are also detected by the LC-MS/MS instrument.\n",
    "We need to remove these blank features to obtain accurate results.</p>\n",
    "\n",
    "<p style='text-align: justify;'>To eliminate these blank features, we initially split the feature table into blanks and samples, and then employ a cutoff filter. Next, we compute the average feature intensities for the blanks and samples, and subsequently calculate the ratio of average_feature_blanks to average_feature_sample. We compare this ratio against the user-defined cutoff to determine which features to be removed. When the cutoff is set to 0.3, it implies that for any feature, up to 30% contribution from the blank and 70% from the sample are allowed. Hence any feature with a ratio greater than 0.3 is removed. By using a lower cutoff, such as 10% (0.1), we would demand a greater contribution from the sample (90%) and restrict the blank's contribution to 10%. Raising the cutoff leads to fewer background features being identified and more analyte features being observed. Conversely, lowering the cutoff is more rigorous and leads to the removal of more features.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13b894-d78b-4088-b271-d3e9c2c02005",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'>  Step 16: Examine Metadata Attributes </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189a54a-8655-4523-90cb-7be714127806",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'subset_data' in locals():\n",
    "    data = subset_data\n",
    "else:\n",
    "    data = new_md\n",
    "display(inside_levels(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a7822-0a0e-45d8-bbcf-c3dd091f612d",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'>  Step 17: Separate Blank Files </font>\n",
    "<font color=\"red\"> Looking at the metadata summary above, change the column name and blank name in the cell below according to your metadata ```['ATTRIBUTE_Sample.Type'] == \"Blank\"``` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc3000-e499-49c9-9bd9-5c7db3c4d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_Blank = new_md[new_md['ATTRIBUTE_Sample.Type'] == \"Blank\"]\n",
    "Blank = ft_t[ft_t.index.isin(md_Blank['filename'])]\n",
    "\n",
    "# Display the chosen blanks\n",
    "print('Dimension: ',Blank.shape)\n",
    "Blank.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de9aaff-8a51-4142-9ac2-d50f40b27f16",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'>  Step 18: Separate Sample Files </font>\n",
    "<font color=\"red\"> Looking at the metadata summary above, change the column name and sample name in the cell below according to your metadata ```['ATTRIBUTE_Sample.Type'] == \"Sample\"``` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7e0e3-3717-4dc3-b03a-1780dc960f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_Samples = new_md[new_md['ATTRIBUTE_Sample.Type'] == \"Sample\"]\n",
    "Samples = ft_t[ft_t.index.isin(md_Samples['filename'])]\n",
    "\n",
    "# Display the chosen samples\n",
    "print('Dimension: ',Samples.shape)\n",
    "Samples.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98614de-beb5-4f40-a133-d67bff0b3ed1",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'> Step 19: Define Cutoff </font>\n",
    "Now that we have our feature table split into blanks and samples, we can start removing blank features. **<font color='red'> We use a cutoff of 0.3 </font>**, meaning that in order for a feature to be considered of interest, it needs to have a ratio of average_feature_blanks vs average_feature_sample <30%. <font color='red'>In the below cell you can interactively change the threshold to any value between 0.1 and 1. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d732faaa-5f31-4cf0-a05c-b65f5dcb1412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Cutoff value between 0.1 & 1 (Ideal cutoff range: 0.1-0.3): 0.3\n"
     ]
    }
   ],
   "source": [
    "Cutoff = float(input('Enter Cutoff value between 0.1 & 1 (Ideal cutoff range: 0.1-0.3):')) # Input cutoff value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207bedd-d63f-4249-af9c-684d349673ea",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'> Step 20: Perform Blank Removal </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c532d-bdb8-424f-85a0-9eb6ece08f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Avg_ft = pd.DataFrame({'Avg_blank': Blank.mean(), \n",
    "                       'Avg_samples': Samples.mean()})\n",
    "\n",
    "Avg_ft['Ratio_blank_Sample'] = (Avg_ft['Avg_blank'] + 1) / (Avg_ft['Avg_samples'] + 1)\n",
    "Avg_ft['Bg_bin'] = (Avg_ft['Ratio_blank_Sample'] > Cutoff).astype(int)\n",
    "\n",
    "print(\"Total no.of features:\", len(Avg_ft))\n",
    "print(\"No.of Background or noise features:\", Avg_ft['Bg_bin'].sum())\n",
    "print(\"No.of features after excluding noise:\", len(Samples.columns) - Avg_ft['Bg_bin'].sum())\n",
    "\n",
    "blk_rem = pd.merge(Samples.transpose(), Avg_ft, left_index=True, right_index=True)\n",
    "blk_rem = blk_rem[blk_rem['Bg_bin'] == 0]\n",
    "blk_rem = blk_rem.drop(['Avg_blank', 'Avg_samples', 'Ratio_blank_Sample', 'Bg_bin'], axis=1)\n",
    "blk_rem = blk_rem.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e958b0c-a43d-45ac-8e36-5b64139e930c",
   "metadata": {},
   "source": [
    "### <font color ='darkblue'> Step 21: Review the blank removed table </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5edff0-2a0e-4b5e-9b73-da3e1571d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimension: ',blk_rem.shape)\n",
    "display(blk_rem.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4956e6-4d14-458b-aef0-2e263099c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blk_rem.to_csv(os.path.join(\"Blanks_Removed_with_cutoff_\"+ str(cutoff) + \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72034cb6-d0ce-482e-8fcd-856c84aedcc2",
   "metadata": {},
   "source": [
    "## Compare which code works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uPMPshn5dXn3",
   "metadata": {
    "id": "uPMPshn5dXn3"
   },
   "outputs": [],
   "source": [
    "blank_removal = samples.copy()\n",
    "\n",
    "    # Getting mean for every feature in blank and Samples\n",
    "    avg_blank = blank.mean(axis=1, skipna=False) # set skipna = False do not exclude NA/null values when computing the result.\n",
    "    avg_samples = samples.mean(axis=1, skipna=False)\n",
    "\n",
    "    # Getting the ratio of blank vs samples\n",
    "    ratio_blank_samples = (avg_blank+1)/(avg_samples+1)\n",
    "\n",
    "    # Create an array with boolean values: True (is a real feature, ratio<cutoff) / False (is a blank, background, noise feature, ratio>cutoff)\n",
    "    is_real_feature = (ratio_blank_samples<cutoff)\n",
    "\n",
    "    # Checking if there are any NA values present. Having NA values in the 4 variables will affect the final dataset to be created\n",
    "    temp_NA_Count = pd.concat([avg_blank, avg_samples, ratio_blank_samples, is_real_feature],\n",
    "                            keys=['avg_blank', 'avg_samples', 'ratio_blank_samples', 'bg_bin'], axis = 1)\n",
    "\n",
    "    print('No. of NA values in the following columns: ')\n",
    "    display(pd.DataFrame(temp_NA_Count.isna().sum(), columns=['NA']))\n",
    "\n",
    "    # Calculating the number of background features and features present (sum(bg_bin) equals number of features to be removed)\n",
    "    print(f\"No. of Background or noise features: {len(samples)-sum(is_real_feature)}\")\n",
    "    print(f\"No. of features after excluding noise: {sum(is_real_feature)}\")\n",
    "\n",
    "    blank_removal = samples[is_real_feature.values]\n",
    "    # save to file\n",
    "    blank_removal.to_csv(os.path.join(result_dir, \"Blanks_Removed.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4210007",
   "metadata": {
    "id": "b4210007",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Imputation\n",
    "<a id=\"norm\"></a>\n",
    "\n",
    "<p style='text-align: justify;'> For several reasons, real world datasets might have some missing values in it, in the form of NA, NANs or 0s. Eventhough the gapfilling step of MZmine fills the missing values, we still end up with some missing values or 0s in our feature table. This could be problematic for statistical analysis. </p>\n",
    "<p style='text-align: justify;'> In order to have a better dataset, we cannot simply discard those rows or columns with missing values as we will lose a chunk of our valuable data. Instead we can try imputing those missing values. Imputation involves replacing the missing values in the data with a meaningful, reasonable guess. There are several methods, such as: </p>\n",
    "\n",
    "1) Mean imputation (replacing the missing values in a column with the mean or average of the column)\n",
    "2) Replacing it with the most frequent value\n",
    "3) Several other machine learning imputation methods such as k-nearest neighbors algorithm(k-NN), Hidden Markov Model(HMM)\n",
    "\n",
    "Here, first, we use the blank-removed feature table to assess frequencies across relative intensities. The plot from the below cell shows us how many features have which relative intensities. We then create random values between 0 and the minimum value in our blank-removed table and randomly replace all 0s with these random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JwFJ1lrlf_1S",
   "metadata": {
    "id": "JwFJ1lrlf_1S"
   },
   "outputs": [],
   "source": [
    "bins, bins_label, a = [-1, 0, 1, 10], [\"-1\",\"0\", \"1\", \"10\"], 2\n",
    "\n",
    "while a<=10:\n",
    "    bins_label.append(np.format_float_scientific(10**a))\n",
    "    bins.append(10**a)\n",
    "    a+=1\n",
    "\n",
    "freq_table = pd.DataFrame(bins_label)\n",
    "frequency = pd.DataFrame(np.array(np.unique(np.digitize(blank_removal.to_numpy(), bins, right=True), return_counts=True)).T).set_index(0)\n",
    "freq_table = pd.concat([freq_table,frequency], axis=1).fillna(0).drop(0)\n",
    "freq_table.columns = [\"intensity\", \"Frequency\"]\n",
    "freq_table[\"Log(Frequency)\"] = np.log(freq_table[\"Frequency\"]+1)\n",
    "\n",
    "# get the lowest intensity (that is not zero) as a cutoff LOD value\n",
    "cutoff_LOD = round(blank_removal.replace(0, np.nan).min(numeric_only=True).min())\n",
    "\n",
    "fig = px.bar(freq_table, x=\"intensity\", y=\"Log(Frequency)\", template=\"plotly_white\",  width=600, height=400)\n",
    "\n",
    "fig.update_traces(marker_color=\"#696880\")\n",
    "fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                  title={\"text\":\"Frequency plot - Gap Filled\", \"x\":0.5, \"font_color\":\"#3E3D53\"},\n",
    "                  xaxis_title= \"Range\")\n",
    "fig.write_image(os.path.join(result_dir, \"frequency_plot.svg\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aE9z6OQZhLbn",
   "metadata": {
    "id": "aE9z6OQZhLbn"
   },
   "source": [
    "The above histogram shows that, in our, blank-removed feature table, there are many zeros present. And no values in the range between 0 to 1E2. The minimum value greater than 0 in our dataframe is in between 1E2 & 1E3 (and that value is 892).\n",
    "\n",
    "A random number between this minimum value and zero will be used for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0rBzEjIUhu-R",
   "metadata": {
    "id": "0rBzEjIUhu-R"
   },
   "outputs": [],
   "source": [
    "imputed = blank_removal.copy()\n",
    "if(input(\"Do you want to perform Imputation? - Y/N: \").upper()==\"Y\"):\n",
    "    #imputed.replace(0, np.random.randint(0, cutoff_LOD), inplace=True)\n",
    "    imputed = imputed.apply(lambda x: [np.random.randint(0, cutoff_LOD) if v == 0 else v for v in x])\n",
    "    print('Dimension: ',imputed.shape)\n",
    "    display(imputed)\n",
    "    print('Number of missing values after imputation:', display(pd.DataFrame(imputed.isna().sum(), columns=['NA'])))\n",
    "    # save to file\n",
    "    imputed.to_csv(os.path.join(result_dir, f\"Imputed_QuantTable.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a2dd7",
   "metadata": {
    "id": "f06a2dd7"
   },
   "source": [
    "## Normalization\n",
    "<a id=\"norm\"></a>\n",
    "\n",
    "Normalization is performed to compensate for differences in total metabolite concentrations among samples (Y. Wu & Li,2016). Many normalization techniques can also correct the batch effects, such as those caused by sample pipetting or extraction. Here, we present 2 types of normalization: Total Ion Current(TIC) or (Probabilistic Quotient Normalization) PQN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D4Bas1IUrXCJ",
   "metadata": {
    "id": "D4Bas1IUrXCJ"
   },
   "source": [
    "### <font color ='darkblue'> Step 25: Total Ion Current (TIC) or sample-centric normalization </font>\n",
    "<a name=\"tic\"></a>\n",
    "\n",
    "<p style='text-align: justify;'> TIC is a simple normalization technique that is commonly used as it is easy to implement and computationally inexpensive. It scales the intensities of all features in a sample by the total ion current (or sum) of the sample. TIC normalization is appropriate when the volume of sample injected into the instrument is consistent across all samples, and when the differences in the total ion count among samples are mainly due to differences in the concentration of sample injected. However, TIC has its limitation such as few features with high ion observation can heavily influence the calculation. Also it assumes most metabolites are stable and equally up/down-regulated among samples, which does not hold true in cases like normal vs cancerous tissues comparison. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea37f9",
   "metadata": {
    "id": "afea37f9"
   },
   "outputs": [],
   "source": [
    "normalized = imputed.copy()\n",
    "if(input(\"Do you want to perform TIC Normalization? - Y/N: \").upper()==\"Y\"):\n",
    "    # Dividing each element of a particular column with its column sum\n",
    "    normalized_TIC = normalized.apply(lambda x: x/np.sum(x), axis=0)\n",
    "\n",
    "    # save to file\n",
    "    normalized_TIC.to_csv(os.path.join(result_dir, \"Normalised_TIC_Quant_table.csv\"))\n",
    "\n",
    "    print('Dimension: ', normalized_TIC.shape)\n",
    "    display(normalized_TIC.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e29a38c",
   "metadata": {
    "id": "1e29a38c"
   },
   "source": [
    "### <font color ='darkblue'> Step 26: Probabilistic Quotient Normalization (PQN) </font>\n",
    "<a name=\"pqn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tjW2_z_1rbx6",
   "metadata": {
    "id": "tjW2_z_1rbx6"
   },
   "source": [
    "PQN is a statistical method that aims to correct for technical variability in MS-based metabolomics data. The goal of PQN is to adjust for systematic variation between samples while preserving the relative differences in metabolite levels between samples.\n",
    "\n",
    "PQN involves a four-step process:\n",
    "\n",
    "1) TIC normalization: Each spectrum is divided by its TIC to adjust for differences in the total signal intensity between samples.\n",
    "2) Control spectrum calculation: A control spectrum representing the typical profile of metabolite levels across all samplesis calculated. This can be a median spectrum from all samples.\n",
    "3) Quotient calculation: For each metabolite feature, the ratio (quotient) of the TIC-normalized intensity of the sample to the control spectrum is calculated.\n",
    "4) Median normalization: The final normalized value for each feature is the median of all the quotients calculated in step 3.\n",
    "\n",
    "By using the median of the quotients, PQN reduces the impact of any individual features with extremely high or low values that might skew the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OJ0p875Aj_F9",
   "metadata": {
    "id": "OJ0p875Aj_F9"
   },
   "outputs": [],
   "source": [
    "if(input(\"Do you want to perform PQN Normalization? - Y/N: \").upper()==\"Y\"):\n",
    "    # Dividing each element of a parPQNular column with its column sum\n",
    "    normalized_PQN = PQN_normalization(normalized ,ref_norm = \"median\" , verbose=False)\n",
    "\n",
    "    # save to file\n",
    "    normalized_PQN.to_csv(os.path.join(result_dir, \"Normalised_PQN_Quant_table.csv\"))\n",
    "\n",
    "    print('Dimension: ', normalized_PQN.shape)\n",
    "    display(normalized_PQN.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w7rYinojzPVx",
   "metadata": {
    "id": "w7rYinojzPVx"
   },
   "source": [
    "## <font color ='darkblue'> 3.5 Scaling </font>\n",
    "<a id=\"norm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YDQKq9Udrinx",
   "metadata": {
    "id": "YDQKq9Udrinx"
   },
   "source": [
    "One can also perform center-scaling after imputation. Scaling is typically done to make sure that the data is centered around 0 and has a consistent spread to adjust for differences in offset between high and low-abundant metabolites, thus leaving only relevant variation for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bwSzUDgomTnm",
   "metadata": {
    "id": "bwSzUDgomTnm"
   },
   "outputs": [],
   "source": [
    "# transposing the imputed table before scaling\n",
    "transposed = imputed.T\n",
    "print(f'Imputed feature table rows/columns: {transposed.shape}')\n",
    "display(transposed.head(3))\n",
    "# put the rows in the feature table and metadata in the same order\n",
    "transposed.sort_index(inplace=True)\n",
    "md_samples.sort_index(inplace=True)\n",
    "\n",
    "if (md_samples.index == transposed.index).all():\n",
    "    pass\n",
    "else:\n",
    "    print(\"WARNING: Sample names in feature and metadata table are NOT the same!\")\n",
    "\n",
    "transposed.to_csv(os.path.join(result_dir, \"Imputed_QuantTable_transposed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L3U41ypivQ4Z",
   "metadata": {
    "id": "L3U41ypivQ4Z"
   },
   "outputs": [],
   "source": [
    "# center and scale filtered data\n",
    "scaled = pd.DataFrame(StandardScaler().fit_transform(transposed), index=transposed.index, columns=transposed.columns)\n",
    "scaled.to_csv(os.path.join(result_dir, \"Imputed_Scaled_QuantTable.csv\"))\n",
    "\n",
    "# Merge feature table and metadata to one dataframe:\n",
    "# \"how=inner\" performs an inner join (only the filenames that appear in md_samples and data are kept)\n",
    "data = pd.merge(md_samples, scaled, left_index=True, right_index=True, how=\"inner\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318ded7-12f5-4c9d-92df-a9feb9b0b312",
   "metadata": {
    "id": "0LUULrFH5uLg"
   },
   "source": [
    "# <font color ='blue'> 5. Multivariate analysis </font>\n",
    "<a name=\"multi\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b793a01-6953-42ea-a393-8e1f885070ec",
   "metadata": {
    "id": "DlmsH5pz7Ez0"
   },
   "source": [
    "## <font color ='darkblue'> 5.1 PCoA PermANOVA </font>\n",
    "<a id=\"norm_test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b019d4f-2a66-40e2-8fcd-535084aa2024",
   "metadata": {
    "id": "IxWdgRVszZ4B"
   },
   "source": [
    "<p style='text-align: justify;'> <b>Principal coordinates analysis (PCoA)</b> is a metric multidimensional scaling (MDS) method that attempts to represent sample dissimilarities in a low-dimensional space. It converts a distance matrix consisting of pair-wise distances (dissimilarities) across samples into a 2- or 3-D graph. <a href=\"https://onlinelibrary.wiley.com/doi/10.1002/0470011815.b2a13070\">(Gower, 2005)</a></p>\n",
    "    \n",
    "<p style='text-align: justify;'> Different distance metrics can be used to calculate dissimilarities among samples (e.g. Euclidean, Canberra, Minkowski). Performing a principal coordinates analysis using the Euclidean distance metric is the same as performing a principal components analysis (PCA). The selection of the most appropriate metric depends on the nature of your data and assumptions made by the metric.</p>\n",
    "\n",
    "<p style='text-align: justify;'> Within the metabolomics field the Euclidean, Bray-Curtis, Jaccard or Canberra distances are most commonly used. The Jaccard distance is an unweighted metric (presence/absence) whereas Euclidean, Bray-Curtis and Canberra distances take into account relative abundances (weighted). Some metrics may be better suited for very sparse data (with many zeroes) than others. For example, the Euclidean distance metric is not recommended to be used for highly sparse data. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1baaaa-6d91-49d1-960f-89c7f96b9e6d",
   "metadata": {
    "id": "jYlcfIj92NSE"
   },
   "source": [
    "This [video tutorial by StatQuest](https://www.youtube.com/watch?v=GEn-_dAyYME) summarizes nicely the basic principles of PCoA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131844f-a392-4b91-9994-a0a87d0113de",
   "metadata": {
    "id": "8131844f-a392-4b91-9994-a0a87d0113de"
   },
   "outputs": [],
   "source": [
    "#calculating Principal components\n",
    "n = 10\n",
    "pca = PCA(n_components=n)\n",
    "pca_df = pd.DataFrame(data = pca.fit_transform(scaled), columns = [f'PC{x}' for x in range(1, n+1)])\n",
    "pca_df.index = md_samples.index\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a7d07",
   "metadata": {
    "id": "3e5a7d07"
   },
   "outputs": [],
   "source": [
    "# To get a scree plot showing the variance of each PC in percentage:\n",
    "percent_variance = np.round(pca.explained_variance_ratio_* 100, decimals =2)\n",
    "\n",
    "fig_bar = px.bar(x=pca_df.columns, y=percent_variance, template=\"plotly_white\",  width=500, height=400)\n",
    "fig_bar.update_traces(marker_color=\"#696880\", width=0.5)\n",
    "fig_bar.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                    title={\"text\":\"PCA - VARIANCE\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "                    xaxis_title=\"principal component\", yaxis_title=\"variance (%)\")\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b896560-1eed-4ae4-a7e3-e40a0e0b9399",
   "metadata": {
    "id": "6f558f90"
   },
   "source": [
    "TODO make the attibute colors work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c798bf-4c4f-409d-8d4d-d963713fd12f",
   "metadata": {
    "id": "f7c798bf-4c4f-409d-8d4d-d963713fd12f"
   },
   "outputs": [],
   "source": [
    "@interact(attribute=sorted(md_samples.columns))\n",
    "def pca_scatter_plot(attribute):\n",
    "    title = f'PRINCIPLE COMPONENT ANALYSIS'\n",
    "\n",
    "    df = pd.merge(pca_df[['PC1', 'PC2']], md_samples[attribute].apply(str), left_index=True, right_index=True)\n",
    "\n",
    "    fig = px.scatter(df, x='PC1', y='PC2', template='plotly_white', width=600, height=400, color=attribute)\n",
    "\n",
    "    fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                      title={\"text\":title, 'x':0.2, \"font_color\":\"#3E3D53\"},\n",
    "                      xaxis_title=f'PC1 {round(pca.explained_variance_ratio_[0]*100, 1)}%',\n",
    "                      yaxis_title=f'PC2 {round(pca.explained_variance_ratio_[1]*100, 1)}%')\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d600578-abec-4d7c-b08b-32f55957dc2d",
   "metadata": {
    "id": "f762eb82"
   },
   "source": [
    "TODO fix the interact thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd99f21-5ff9-4766-ae37-982d0ef956b2",
   "metadata": {
    "id": "edd99f21-5ff9-4766-ae37-982d0ef956b2"
   },
   "outputs": [],
   "source": [
    "matrices = ['canberra', 'chebyshev', 'correlation', 'cosine', 'euclidean', 'hamming', 'jaccard', 'matching', 'minkowski', 'seuclidean']\n",
    "@interact(attribute=sorted(md_samples.columns), distance_matrix=matrices)\n",
    "def pcoa(attribute, distance_matrix):\n",
    "    # Create the distance matrix from the original data\n",
    "    distance_matrix = skbio.stats.distance.DistanceMatrix(distance.squareform(distance.pdist(scaled.values, distance_matrix)))\n",
    "    # perform PERMANOVA test\n",
    "    permanova = skbio.stats.distance.permanova(distance_matrix, md_samples[attribute])\n",
    "    permanova['R2'] = 1 - 1 / (1 + permanova['test statistic'] * permanova['number of groups'] / (permanova['sample size'] - permanova['number of groups'] - 1))\n",
    "    display(permanova)\n",
    "    # perfom PCoA\n",
    "    pcoa = skbio.stats.ordination.pcoa(distance_matrix)\n",
    "    df = pcoa.samples[['PC1', 'PC2']]\n",
    "    df = df.set_index(md_samples.index)\n",
    "    df = pd.merge(df[['PC1', 'PC2']], md_samples[attribute].apply(str), left_index=True, right_index=True)\n",
    "\n",
    "    title = f'PRINCIPLE COORDINATE ANALYSIS'\n",
    "    fig = px.scatter(df, x='PC1', y='PC2', template='plotly_white', width=600, height=400, color=attribute)\n",
    "\n",
    "    fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                      title={\"text\":title, 'x':0.18, \"font_color\":\"#3E3D53\"},\n",
    "                      xaxis_title=f'PC1 {round(pcoa.proportion_explained[0]*100, 1)}%',\n",
    "                      yaxis_title=f'PC2 {round(pcoa.proportion_explained[1]*100, 1)}%')\n",
    "    display(fig)\n",
    "\n",
    "    # To get a scree plot showing the variance of each PC in percentage:\n",
    "    percent_variance = np.round(pcoa.proportion_explained* 100, decimals =2)\n",
    "\n",
    "    fig = px.bar(x=[f'PC{x}' for x in range(1, len(pcoa.proportion_explained)+1)], y=percent_variance, template=\"plotly_white\",  width=500, height=400)\n",
    "    fig.update_traces(marker_color=\"#696880\", width=0.5)\n",
    "    fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                      title={\"text\":\"PCoA - VARIANCE\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "                      xaxis_title=\"principal component\", yaxis_title=\"variance (%)\")#\n",
    "    display(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fdfa2-c353-4426-84dc-36b970b31b43",
   "metadata": {
    "id": "RB-mRw1j2sme"
   },
   "source": [
    "### ToDo: 5.1.1 PERMANOVA\n",
    "\n",
    "<p style='text-align: justify;'> Permutational multivariate analysis of variance (PERMANOVA) is a non-parametric method for multivariate ANOVA, where P-values are obtained using permutations. The metric was originally developed within the field of ecology <a href ='https://onlinelibrary.wiley.com/doi/full/10.1002/9781118445112.stat07841'>(Anderson, 2008)</a> but is today widely used in other fields, including the microbiome and metabolomics field. PERMANOVA is used to compare groups of samples and it tests whether the centroid and/or the spread of the samples is different among the groups. Here, H0 states no differences among the groups and it is rejected when there is a significance difference among the groups. </p>\n",
    "\n",
    "<p style='text-align: justify;'> The adonis2() function in the <a href ='https://cran.r-project.org/web/packages/vegan/index.html'>(vegan package)</a> can be used to perform a PERMANOVA. The input is any dissimilarity matrix and the test-statistic retrieved is a multivariate analogue to Fisher's F-ratio as well as an R2 value (Adonis R2). </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f14477-430c-4e55-ae8e-cb724c8d1133",
   "metadata": {
    "id": "GsJUyFYi2vds"
   },
   "source": [
    "### ToDo: 5.1.2 Perform PCoA and assess separation using PERMANOVA\n",
    "\n",
    "<p style='text-align: justify;'> To speed up the analysis and so we don't have to rewrite the entire code when testing different parameters, we can define a function, which will perform a principal coordinates analysis (PCoA) using a distance metric of choice, calculate a PERMANOVA and plot results in a 2-D graph: </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b07e1d-d274-4665-99ab-0f805c747643",
   "metadata": {
    "id": "D-7F-89O23mH"
   },
   "source": [
    "## <font color = 'darkblue'> 5.2 Hierarchial Clustering Algorithm</font>\n",
    "<a name=\"hca\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77950548-b29f-499d-949b-7ee189ffb12a",
   "metadata": {
    "id": "6b774363-4d57-408b-bf42-25281f2f6017"
   },
   "source": [
    "We are now ready to perform a cluter analysis. The concept behind hierarchical clustering is to repeatedly combine the two nearest clusters into a larger cluster.\n",
    "\n",
    "The first step consists of calculating the distance between every pair of observation points and stores it in a matrix;\n",
    "1. It puts every point in its own cluster;\n",
    "2. It merges the closest pairs of points according to their distances;\n",
    "3. It recomputes the distance between the new cluster and the old ones and stores them in a new distance matrix;\n",
    "4. It repeats steps 2 and 3 until all the clusters are merged into one single cluster. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401920a-a61a-4d00-af0d-5575ccbbd83e",
   "metadata": {
    "id": "QXW8vB7E3PCj"
   },
   "source": [
    "### 5.2.1 Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d6812-7030-4c4e-b787-8b6a5c86c223",
   "metadata": {
    "id": "520d6812-7030-4c4e-b787-8b6a5c86c223"
   },
   "outputs": [],
   "source": [
    "fig = ff.create_dendrogram(scaled, labels=list(scaled.index))\n",
    "fig.update_layout(width=700, height=500, template='plotly_white')\n",
    "\n",
    "# save image as pdf\n",
    "fig.write_image(os.path.join(result_dir, \"Cluster_Dendrogram.pdf\"), scale=3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e16aab-9b4d-4c5f-bc94-9083731037d7",
   "metadata": {
    "id": "H-20vScw3TwR"
   },
   "source": [
    "### ToDo: 5.2.2 Methods to find the optimum number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad3f85-5112-486b-800a-f5618991d704",
   "metadata": {
    "id": "7fad3f85-5112-486b-800a-f5618991d704"
   },
   "outputs": [],
   "source": [
    "# SORT DATA TO CREATE HEATMAP\n",
    "\n",
    "# Compute linkage matrix from distances for hierarchical clustering\n",
    "linkage_data_ft = linkage(scaled, method='complete', metric='euclidean')\n",
    "linkage_data_samples = linkage(scaled.T, method='complete', metric='euclidean')\n",
    "\n",
    "# Create a dictionary of data structures computed to render the dendrogram.\n",
    "# We will use dict['leaves']\n",
    "cluster_samples = dendrogram(linkage_data_ft, no_plot=True)\n",
    "cluster_ft = dendrogram(linkage_data_samples, no_plot=True)\n",
    "\n",
    "# Create dataframe with sorted samples\n",
    "ord_samp = scaled.copy()\n",
    "ord_samp.reset_index(inplace=True)\n",
    "ord_samp = ord_samp.reindex(cluster_samples['leaves'])\n",
    "ord_samp.rename(columns={'index': 'Filename'}, inplace=True)\n",
    "ord_samp.set_index('Filename', inplace=True)\n",
    "\n",
    "# Create dataframe with sorted features\n",
    "ord_ft = ord_samp.T.reset_index()\n",
    "ord_ft = ord_ft.reindex(cluster_ft['leaves'])\n",
    "ord_ft.rename(columns={'index': 'Feature'}, inplace=True)\n",
    "ord_ft.set_index('Feature', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52fa280-7b8a-4762-949f-445418a5d67f",
   "metadata": {
    "id": "ZsrzDCxh3dV7"
   },
   "source": [
    "## <font color = 'darkblue'> ToDo: 5.3 Heatmaps</font>\n",
    "<a name=\"heat_maps\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f60dc-7522-4f57-9bb0-b6f8a196cca2",
   "metadata": {
    "id": "8a4f60dc-7522-4f57-9bb0-b6f8a196cca2"
   },
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "fig = px.imshow(ord_ft,y=list(ord_ft.index), x=list(ord_ft.columns), text_auto=True, aspect=\"auto\",\n",
    "               color_continuous_scale='PuOr_r', range_color=[-3,3])\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=700,\n",
    "    height=800)\n",
    "\n",
    "fig.update_yaxes(visible=False)\n",
    "fig.update_xaxes(tickangle = 35)\n",
    "\n",
    "# save image as pdf\n",
    "fig.write_image(os.path.join(result_dir, \"Heatmap.pdf\"), scale=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb54ee7-2214-44af-8687-2c76ebc37751",
   "metadata": {
    "id": "uwBY842O3mMi"
   },
   "source": [
    "#### ToDo: Heatmap with k-means clustering\n",
    "ComplexHeatmap contains a function to find clusters by using another clustering algorithm called k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ed555-0de0-4236-ac4d-48e127fe1f8c",
   "metadata": {
    "id": "vxduFnV-gy55"
   },
   "source": [
    "# <font color = 'blue'> 6. Supervised learning with Random Forest</font>\n",
    "<a name=\"rf\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e962450-f893-4f97-a37f-545edb284059",
   "metadata": {
    "id": "xSnQaStHg1hw"
   },
   "source": [
    "<p style='text-align: justify;'> Random Forest is a powerful machine learning algorithm used for both classification and regression tasks. It is an ensemble learning method that combines multiple decision trees to improve the accuracy and robustness of the model. Each tree in the forest is constructed using a random subset of features and training data, which helps to reduce overfitting and increase the diversity of the trees. Random Forest can handle multidimensional data with complex interactions and provides feature importance measures, making it a popular choice for many applications in various fields. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1iXTBccg6DT",
   "metadata": {
    "id": "f1iXTBccg6DT"
   },
   "outputs": [],
   "source": [
    "rf_data = Data_merged.copy() # Storing Data_merged after scaling into another variable named rf_data\n",
    "print(rf_data.shape)\n",
    "display(rf_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf3de5-82e8-4fdd-8294-bf8980bfb0f9",
   "metadata": {
    "id": "qygpC3yghPtJ"
   },
   "source": [
    "Printing all the column names that belong to the metadata category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uz0IKv5KhQNu",
   "metadata": {
    "id": "uz0IKv5KhQNu"
   },
   "outputs": [],
   "source": [
    "display(rf_data.filter(regex='^(?!X)').columns.tolist()) # Display the column names corresponding to the metadata columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8c307d-6e9c-422e-a07d-387dcbb8d2e4",
   "metadata": {
    "id": "lR7BBHTphUfc"
   },
   "source": [
    "Here, we are trying to see how RF classifies the samples according to different sample area.\n",
    "In the cell below, enter the column name of the interested attribute to use for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KAJb59iMhW51",
   "metadata": {
    "id": "KAJb59iMhW51"
   },
   "outputs": [],
   "source": [
    "interested_attr = 'ATTRIBUTE_Sample_Area' # Define the attribute of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76629fef-fe09-4f50-a527-387cb947b7cb",
   "metadata": {
    "id": "-qEVMwuqhYAK"
   },
   "source": [
    "For the RF classification we need to transform the location names to numbers using the OrdinalEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X638EehxhZ3B",
   "metadata": {
    "id": "X638EehxhZ3B"
   },
   "outputs": [],
   "source": [
    "# Change the values of the attribute of interest from strings to a numerical\n",
    "enc = OrdinalEncoder()\n",
    "labels = rf_data[[interested_attr]]\n",
    "display(labels.value_counts()) # Displays the sample size for each group\n",
    "labels = enc.fit_transform(labels)\n",
    "labels = np.array([x[0] + 1 for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NuZbmN9ehgxj",
   "metadata": {
    "id": "NuZbmN9ehgxj"
   },
   "outputs": [],
   "source": [
    "# Extract the features (columns starting with X) and their column names\n",
    "features = rf_data.filter(regex='^X')\n",
    "feature_names = features.columns.values.tolist()\n",
    "print(features.shape)\n",
    "display(features.head())\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16bc65e-d680-4c44-9904-1e27e44ce22a",
   "metadata": {
    "id": "T8FLT3_DhnKP"
   },
   "source": [
    "Generate a training and test set for the RF classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeHDOOHUhi9f",
   "metadata": {
    "id": "aeHDOOHUhi9f"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.25, random_state=123)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788ed01-1293-4792-a746-12bb5efc4fef",
   "metadata": {
    "id": "I0ATn1fi7WgW"
   },
   "source": [
    "The sample size is different among the groups (see above). We therefore need to balance the size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-JsQcOa8Os4J",
   "metadata": {
    "id": "-JsQcOa8Os4J"
   },
   "outputs": [],
   "source": [
    "# Balance the weights of the attribute of interest to account for unbalanced sample sizes per group\n",
    "sklearn_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels)\n",
    "weights = {}\n",
    "for i,w in enumerate(np.unique(train_labels)):\n",
    "  weights[w] = sklearn_weights[i]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade50529-5610-4305-9423-60e92b0d179c",
   "metadata": {
    "id": "x9gYgDSphsdo"
   },
   "source": [
    "<p style='text-align: justify;'> Performing Random Forest with 500 trees (n_estimators). Generally, a larger number of this variable improves the performance of the model but also increases the computational cost. It is recommended to start with a reasonable number of trees (e.g., 500-1000) and then tune it based on the performance. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df701632-45e7-4594-80f3-16fcca1440c3",
   "metadata": {
    "id": "iRFb7upWhthk"
   },
   "source": [
    "<font color=\"red\"><font size=3> Depending on the number of trees and permutations provided, the following cell may take a considerable amount of time to execute !!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7jL7Q1Rnhvo8",
   "metadata": {
    "id": "7jL7Q1Rnhvo8"
   },
   "outputs": [],
   "source": [
    "# Set up the random forest classifier with 500 tress, balanded weights, and a random state to make it reproducible\n",
    "rf = RandomForestClassifier(n_estimators=500, class_weight=weights, random_state=123)\n",
    "# Fit the classifier to the training set\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xmj64BJOhxQy",
   "metadata": {
    "id": "Xmj64BJOhxQy"
   },
   "outputs": [],
   "source": [
    "# Use the random forest classifier to predict the sample areas in the test set\n",
    "predictions = rf.predict(test_features)\n",
    "print('Classifier mean accuracy score:', round(rf.score(test_features, test_labels)*100, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vwmUQNzRPV8L",
   "metadata": {
    "id": "vwmUQNzRPV8L"
   },
   "outputs": [],
   "source": [
    "# Report of the accuracy of predictions on the test set\n",
    "print(classification_report(test_labels, predictions))\n",
    "# Print the sample areas corresponding to the numbers in the report\n",
    "for i,cat in enumerate(enc.categories_[0]):\n",
    "  print(i+1.0, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e2d5d-95da-4dd9-abad-492dd557e42a",
   "metadata": {
    "id": "FYma7KXUh0iz"
   },
   "source": [
    "**To be edited**\n",
    "\n",
    "<p style='text-align: justify;'>In Random Forest, the OOB (Out-Of-Bag) error is an estimate of the model's prediction error on unseen data. During the training process, each tree in the forest is grown using a bootstrap sample of the original data, leaving out about one-third of the observations on average. These left-out observations are called the OOB samples. The OOB error line in Random Forest provides an estimate of the model's error rate based on the OOB samples. It is a useful metric for evaluating the performance of the model and for comparing different random forest models. A lower OOB error indicates better predictive performance, but it is important to validate the model on a separate test set to obtain a more accurate estimate of its performance on new data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0QLkCacvh2QZ",
   "metadata": {
    "id": "0QLkCacvh2QZ"
   },
   "outputs": [],
   "source": [
    "# Most important model quality plot\n",
    "# OOB error lines should flatline. If it doesn't flatline add more trees\n",
    "rf = RandomForestClassifier(class_weight=weights, warm_start=True, oob_score=True, random_state=123)\n",
    "errors = []\n",
    "tree_range = np.arange(1,500,5)\n",
    "for i in tree_range:\n",
    "  rf.set_params(n_estimators=i)\n",
    "  rf.fit(train_features, train_labels)\n",
    "  errors.append(rf.oob_score_*100)\n",
    "\n",
    "plt.plot(tree_range, errors)\n",
    "plt.ylim(0)\n",
    "plt.xlabel(\"Trees\")\n",
    "plt.ylabel(\"Percent Correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e818cf2-7182-4642-b557-e0ffa7921bfd",
   "metadata": {
    "id": "qiT3Pf0zh4Hq"
   },
   "source": [
    "\n",
    "**Ranking features by importance**:\n",
    "\n",
    "We can also rank the feature by importance. The top features had a larger impact on determining the prediction of sampling area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K3JyYL1Wh4ue",
   "metadata": {
    "id": "K3JyYL1Wh4ue"
   },
   "outputs": [],
   "source": [
    "# Extract the important features in the model\n",
    "imp = pd.DataFrame(rf.feature_importances_, index=feature_names).sort_values(by=0, ascending=False)\n",
    "display(imp[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T8pe7TxDQre8",
   "metadata": {
    "id": "T8pe7TxDQre8"
   },
   "outputs": [],
   "source": [
    "# Save the features and their importance\n",
    "imp.to_csv(os.path.join(result_dir,'Importance_features_RF_500trees.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etFY1dzWrq_2",
   "metadata": {
    "id": "etFY1dzWrq_2"
   },
   "source": [
    "# <font color ='blue'> 4. Univariate Analysis </font>\n",
    "<a id=\"uni\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DRHowqAkApBe",
   "metadata": {
    "id": "DRHowqAkApBe"
   },
   "source": [
    "<p style='text-align: justify;'>Univariate statistics involves analysing \"one\" variable (or one category) at a time in an attempt to describe the data. In univariate statistics, our null hypothesis H0 states that there is no relationship between different groups or categories. To test this hypothesis, we use statistical tests to either reject (meaning there is a relationship between groups) or accept the null hypothesis (means no relationship). Below here is a list of some parametric and non-parametric tests used for hypothesis testing. In general, parametric test assumes the data to have a normal distribution whereas non-parametric tests have no such assumption about the distribution of the data. </p>\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr><th><font size=3>Parametric Test</font></th>\n",
    "            <th><font size=3>Non-Parametric test</font></th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr><td><font size=3>Paired t-test</font></td>\n",
    "            <td><font size=3>Wilcoxon Rank sum test</font></td></tr>\n",
    "        <tr><td><font size=3>Unpaired t-test</font></td>\n",
    "            <td><font size=3>Mann Whitney U-test</font></td></tr>\n",
    "        <tr><td><font size=3>One-way ANOVA</font></td>\n",
    "            <td><font size=3>Kruskal Wallis Test</font></td></tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "In the following section we will use univariate statistical analyses to investigate how the metabolome is influenced by:\n",
    "*   Sampling site: We will compare seven different sampling areas and investigate if there is a gradual shift in metabolite levels from along the coast.\n",
    "*   Heavy rainfall: We will compare the metabolite levels before and and after a heavy rainfall in January 2018.\n",
    "\n",
    "Once again, let's merge metadata and the data to one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af737a",
   "metadata": {
    "id": "17af737a"
   },
   "outputs": [],
   "source": [
    "Data_merged = pd.merge(md_samples, scaled, left_index=True, right_index=True) #merging metadata with the sclaed data\n",
    "Data_merged.to_csv(os.path.join(result_dir, \"cleaned_Imp_s_metadata_merged.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618a6f4",
   "metadata": {
    "id": "0618a6f4"
   },
   "outputs": [],
   "source": [
    "Data_norm_merged = pd.merge(md_samples, normalized_TIC, left_index=True, right_index=True) #merging metadata with the TIC normalized data\n",
    "Data_norm_merged.to_csv(os.path.join(result_dir, \"cleaned_Imp_normTIC_metadata_merged.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e28a7f",
   "metadata": {
    "id": "11e28a7f"
   },
   "outputs": [],
   "source": [
    "# make sure that the merging was successful\n",
    "print(scaled.shape)\n",
    "print(md_samples.shape)\n",
    "print(Data_merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e15c5c",
   "metadata": {
    "id": "31e15c5c"
   },
   "source": [
    "## <font color ='darkblue'> 4.1 Test for normality </font>\n",
    "<a id=\"norm_test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tE6ZpqqDrwre",
   "metadata": {
    "id": "tE6ZpqqDrwre"
   },
   "source": [
    "In order to decide whether to go for parametric or non-parametric tests, we test for normality. Some common methods to test for normality are:\n",
    "1. Visual representations like histogram, Q–Q Plot\n",
    "2. Statistical tests such as Shapiro–Wilk test, Kolmogorov–Smirnov test\n",
    "\n",
    "The null hypothosis(H0) of these statistical tests states that the data has a normal distribution. H0= TRUE if p > 0.05. <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6350423/\">Read more about normality tests</a>\n",
    "\n",
    "Let's start by inspecting a histogram of the first feature in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4308da3",
   "metadata": {
    "id": "a4308da3"
   },
   "outputs": [],
   "source": [
    "norm_test = Data_merged\n",
    "cols= norm_test.columns\n",
    "filter_col= [col for col in cols if col.startswith(\"X\")] #select all columns that starts with X\n",
    "norm_test= norm_test[filter_col]\n",
    "norm_test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f052b",
   "metadata": {
    "id": "b05f052b"
   },
   "outputs": [],
   "source": [
    "plt.hist(norm_test.iloc[:,0], bins, facecolor='grey', ec=\"black\")\n",
    "plt.xlabel(norm_test.iloc[:,0].name)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a2d59",
   "metadata": {
    "id": "575a2d59"
   },
   "source": [
    "Visualization: We can also visualize the same using a quantile-quantile plot (Q-Q plot), which plots the observed sample distribution against a theoretical normal distribution.  For ex: In our case, the length of <code>norm_test[,1]</code> is 180. There are 180 samples (or datapoints) for that particular feature. In a Q-Q plot, we first sort these 180 datapoints and then calculate the quantile for each datapoint. The quantile of a datapoint will describe the number of datapoints that falls under that particular datapoint with respect to total number of datapoints. Then these sample quantiles are plotted against the quantiles obtained using a standard normal distribution. With a Q-Q plot, we can visually see how much a particular feature varies from normality, but it is very subjective. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53925e6",
   "metadata": {
    "id": "d53925e6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import lognorm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "sm.qqplot(norm_test.iloc[:,0], line=\"s\") #add the line with theoretic normal distribution\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa093e0",
   "metadata": {
    "id": "5aa093e0"
   },
   "source": [
    "Interpretation: The plot indicates that the particular feature follows a non-normal distribution. We can also perform a Shapiro-Wilk test on this feature to assess the normality. Here, H0 states that the data has a normal distribution. In a Shapiro-Wilk test, when W=1, it means H0 is true. Also, H0=TRUE when p > 0.05. Howver, in most cases, it is common to have W<1. To see how different the observed distribution from the \"empirical\" normal distribution, we can look at the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df23a90",
   "metadata": {
    "id": "3df23a90"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "#perform Shapiro-Wilk test for normality\n",
    "shapiro(norm_test.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781de77",
   "metadata": {
    "id": "1781de77"
   },
   "source": [
    "In our case, p<0.05 indicates that the variable follows a non-normal distribution. We can now systematically investigate all metabolite features in the dataset for normality:\n",
    "\n",
    "We can repeat the same steps and test it on a <b>log-transformed data</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930779d",
   "metadata": {
    "id": "6930779d"
   },
   "outputs": [],
   "source": [
    "imputed_log= np.log(transposed) #transposing the imputed table\n",
    "plt.hist(imputed_log.iloc[:,0], bins=6, facecolor='grey', ec=\"black\") # plot histogram of the first feature\n",
    "plt.xlabel(imputed_log.iloc[:,0].name)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "sm.qqplot(imputed_log.iloc[:,0], line=\"s\") # plot the Q-Q plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14e73a",
   "metadata": {
    "id": "de14e73a"
   },
   "outputs": [],
   "source": [
    "#perform Shapiro-Wilk test for normality\n",
    "shapiro(imputed_log.iloc[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741c6ce",
   "metadata": {
    "id": "0741c6ce"
   },
   "source": [
    "Here, from the qqplot and histogram, one can see that log-transformed data is close to normality than the scaled data that was used before.\n",
    "\n",
    "<font color=\"red\"> You can also change the data in the next cell to the <code> norm_TIC</code> or <code> norm_pqn</code> and continue with the next code blocks to test the normality of normalized features. </font>.\n",
    "\n",
    "Next, we perform shapiro test on each feature and obtain their p-values. These p-values are then corrected for false discovery rate using BH (Benjamini & Hochberg) correction method. When the significance level of p_adj < 0.05, it rejects H0, thus it will be counted as a non-normal distribution, else it follows normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5141e8",
   "metadata": {
    "id": "ab5141e8"
   },
   "outputs": [],
   "source": [
    "norm_test = Data_merged\n",
    "cols= norm_test.columns\n",
    "filter_col= [col for col in cols if col.startswith(\"X\")] #select all columns that starts with X\n",
    "norm_test= norm_test[filter_col]\n",
    "uni_data= norm_test\n",
    "op_shapiro = pd.DataFrame(uni_data.columns) # Get all column to a dataframe called \"op_shapiro\"\n",
    "op_shapiro.rename(columns={ op_shapiro.columns[0]: \"Metabolites\" }, inplace = True) # Convert row \"Metabolites\" to header and remove the row\n",
    "op_shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a9ab3",
   "metadata": {
    "id": "869a9ab3"
   },
   "outputs": [],
   "source": [
    "# Compute Shapiro-Wilk test for each column\n",
    "p_values = []\n",
    "for col in uni_data.columns:\n",
    "    _, p_value = shapiro(uni_data[col])\n",
    "    p_values.append(p_value)\n",
    "\n",
    "p_adjusted = sm.stats.fdrcorrection(p_values)[1]\n",
    "op_shapiro['p_adj'] = p_adjusted #adding a column \"p_adj\" with corrected p-values. The method used is FDR\n",
    "op_shapiro['p_adj'] = op_shapiro['p_adj'].astype(float)\n",
    "op_shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c709c43",
   "metadata": {
    "id": "8c709c43"
   },
   "outputs": [],
   "source": [
    "# Classify distributions as normal or non-normal based on adjusted p-values\n",
    "op_shapiro['distribution'] = ['Normal' if p_adj >= 0.05 else 'Non-normal' for p_adj in op_shapiro['p_adj']]\n",
    "\n",
    "# Compute and print number of features with normal and non-normal distribution\n",
    "num_normal = sum(op_shapiro['distribution'] == 'Normal')\n",
    "num_non_normal = sum(op_shapiro['distribution'] == 'Non-normal')\n",
    "print(\"No. of features with normal distribution:\", num_normal)\n",
    "print(\"No. of features with non-normal distribution:\", num_non_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd8c79",
   "metadata": {
    "id": "c1cd8c79"
   },
   "source": [
    "This shows majority of the features have non-normal distribution.  Also, performing shapiro test with log-transformed data shows that 479 features are normally distributed. However, majoity  of them are still not normal. Thus non-parametric tests might make more sense to our data. These normality tests are particularly more useful for small dataset. When your sample size is large, one can still perform the paramteric tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LKPTigNdTGyN",
   "metadata": {
    "id": "LKPTigNdTGyN"
   },
   "source": [
    "## <font color ='darkblue'> 4.2 ANOVA </font>\n",
    "<a id=\"anova\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v8-w6_2_r44x",
   "metadata": {
    "id": "v8-w6_2_r44x"
   },
   "source": [
    "<p style='text-align: justify;'> We can also perform the parametric test, ANOVA (Analysis of Variance) on our data. Here, we will test whether metabolite levels were different between different sampling sites. Here, the seven different sampling areas will be compared. We  use the function 'aov' to run statistical analyses using ANOVA. ANOVA makes use of variances of different groups to see if they are different from each other. (Variance = SD<sup>2</sup>) </p>\n",
    "<p style='text-align: justify;'>\n",
    "<b>H0 = No differences among the groups (their means or Standard deviations)</b>.Using p-value, one can see if the groups are statistically different from one another. When there is a significant difference, F-ratio, another output of ANOVA will be larger and H0 will be rejected. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PjNVZWgqr84D",
   "metadata": {
    "id": "PjNVZWgqr84D"
   },
   "source": [
    "$$F-statistic = \\frac{\\text{between-group variance}}{\\text{within groups variance}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05abc2",
   "metadata": {
    "id": "6a05abc2"
   },
   "outputs": [],
   "source": [
    "# select an attribute to perform ANOVA\n",
    "anova_attribute = 'ATTRIBUTE_Sample_Area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MDCZdylZywGX",
   "metadata": {
    "id": "MDCZdylZywGX"
   },
   "outputs": [],
   "source": [
    "def gen_anova_data(df, columns, groups_col):\n",
    "    for col in columns:\n",
    "        result = pg.anova(data=df, dv=col, between=groups_col, detailed=True).set_index('Source')\n",
    "        p = result.loc[groups_col, 'p-unc']\n",
    "        f = result.loc[groups_col, 'F']\n",
    "        meansq = result.loc[groups_col, 'MS']\n",
    "        yield col, p, f, meansq\n",
    "\n",
    "dtypes = [('metabolite', 'U100'), ('p', 'f'), ('F', 'f'), ('MS', 'f')]\n",
    "anova = pd.DataFrame(np.fromiter(gen_anova_data(Data_merged, uni_data.columns, anova_attribute), dtype=dtypes))\n",
    "anova[\"Significance\"] = ['Non-significant' if p >= 0.05 else 'Significant' for p in anova['p']]\n",
    "anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xRLmgVnq5MOI",
   "metadata": {
    "id": "xRLmgVnq5MOI"
   },
   "outputs": [],
   "source": [
    "# add Bonferroni corrected p-values for multiple testing correction\n",
    "if 'p_bonferroni' not in anova.columns:\n",
    "    anova.insert(2, 'p_bonferroni', pg.multicomp(anova['p'], method='bonf')[1])\n",
    "# add significance\n",
    "if 'significant' not in anova.columns:\n",
    "    anova.insert(3, 'significant', anova['p_bonferroni'] < 0.05)\n",
    "# sort by p-value\n",
    "anova.sort_values('p', inplace=True)\n",
    "# save ANOVA table\n",
    "anova.to_csv(os.path.join(result_dir, 'ANOVA_results.csv'))\n",
    "anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e7e65",
   "metadata": {
    "id": "829e7e65"
   },
   "outputs": [],
   "source": [
    "print(\"Total no.of features on which ANOVA test was performed:\", len(anova))\n",
    "print(\"No.of features that showed significant difference:\", len(anova[anova[\"Significance\"]==\"Significant\"]))\n",
    "print(\"No.of features that did not show significant difference:\", len(anova[anova[\"Significance\"]==\"Non-significant\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P9OMCkdmVrNY",
   "metadata": {
    "id": "P9OMCkdmVrNY"
   },
   "source": [
    "**Plot ANOVA results**\n",
    "<p style='text-align: justify;'> The 'anova_out' results are sorted after the p value. To see if there any significant findings, we will use ggplot to visualize results from the ANOVA, with log(F-Statistic values) on the x-axis and -log(p) on the y-axis. Since there are large differences in the F-Statistic and P-values, it is easier to plot their log values. When displaying the names of the top features in the plot, it can  easily get cluttered if we decide to display too many names. So, starting at the top 6 could be a good idea. Using 'geom_text_repel', one can make sure the labels are not overlapping. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OwSyax7p-IPk",
   "metadata": {
    "id": "OwSyax7p-IPk"
   },
   "outputs": [],
   "source": [
    "# first plot insignificant features\n",
    "fig = px.scatter(x=anova[anova['significant'] == False]['F'].apply(np.log),\n",
    "                y=anova[anova['significant'] == False]['p'].apply(lambda x: -np.log(x)),\n",
    "                template='plotly_white', width=600, height=600)\n",
    "fig.update_traces(marker_color=\"#696880\")\n",
    "\n",
    "# plot significant features\n",
    "fig.add_scatter(x=anova[anova['significant']]['F'].apply(np.log),\n",
    "                y=anova[anova['significant']]['p'].apply(lambda x: -np.log(x)),\n",
    "                mode='markers+text',\n",
    "                text=anova['metabolite'].iloc[:4],\n",
    "                textposition='top left', textfont=dict(color='#ef553b', size=7), name='significant')\n",
    "\n",
    "fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                  title={\"text\":\"ANOVA - FEATURE SIGNIFICANCE\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "                  xaxis_title=\"log(F)\", yaxis_title=\"-log(p)\", showlegend=False)\n",
    "\n",
    "# save fig as pdf\n",
    "fig.write_image(os.path.join(result_dir, \"plot_ANOVA.pdf\"), scale=3) # you can use different file types here e.g. svg, png\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pc0VaEmlK90g",
   "metadata": {
    "id": "pc0VaEmlK90g"
   },
   "outputs": [],
   "source": [
    "# boxplots with top 4 metabolites from ANOVA\n",
    "for metabolite in anova.sort_values('p_bonferroni').iloc[:4, 0]:\n",
    "    fig = px.box(data, x=anova_attribute, y=metabolite, color=anova_attribute)\n",
    "    fig.update_layout(showlegend=False, title=metabolite, xaxis_title=\"\", yaxis_title=\"intensity\", template=\"plotly_white\", width=500)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54640229",
   "metadata": {
    "id": "54640229"
   },
   "source": [
    "For the top four metabolites, Mission bay is the area that drives the difference between sampling sites, with much higher levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y6E_i2aqeevQ",
   "metadata": {
    "id": "y6E_i2aqeevQ"
   },
   "source": [
    "## <font color ='darkblue'> 4.3 Tukey's post-hoc test </font>\n",
    "<a id =\"tukey\"></a>\n",
    "As mentioned above, Tukey's post hoc test is a common post-hoc test after a 1-way anova. It also assumes the data to be normally distributed and homoscedastic (having same variances). One we know that there is a significant difference among different sampling sites, we can use tukey-test to calculate, which features show statistically significant differences between 2 sampling sites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdFyHQTwS1I1",
   "metadata": {
    "id": "qdFyHQTwS1I1"
   },
   "outputs": [],
   "source": [
    "# functions to run Tukey's and plot results\n",
    "\n",
    "def tukey_post_hoc_test(anova_attribute, contrasts, metabolites):\n",
    "    \"\"\"\n",
    "    Perform pairwise Tukey test for all metabolites between contrast combinations.\n",
    "\n",
    "    Args:\n",
    "        anova_attribute: A string representing the attribute to use in ANOVA.\n",
    "        contrasts: A list of tuples, where each tuple contains two strings representing the groups to compare.\n",
    "        metabolites: A list of strings representing the metabolites to test.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the results of the pairwise Tukey test, including the contrast,\n",
    "        metabolite, absolute value of the metabolite ID, difference between the means, p-value, Bonferroni\n",
    "        corrected p-value, and significance (True or False).\n",
    "    \"\"\"\n",
    "\n",
    "    # if a single metabolite gets passed make sure to put it in a list\n",
    "    if isinstance(metabolites, str):\n",
    "        metabolites = [metabolites]\n",
    "\n",
    "    def gen_pairwise_tukey(df, contrasts, metabolites):\n",
    "        \"\"\" Yield results for pairwise Tukey test for all metabolites between contrast combinations.\"\"\"\n",
    "        for metabolite in metabolites:\n",
    "            for contrast in contrasts:\n",
    "                df_for_tukey = df.iloc[np.where(data[anova_attribute].isin([contrast[0], contrast[-1]]))][[metabolite, anova_attribute]]\n",
    "                pairwise_tukey = pg.pairwise_tukey(df_for_tukey, dv=metabolite, between=anova_attribute)\n",
    "                yield f'{contrast[0]}-{contrast[1]}', metabolite, int(metabolite.split('_')[0].replace('X', '')), pairwise_tukey['diff'], pairwise_tukey['p-tukey']\n",
    "\n",
    "    dtypes = [('contrast', 'U100'), ('stats_metabolite', 'U100'), ('stats_ID', 'i'), ('stats_diff', 'f'), ('stats_p', 'f')]\n",
    "    tukey = pd.DataFrame(np.fromiter(gen_pairwise_tukey(data, contrasts, metabolites), dtype=dtypes))\n",
    "    # add Bonferroni corrected p-values\n",
    "    tukey.insert(5, 'stats_p_bonferroni', pg.multicomp(tukey['stats_p'], method='bonf')[1])\n",
    "    # add significance\n",
    "    tukey.insert(6, 'stats_significant', tukey['stats_p_bonferroni'] < 0.05)\n",
    "    # sort by p-value\n",
    "    tukey.sort_values('stats_p', inplace=True)\n",
    "\n",
    "    # write output to csv file\n",
    "    tukey.to_csv(os.path.join(result_dir, 'TukeyHSD_output.csv'))\n",
    "\n",
    "    return tukey\n",
    "\n",
    "def plot_tukey(df):\n",
    "\n",
    "    # create figure\n",
    "    fig = px.scatter(template='plotly_white', width=600, height=600)\n",
    "\n",
    "    # plot insignificant values\n",
    "    fig.add_trace(go.Scatter(x=df[df['stats_significant'] == False]['stats_diff'],\n",
    "                            y=df[df['stats_significant'] == False]['stats_p'].apply(lambda x: -np.log(x)),\n",
    "                            mode='markers', marker_color='#696880', name='insignificant'))\n",
    "\n",
    "    # plot significant values\n",
    "    fig.add_trace(go.Scatter(x=df[df['stats_significant']]['stats_diff'],\n",
    "                            y=df[df['stats_significant']]['stats_p'].apply(lambda x: -np.log(x)),\n",
    "                            mode='markers+text', text=anova['metabolite'].iloc[:4], textposition='top left',\n",
    "                            textfont=dict(color='#ef553b', size=8), marker_color='#ef553b', name='significant'))\n",
    "\n",
    "    fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                    title={\"text\":\"TUKEY\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "                    xaxis_title=\"stats_diff\", yaxis_title=\"-log(p)\")\n",
    "\n",
    "    # save image as pdf\n",
    "    fig.write_image(os.path.join(result_dir, \"TukeyHSD.pdf\"), scale=3)\n",
    "\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a52a9",
   "metadata": {
    "id": "1c6a52a9"
   },
   "source": [
    "For the most significant feature from ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6uWEoGWS_bH",
   "metadata": {
    "id": "i6uWEoGWS_bH"
   },
   "outputs": [],
   "source": [
    "contrasts = list(itertools.combinations(set(Data_merged[anova_attribute]), 2)) # all possible combinations\n",
    "tukey = tukey_post_hoc_test(anova_attribute, contrasts, anova['metabolite'].iloc[0])\n",
    "display(tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31ebd0",
   "metadata": {
    "id": "3a31ebd0"
   },
   "source": [
    "According to the TukeyHSD function description:\n",
    "- term: shows the regression model.\n",
    "- contrast: shows the levels that are compared.\n",
    "- estimate: The estimated difference between the means of the contrast groups.\n",
    "- null.value: Value to which the estimate is compared.\n",
    "- adj.p.value: P-value adjusted for multiple comparisons.\n",
    "- conf.high: Upper bound on the confidence interval for the estimate.\n",
    "- conf.low: Lower bound on the confidence interval for the estimate.\n",
    "\n",
    "Here, every possible pair-wise group difference is explored. From ANOVA results, since Mission Bay seemed to differ from other sampling sites for the four most significant metabolites, we could specifically look at the results from comparison between Mission Bay and another sampling site.\n",
    "\n",
    "In the example below, we look at the differences between Mission Bay and La Jolla Reefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d9f93",
   "metadata": {
    "id": "3e3d9f93"
   },
   "outputs": [],
   "source": [
    "contrasts = [('Mission_Bay', 'La_Jolla Reefs')]\n",
    "tukey = tukey_post_hoc_test(anova_attribute, contrasts, anova[anova['significant']]['metabolite'])\n",
    "display(tukey)\n",
    "plot_tukey(tukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0aa053",
   "metadata": {
    "id": "7c0aa053"
   },
   "outputs": [],
   "source": [
    "print(\"Total no.of features on which ANOVA test was performed:\", len(tukey))\n",
    "print(\"No.of features that showed significant difference:\", len(tukey[tukey[\"stats_significant\"]==True]))\n",
    "print(\"No.of features that did not show significant difference:\", len(tukey[tukey[\"stats_significant\"]==False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb74d4",
   "metadata": {
    "id": "9bfb74d4"
   },
   "source": [
    "## <font color ='darkblue'> 4.4 T-tests </font>\n",
    "<a id =\"tukey\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pYt0ARoDsOcz",
   "metadata": {
    "id": "pYt0ARoDsOcz"
   },
   "source": [
    "A T-test is commonly used when one has to compare between only two groups. Here, null hypothesis H0 states no difference between the mean of 2 groups. Similar to the F-statistic used by ANOVA, T-tests use T-statistic.\n",
    "\n",
    "\n",
    "$$\\text{T-statistic} = \\frac{\\text{Mean}_{\\text{group}} - \\text{Mean}_{\\text{population}}}{\\text{SD}_{\\text{group}} / \\sqrt{\\text{group size}}}$$\n",
    "\n",
    "\n",
    "In our dataset, a heavy rainfall in January 2018 could have influenced the metabolome. We will investigate the effect of the rainfall using t-tests. The 2 conditions will be 'Jan-2018' or 'not Jan-2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6deb4b",
   "metadata": {
    "id": "eb6deb4b"
   },
   "outputs": [],
   "source": [
    "ttest_attribute = 'ATTRIBUTE_Month'\n",
    "target_group = 'Jan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2359a",
   "metadata": {
    "id": "5ce2359a"
   },
   "outputs": [],
   "source": [
    "def gen_ttest_data(df, columns, ttest_attribute, target_group):\n",
    "    ttest = []\n",
    "    for col in columns:\n",
    "        group1 = df[col][df[ttest_attribute]==target_group]\n",
    "        group2 = df[col][df[ttest_attribute]!=target_group]\n",
    "        result = pg.ttest(group1, group2)\n",
    "        result['Metabolite'] = col\n",
    "\n",
    "        ttest.append(result)\n",
    "\n",
    "    ttest = pd.concat(ttest).set_index('Metabolite')\n",
    "\n",
    "    ttest.insert(8, 'p-bonf', pg.multicomp(ttest['p-val'], method='bonf')[1])\n",
    "    # add significance\n",
    "    ttest.insert(9, 'Significance', ttest['p-bonf'] < 0.05)\n",
    "\n",
    "    return ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab96a91",
   "metadata": {
    "id": "4ab96a91"
   },
   "outputs": [],
   "source": [
    "ttest = gen_ttest_data(data, scaled.columns, ttest_attribute, target_group)\n",
    "ttest.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d386d",
   "metadata": {
    "id": "003d386d"
   },
   "outputs": [],
   "source": [
    "# Plot T-test\n",
    "\n",
    "fig = px.scatter(x=ttest['T'],\n",
    "                y=ttest['p-bonf'].apply(lambda x: -np.log(x)),\n",
    "                template='plotly_white', width=600, height=600,\n",
    "                 color=ttest['Significance'].apply(lambda x: str(x)),\n",
    "                color_discrete_sequence = ['#696880', '#ef553b'])\n",
    "\n",
    "fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                  title={\"text\":\"T-test - FEATURE SIGNIFICANCE\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "                  xaxis_title=\"T\", yaxis_title=\"-Log(p)\", showlegend=False)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G8wt62CtzLxo",
   "metadata": {
    "id": "G8wt62CtzLxo"
   },
   "source": [
    "## <font color ='darkblue'> ToDo: 4.5 Kruskal-Wallis </font>\n",
    "<a name=\"kr_wallis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HCUjjpNlzNdD",
   "metadata": {
    "id": "HCUjjpNlzNdD"
   },
   "source": [
    "Kruskal-Wallis Test is a non-parametric version of ANOVA. Here, the test does not assume normality of the data. The median of multiple groups are compared to see if they are statistically different from one another. The null hypothesis H0 states no significant difference among different groups. Based on the p value, we decide whether to reject H0 or not. When H0 is rejected, the alternate hypothesis H1 states that atleast one group is statistically different from the others.\n",
    "<a href=\"https://statsandr.com/blog/kruskal-wallis-test-nonparametric-version-anova/#introduction\">Read more about Kruskal-Wallis test</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DIAqA5nwzSjp",
   "metadata": {
    "id": "DIAqA5nwzSjp"
   },
   "source": [
    "## <font color ='darkblue'> ToDo: 4.6 Dunn's post hoc test </font>\n",
    "<a name =\"dunn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dZT3sWzUvq",
   "metadata": {
    "id": "72dZT3sWzUvq"
   },
   "source": [
    "If Kruskal_Wallis test shows there is a significant difference among groups, then we can perform post-hoc test. The most common post-hoc test for Kruskal-Wallis is \"Dunn Test\" where one can perform multiple pairwise comparison to know exactly which groups are significantly different.\n",
    "Here, in our example, we use Dunn test to calculate, which features show statistically significant differences between individual sampling sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xB4cw-Qj5QrW",
   "metadata": {
    "id": "xB4cw-Qj5QrW"
   },
   "source": [
    "# <font color = 'blue'>  7. Conclusion </font>\n",
    "<a name=\"outro\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baYpCGRG5SQT",
   "metadata": {
    "id": "baYpCGRG5SQT"
   },
   "outputs": [],
   "source": [
    "session_info.show() # To see all the information about the current R session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h4An6RZP5iuW",
   "metadata": {
    "id": "h4An6RZP5iuW"
   },
   "source": [
    "### Getting output files from Google Colab\n",
    "For Google Collab users, we can zip the result folder which contains all the output files using the next cell and download the zip file directly from the folder \"/content/My_TestData\" into the local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3_uRwuv5l1E",
   "metadata": {
    "id": "s3_uRwuv5l1E"
   },
   "outputs": [],
   "source": [
    "# Only for Google Colab\n",
    "output_filename = '../TestData_Workflow_Results' # Specify the name and location of the output directory (here it will be in '/content/TestData_Workflow_Results')\n",
    "dir_input = result_dir # Specify the directory to be zipped (here the Results directory specified in the beginning)\n",
    "shutil.make_archive(output_filename, 'zip', dir_input)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "572e6a6a0d902086d9fab211e1f6ea25118f94d6a17d1c4885801f9b3fb3838f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
