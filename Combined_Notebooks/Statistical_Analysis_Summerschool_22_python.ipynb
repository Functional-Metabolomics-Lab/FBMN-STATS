{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55766981",
   "metadata": {
    "id": "55766981"
   },
   "source": [
    "**Updated on:** 2022-10-23 23:10:05 CEST\n",
    "\n",
    "This Notebook is used for cleaning the feature table, an output of metabolomics experiment, then performing some preliminary univariate and multivariate statistics analyses.\n",
    "\n",
    "**Authors**: Abzer Kelminal (abzer.shah@uni-tuebingen.de), Francesco Russo (frru@ssi.dk), Filip Ottosson (faot@ssi.dk), Madaleine Ernst (maet@ssi.dk), Axel Walter (axel.walter@uni-tuebingen.de), Carolina Gonzalez, Judith Boldt <br>\n",
    "**Input file format**: .csv files or .txt files <br>\n",
    "**Outputs**: .csv files, .pdf & .svg images  <br>\n",
    "**Dependencies**: pandas numpy plotly pingouin kaleido scikit-learn\n",
    "\n",
    "---\n",
    "This Notebook can be run with both Jupyter Notebook & Google Colab. To know more about how to get the Jupyter Notebook running with R code, please have a look at this document: [GitHub Link](https://github.com/Functional-Metabolomics-Lab/Jupyter-Notebook-Installation/blob/main/Anaconda%20with%20R%20kernel%20installation.pdf)\n",
    "\n",
    "---\n",
    "**Before starting to run this notebook with your own data, remember to save a copy of this notebook in your own Google Drive! Do so by clicking on File --> Save a copy in Drive. You can give whatever meaningful name to your notebook.** This file should be located in a new folder of your Google Drive named 'Colab Notebooks'. You can also download this notebook: File --> Download --> Download .ipynb.<br>\n",
    "\n",
    "---\n",
    "<b><font size=3> SPECIAL NOTE: Please read the comments before proceeding with the code and let us know if you run into any errors and if you think it could be commented better. We would highly appreciate your suggestions and comments!!</font> </b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8zlElKRwhzja",
   "metadata": {
    "id": "8zlElKRwhzja"
   },
   "source": [
    "# **About the Data**\n",
    "\n",
    "The files used in this tutorial are part of an interlab comparison study, where different laboratories around the world analysed the same environmental samples on their respective LC-MS/MS equipments. To simulate algal bloom, standardized algae extracts (A) in marine dissovled organic matter (M) at different concentrations were prepared (450 (A45M); 150 (A15M); and 50 (A5M) ppm A). Samples were then shipped to different laboratories for untargeted LC-MS/MS metabolomics analysis. The data used particularly for this notebook is from Lab 1 (Dorrestein Lab, University of California at San Diego, USA; Data submitted by Allegra Aron allegra.aron@gmail.com ) <br><br>\n",
    "(*To be edited*) In this tutorial, we are working with one of the datasets, which was acquired on a UHPLC system coupled to a Thermo Scientific Q Exactive HF Orbitrap LC-MS/MS mass spectrometer. MS/MS data were acquired in data-dependent acquisition (DDA) with fragmentation of the five most abundant ions in the spectrum per precursor scan. Data files were subsequently preprocessed using [MZmine3](http://mzmine.github.io/) and the [feature-based molecular networking workflow in GNPS](https://gnps.ucsd.edu/ProteoSAFe/status.jsp?task=d207c3a831264d61810ad69ac09b14e9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YhJRIil_cVcQ",
   "metadata": {
    "id": "YhJRIil_cVcQ"
   },
   "source": [
    "# **About the different sections in the Notebook:**\n",
    "### **1. Data-cleaning**\n",
    "\n",
    "It involves cleaning the feature table, which contains all the features (metabolites, in our case) with their corresponding intensities. The data cleanup steps involved are: 1) Blank removal 2) Imputation 3) Normalisation. Each step would be discussed in detail later. Once the data is cleaned, we can then use it for further statistical analyses.\n",
    "\n",
    "### **2. Univariate statistical analysis**\n",
    "\n",
    "Here, we will use univariate statistical methods, such as ANOVA, to investigate whether there are differences in the levels of individual features between different time points in the dataset.\n",
    "\n",
    "### **3. Unsupervised multivariate analyses:**\n",
    "#### **i. PCoA and PERMANOVA**\n",
    "Here, we will perform a Principal Coordinate Analysis (PCoA), also known as metric or classical Multidimensional Scaling (metric MDS) to explore and visualize patterns in an untargeted mass spectromtery-based metabolomics dataset. We will then assess statistical significance of the patterns and dispersion of different sample types using permutational multivariate analysis of variance (PERMANOVA).\n",
    "\n",
    "#### **ii. Cluster Analyses and Heatmaps**\n",
    "We will also perform different cluster analyses to explore patterns in the data. This will help us to discover subgroups of samples or features that share a certain level of similarity. Clustering is an example of unsupervised learning where no labels are given to the learning algorithm which will try to find patterns/structures in the input data on its own. The goal of clustering is to find these hidden patterns.<br>\n",
    "\n",
    "Some types of cluster analyses (e.g. hierarchical clustering) are often associated with heatmaps. Heatmaps are a visual representation of the data where columns are usually samples and rows are features (in our case, different metabolic features). The color scale of heatmaps indicates higher or lower intensity (for instance, blue is lower and red is higher intensity).<br>\n",
    "\n",
    "There are a lot of good videos and resources out there explaining very well the principle behind clustering. Some good ones are the following:<br>\n",
    "- Hierarchical clustering and heatmaps: https://www.youtube.com/watch?v=7xHsRkOdVwo<br>\n",
    "- K-means clustering: https://www.youtube.com/watch?v=4b5d3muPQmA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J4MfRygybbT_",
   "metadata": {
    "id": "J4MfRygybbT_"
   },
   "source": [
    "# **Questions to be asked in the Statistical analysis sections**: </br>\n",
    "**Univariate Statistical analysis:**\n",
    "*   Are metabolite levels dependent on the dilution?\n",
    "*   How does the affected metabolite change throughout the dilution series?\n",
    "*   How large are the differences? \n",
    "---\n",
    "**Unsupervised multivariate analyses: PCoA & PERMANOVA**\n",
    "*   Can we monitor algal bloom by looking at metabolomic profiles of marine dissolved organic matter?\n",
    "---\n",
    "**Cluster analysis and Heatmaps**\n",
    "- Can we monitor algal bloom by looking at metabolomic profiles of marine dissolved organic matter?\n",
    "- Are we able to group/cluster together samples derived from different concentrations of algae extracts using metabolic profiles? <br>\n",
    "- Which samples are the most similar? <br>\n",
    "- Are there any patterns defining the groups/clusters? That is, which features cluster together? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4705b9a",
   "metadata": {
    "id": "a4705b9a"
   },
   "source": [
    "# **Package installation:**\n",
    "Since we are running the notebook via Colab environment which runs completely in cloud, we need to install the packages every time we run the notebook.This might take some time to install all these packages. In case you are running the notebook directly via Jupyter Notebook IDE, you need to install the packages only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NQnoUTaU1s9_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQnoUTaU1s9_",
    "outputId": "4bff9ded-7c3f-4d28-a81d-049d19216ba8"
   },
   "outputs": [],
   "source": [
    "# Install libraries that are not preinstalled\n",
    "!pip install pandas numpy plotly scikit-learn scikit-bio pingouin kaleido ipyfilechooser nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z4eG3cNzmX-x",
   "metadata": {
    "id": "Z4eG3cNzmX-x"
   },
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "import pingouin as pg\n",
    "import skbio # Don't import on Windows!!\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d79f3-5b59-4d57-8a82-c59b92c69193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings for cleaner output, comment out for debugging\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a44d4",
   "metadata": {
    "id": "863a44d4"
   },
   "source": [
    "# **Setting a local working directory:**\n",
    "### For Google Colab Users:\n",
    "<p style='text-align: justify;'> <font color='red'>For Google Colab, it is not possible to access the files from your local computer as it is hosted on Google's cloud server. An easier workaround is to upload the necessary files into the Google colab session using the 'Files' icon on the left as shown in the image. The code in the next cell creates a new folder 'My_TestData' in the Colab space and sets the folder as working directory. Following the steps in the image, you can check in your Colab to see if the folder has been created. Once you see it, simply upload the files from your local PC to the folder 'My_TestData' and then continue running the rest of the script.</font> </p>\n",
    "\n",
    "<p style='text-align: justify;'><b>SPECIAL NOTE: All the files uploaded to Google Colab would generally disappear after 12 hours. Similarly, all the outputs would be saved only in the Colab, so we need to download them into our local system at the end of our session.</b></p> \n",
    "\n",
    "[Go to section: Getting outputs from Colab](#colab_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RtmlfVBTVeT-",
   "metadata": {
    "id": "RtmlfVBTVeT-"
   },
   "source": [
    "**Importing files into Google Colab environment:**\n",
    "![Google-Colab Files Upload](https://github.com/abzer005/Images-for-Jupyter-Notebooks/blob/main/StepsAll.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-AHDHHY6TEbI",
   "metadata": {
    "id": "-AHDHHY6TEbI"
   },
   "outputs": [],
   "source": [
    "# Get folder with data files\n",
    "result_dir = input(\"Enter path to folder for your results (or leave empty to stay in this folder):\\n\")\n",
    "if not result_dir:\n",
    "    result_dir = \".\"\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "print(f\"Results folder is: {os.path.abspath(result_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6zqeJcBh-dOD",
   "metadata": {
    "id": "6zqeJcBh-dOD"
   },
   "source": [
    "**For users running the script directly in Jupyter Notebook instead through Google Colab**, please make sure to include all the input files in one folder before running the script. Then for setting the working directory, use the below code on a new cell. When you run the cell, it will display an output box where you can enter the path of the folder containing all your input files in your local computer and it will set as your working directory<br> For ex: D:\\User\\Project\\Test_Data\n",
    "\n",
    "```\n",
    "directory = input(\"Enter the path of the folder with input files:\\n\")\n",
    "os.chdir(directory)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9546ef5-73f8-4623-877d-9c4daf3c18b9",
   "metadata": {
    "id": "e9546ef5-73f8-4623-877d-9c4daf3c18b9"
   },
   "source": [
    "# **Input files needed for the Notebook:**\n",
    "1) <b>Feature table:</b> An output of metabolomics experiment, containing all the features or peaks (LC-MS/MS peaks here) with their corresponding intensities. The feature table used in the test data is obtained by MZmine3. (Filetype: .csv file) </br> \n",
    "2) <b>Metadata:</b> Created by the user about the files used obtaining the feature table (It can be a csv/txt/tsv file). The columns in a metadata should be created with the following format: filename (1st column having all the filenames in the same order as the columns in feature table), all the other columns with column name such as: ATTRIBUTE_yourDesiredAttribute. </br>\n",
    "\n",
    "Please have a look at the metadata used here for reference. Creating a metadata in the above-mentioned format is necessary for uploading the files in GNPS and to obtain a molecular network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a31eafc",
   "metadata": {
    "id": "3a31eafc"
   },
   "source": [
    "## Reading the input data using URL:\n",
    "Here, we can directly pull **example data** files from our Functional Metabolomics GitHub page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988ffd6",
   "metadata": {
    "id": "7988ffd6"
   },
   "outputs": [],
   "source": [
    "#Reading the input data using URL \n",
    "ft_url = 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/Statistical-analysis-of-non-targeted-LC-MSMS-data/main/data/SD_BeachSurvey_GapFilled_quant.csv'\n",
    "md_url = 'https://raw.githubusercontent.com/Functional-Metabolomics-Lab/Statistical-analysis-of-non-targeted-LC-MSMS-data/main/data/20221125_Metadata_SD_Beaches_with_injection_order.txt'\n",
    "\n",
    "ft = pd.read_csv(ft_url)\n",
    "md = pd.read_csv(md_url, sep = \"\\t\").set_index(\"filename\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7200917",
   "metadata": {},
   "source": [
    "Specify your own feauture quantification and meta data table. If not, the example data will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf68eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature quantification table file location\n",
    "ft_file = \"\"\n",
    "# meta data table file location\n",
    "md_file = \"\"\n",
    "\n",
    "\n",
    "# define separators for different input file formats\n",
    "separators = {\"csv\": \",\", \"tsv\": \"\\t\", \"txt\": \"\\t\"}\n",
    "\n",
    "# read feature table\n",
    "if ft_file:\n",
    "    ft = pd.read_csv(ft_file, sep = separators[file.split(\".\")[-1]])\n",
    "else:\n",
    "    print(\"Please select a feature file and rerun this cell.\")\n",
    "# read metadata table\n",
    "if md_file:\n",
    "    md = pd.read_csv(md_file, sep = separators[file.split(\".\")[-1]]).set_index(\"filename\")\n",
    "else:\n",
    "    print(\"Please select a metavalue file and rerun this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12e24c",
   "metadata": {
    "id": "4f12e24c"
   },
   "source": [
    "Let's check if the data has been read correclty!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6gemopL1lV45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "6gemopL1lV45",
    "outputId": "899d5afa-0c4c-4fd4-d59b-3e99c46ef0ef"
   },
   "outputs": [],
   "source": [
    "print('Dimension: ',ft.shape) #gets the dimension (number of rows and columns) of ft\n",
    "ft.head() # gets the first 5 rows of ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8pl29xJVlncw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "8pl29xJVlncw",
    "outputId": "a3a7bc4b-78cd-4623-a638-e6748c5694f3"
   },
   "outputs": [],
   "source": [
    "print('Dimension: ',md.shape)\n",
    "md.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5af06-1073-414a-b306-c9f5f6bae852",
   "metadata": {
    "id": "7da5af06-1073-414a-b306-c9f5f6bae852"
   },
   "source": [
    "## **Creating Functions:**\n",
    "<p style='text-align: justify;'> Before getting into the Data cleanup steps, we have created a function that can be used later for data summarization. By creating functions, we don't have to write these big codes multiple times. Instead, we just use the function name. <font color=\"red\">The following cell in this section will not produce any outputs here. </font> The outputs will be produced when we give input variables to the function in the later sections. </p>\n",
    "\n",
    "<p style='text-align: justify;'> Using this function InsideLevels, we get an idea of the multiple levels in each of the metioned attributes in the metadata as well as the datatype of each attribute.  <font color =\"blue\"> This function takes metadata table as its input. </font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rd1O2VJ1nM1i",
   "metadata": {
    "id": "Rd1O2VJ1nM1i"
   },
   "outputs": [],
   "source": [
    "def inside_levels(df):\n",
    "    # get all the columns (equals all attributes) -> will be number of rows\n",
    "    levels = []\n",
    "    types = []\n",
    "    count = []\n",
    "    for col in df.columns:\n",
    "        types.append(type(df[col][0]))\n",
    "        levels.append(sorted(set(df[col].dropna())))\n",
    "        tmp = df[col].value_counts()\n",
    "        count.append([tmp[levels[-1][i]] for i in range(len(levels[-1]))])\n",
    "    return pd.DataFrame({\"ATTRIBUTES\": df.columns, \"LEVELS\": levels, \"COUNT\":count, \"TYPES\": types}, index=range(1, len(levels)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4502e-3f83-4c02-b5d1-cd661ee1fec1",
   "metadata": {
    "id": "8dc4502e-3f83-4c02-b5d1-cd661ee1fec1"
   },
   "source": [
    "First, let's have a look at the different conditions within each attribute of our metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d67d6-3e74-4eec-885c-06c434381110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "535d67d6-3e74-4eec-885c-06c434381110",
    "outputId": "5469c319-262e-4dae-be2d-44410514fc0b"
   },
   "outputs": [],
   "source": [
    "inside_levels(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3rHvKfUD9kAw",
   "metadata": {
    "id": "3rHvKfUD9kAw"
   },
   "source": [
    "The above table is a summary of our metadata tabel. For example, the 1st row says that there are 5 different types of sample under 'ATTRIBUTE_Sample' category namely A15M,A45M,A5M,M,PPL and the count of each of these types is 3,3,3,1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcf409-e945-480d-9501-a70da21454c9",
   "metadata": {
    "id": "0ebcf409-e945-480d-9501-a70da21454c9"
   },
   "source": [
    "# **Arranging metadata and feature table in the same order:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19073e0",
   "metadata": {
    "id": "f19073e0"
   },
   "source": [
    "<p style='text-align: justify;'> In the next cell, we are trying to bring the feature table and metadata in the correct format such as <font color =\"green\"> the rownames of metadata and column names of feature table are the same. </font> They both are the file names and they need to be the same, as from now on, we will call the columns in our feature table based on our metadata information. Thus, using the metadata, the user can filter their data easily. You can also directly deal with your feature table without metadata by getting your hands dirty with some coding!! But having a metadata improves the user-experience greatly. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abo1TVWP0iIy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "abo1TVWP0iIy",
    "outputId": "c671ed34-c077-45b9-8e6d-785569e3179d"
   },
   "outputs": [],
   "source": [
    "# structure of the original metadata file\n",
    "md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VZk46QZLx65B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "VZk46QZLx65B",
    "outputId": "957d2bbe-7aa5-4b53-d364-60c6a95a9d96"
   },
   "outputs": [],
   "source": [
    "new_md = md.copy() #storing the files under different names to preserve the original files\n",
    "# remove the (front & tail) spaces, if any present, from the rownames of md\n",
    "new_md.index = [name.strip() for name in md.index]\n",
    "# for each col in new_md\n",
    "# 1) removing the spaces (if any)\n",
    "# 2) replace the spaces (in the middle) to underscore\n",
    "# 3) converting them all to UPPERCASE\n",
    "for col in new_md.columns:\n",
    "    if new_md[col].dtype == str:\n",
    "        new_md[col] = [item.strip().replace(\" \", \"_\").upper() for item in new_md[col]]\n",
    "print('Dimension: ',new_md.shape)\n",
    "new_md.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6TtSaQC0r5Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "p6TtSaQC0r5Y",
    "outputId": "02b6e4ae-7cb1-4da3-ee0b-1911b3976bf3"
   },
   "outputs": [],
   "source": [
    "# structure of the original feature file\n",
    "ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QTb9SmFCyUee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "QTb9SmFCyUee",
    "outputId": "6bb3c4fe-f8a3-4848-d3ab-4fca4ae0f8d7"
   },
   "outputs": [],
   "source": [
    "new_ft = ft.copy() #storing the files under different names to preserve the original files\n",
    "# changing the index in feature table to contain m/z and RT information\n",
    "new_ft.index = [f\"{id}_{round(mz, 3)}_{round(rt, 3)}\" for id, mz, rt in zip(ft[\"row ID\"], ft[\"row m/z\"], ft[\"row retention time\"])]\n",
    "# drop all columns that are not mzML or mzXML file names\n",
    "new_ft.drop(columns=[col for col in new_ft.columns if \".mz\" not in col], inplace=True)\n",
    "# remove \" Peak area\" from column names\n",
    "new_ft.rename(columns={col: col.replace(\" Peak area\", \"\").strip() for col in new_ft.columns}, inplace=True)\n",
    "print('Dimension: ',new_ft.shape)\n",
    "new_ft.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i2pC4HQk2JOg",
   "metadata": {
    "id": "i2pC4HQk2JOg"
   },
   "source": [
    "Checking the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yU8dVGu-J5i8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yU8dVGu-J5i8",
    "outputId": "03da5ed7-bff9-41e4-cc13-074e1d4855b1"
   },
   "outputs": [],
   "source": [
    "# check if new_ft column names and md row names are the same\n",
    "if sorted(new_ft.columns) == sorted(new_md.index):\n",
    "    print(f\"All {len(new_ft.columns)} files are present in both new_md & new_ft.\")\n",
    "else:\n",
    "    print(\"Not all files are present in both new_md & new_ft.\\n\")\n",
    "    # print the md rows / ft column which are not in ft columns / md rows and remove them\n",
    "    ft_cols_not_in_md = [col for col in new_ft.columns if col not in new_md.index]\n",
    "    print(f\"These {len(ft_cols_not_in_md)} columns of feature table are not present in metadata table and will be removed:\\n{', '.join(ft_cols_not_in_md)}\\n\")\n",
    "    new_ft.drop(columns=ft_cols_not_in_md, inplace=True)\n",
    "    md_rows_not_in_ft = [row for row in new_md.index if row not in new_ft.columns]\n",
    "    print(f\"These {len(md_rows_not_in_ft)} rows of metadata table are not present in feature table and will be removed:\\n{', '.join(md_rows_not_in_ft)}\\n\")\n",
    "    new_md.drop(md_rows_not_in_ft, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8phJpNw5QtS",
   "metadata": {
    "id": "e8phJpNw5QtS"
   },
   "outputs": [],
   "source": [
    "new_ft = new_ft.reindex(sorted(new_ft.columns), axis=1) #ordering the ft by its column names\n",
    "new_md.sort_index(inplace=True) #ordering the md by its row names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881877f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "881877f6",
    "outputId": "28661b4d-f71f-4e9b-b370-148cd713b074"
   },
   "outputs": [],
   "source": [
    "# checking the dimensions of our new ft and md\n",
    "print(f\"The number of rows and columns in our original ft is: {ft.shape}\")\n",
    "print(f\"The number of rows and columns in our new ft is: {new_ft.shape}\")\n",
    "print(f\"The number of rows and columns in our original md is: {md.shape}\")\n",
    "print(f\"The number of rows and columns in our new md is: {new_md.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594448",
   "metadata": {
    "id": "ed594448"
   },
   "source": [
    "Notice that the number of columns of feature table is same as the number of rows in our metadata. Now, we have both our feature table and metadata in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01979e55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01979e55",
    "outputId": "14117312-6f02-4aee-c128-ee697f9bdca3"
   },
   "outputs": [],
   "source": [
    "#checking if they the files are in the same order\n",
    "list(new_ft.columns) == list(new_md.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927cf45",
   "metadata": {
    "id": "e927cf45"
   },
   "source": [
    "Lets check the files once again!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d8a48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "499d8a48",
    "outputId": "0c8a65b4-f889-464d-944d-78c349352cc3"
   },
   "outputs": [],
   "source": [
    "print('Dimension: ',new_ft.shape)\n",
    "new_ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6027aea7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "6027aea7",
    "outputId": "0db79084-a58a-46a8-efe0-5e85d459ca67"
   },
   "outputs": [],
   "source": [
    "print('Dimension: ',new_md.shape)\n",
    "new_md.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45b589",
   "metadata": {
    "id": "4f45b589"
   },
   "source": [
    "# Splitting the data into Blanks and Samples using Metadata:\n",
    "<a id=\"data_split\"></a>\n",
    "\n",
    "For the first step: Blank removal, we need to split the data as spectra obtained from blanks and samples respectively using the metadata. More about Blank removal in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gY83qEwDat9i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "gY83qEwDat9i",
    "outputId": "3b0c5eba-ac0b-4643-ab0e-c1e55899d3bc"
   },
   "outputs": [],
   "source": [
    "inside_levels(new_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e5128-1077-4413-9c7f-eb044d2e4993",
   "metadata": {
    "id": "f51e5128-1077-4413-9c7f-eb044d2e4993"
   },
   "source": [
    "In case we want to remove certain files of a particular condition, for ex: ATTRIBUTE_sample = \"M\", we can subset them out of our dataframe using the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DZq0h99Na59R",
   "metadata": {
    "id": "DZq0h99Na59R"
   },
   "outputs": [],
   "source": [
    "# subset_data = new_md[new_md['ATTRIBUTE_Sample']!='M']\n",
    "# print('Dimension: ',subset_data.shape)\n",
    "# inside_levels(subset_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe2ef5",
   "metadata": {
    "id": "68fe2ef5"
   },
   "source": [
    "Once we subset the data, we can further proceed to split the blanks from the sample in the cell below. If no subsetting is involved, you can simply split your metadata into blank and sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maMkno4qbEET",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "maMkno4qbEET",
    "outputId": "329a36f2-d04e-4739-aeca-98e97465ea65"
   },
   "outputs": [],
   "source": [
    "#If subset_data exists, it will take it as \"data\", else take new_md as \"data\"\n",
    "if 'subset_data' in locals():\n",
    "    data = subset_data\n",
    "else:\n",
    "    data = new_md\n",
    "display(inside_levels(data))\n",
    "\n",
    "condition = int(input(\"Enter the index number of the attribute to split sample and blank: \"))\n",
    "df = pd.DataFrame({\"LEVELS\": inside_levels(data).iloc[condition-1][\"LEVELS\"]})\n",
    "df.index = [*range(1, len(df)+1)]\n",
    "display(df)\n",
    "\n",
    "#Among the shown levels of an attribute, select the ones to keep\n",
    "blank_id = int(input(\"Enter the index number of your BLANK: \"))\n",
    "print('Your chosen blank is: ', df['LEVELS'][blank_id])\n",
    "\n",
    "#Splitting the data into blanks and samples based on the metadata\n",
    "md_blank = data[data[inside_levels(data)['ATTRIBUTES'][condition]] == df['LEVELS'][blank_id]]\n",
    "blank = new_ft[list(md_blank.index)]\n",
    "md_samples = data[data[inside_levels(data)['ATTRIBUTES'][condition]] != df['LEVELS'][blank_id]]\n",
    "samples = new_ft[list(md_samples.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zi5coduWb7w3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "Zi5coduWb7w3",
    "outputId": "5f4fb012-238d-4059-cdb1-9318d37ea028"
   },
   "outputs": [],
   "source": [
    "# Display the chosen blank\n",
    "print('Dimension: ',blank.shape)\n",
    "blank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FOxnCjr2LSJl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "FOxnCjr2LSJl",
    "outputId": "b22b12d5-7249-487f-8fab-ff7465b1a3b3"
   },
   "outputs": [],
   "source": [
    "# Display the chosen samples\n",
    "print('Dimension: ',samples.shape)\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ce796",
   "metadata": {
    "id": "105ce796"
   },
   "source": [
    "**Now that we have our data ready, we can start with the cleanup steps!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43099574",
   "metadata": {
    "id": "43099574",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Step1: Blank Removal\n",
    "\n",
    "<p style='text-align: justify;'> In LC-MS/MS, we use solvents called Blanks which are usually injected time-to-time to prevent carryover of the sample. The features coming from these Blanks would also be detected by LC-MS/MS instrument. Our goal here is to remove these features from our samples. The other blanks that can be removed are: Signals coming from growth media alone in terms of microbial growth experiment, signals from the solvent used for extraction methods and so on. Therefore, it is best practice to measure mass spectra of these blanks as well in addition to your sample spectra. </p>\n",
    "\n",
    "**How do we remove these blank features?** </br> \n",
    "<p style='text-align: justify;'> Since we have the feature table split into Control blanks and Sample groups now, we can compare blanks to the sample to identify the background features coming from blanks. A common filtering method is to use a cutoff to remove features that are not present sufficient enough in our biological samples. </p>\n",
    "\n",
    "The steps followed in the next few cells are:\n",
    "1. <p style='text-align: justify;'> We find an average for all the feature intensities in your blank set and sample set. Therefore, for n no.of features in a blank or sample set, we get n no.of averaged features. </p>\n",
    "2. <p style='text-align: justify;'> Next, we get a ratio of this average_blanks vs average_sample. This ratio Blank/sample tells us how much of that particular feature of a sample gets its contribution from blanks. If it is more than 30% (or Cutoff as 0.3), we consider the feature as noise. </p>\n",
    "3. <p style='text-align: justify;'> The resultant information (if ratio > Cutoff or not) is stored in a bin such as 1 = Noise or background signal, 0 = Feature Signal</p>\n",
    "4. <p style='text-align: justify;'> We count the no.of features in the bin that satisfies the condition ratio > cutoff, and consider those features as 'noise or background features' and remove them. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaf47e",
   "metadata": {
    "id": "74eaf47e"
   },
   "source": [
    "**<font color='red'> The Cutoff used to obtain the all the files in MZmine Results folder is 0.3 </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uPMPshn5dXn3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "uPMPshn5dXn3",
    "outputId": "96309317-f4ce-40eb-e3f6-0d1005a3fcc1"
   },
   "outputs": [],
   "source": [
    "blank_removal = samples.copy()\n",
    "if (input(\"Do you want to perform Blank Removal- Y/N: \").upper()==\"Y\"):\n",
    "    \n",
    "    # When cutoff is low, more noise (or background) detected; With higher cutoff, less background detected, thus more features observed\n",
    "    cutoff = float(input(\"Enter Cutoff value between 0.1 & 1 (Ideal cutoff range: 0.1-0.3): \")) # (i.e. 10% - 100%). Ideal cutoff range: 0.1-0.3\n",
    "    \n",
    "    # Getting mean for every feature in blank and Samples\n",
    "    avg_blank = blank.mean(axis=1, skipna=False) # set skipna = False do not exclude NA/null values when computing the result.\n",
    "    avg_samples = samples.mean(axis=1, skipna=False)\n",
    "\n",
    "    # Getting the ratio of blank vs samples\n",
    "    ratio_blank_samples = (avg_blank+1)/(avg_samples+1)\n",
    "\n",
    "    # Create an array with boolean values: True (is a real feature, ratio<cutoff) / False (is a blank, background, noise feature, ratio>cutoff)\n",
    "    is_real_feature = (ratio_blank_samples<cutoff)\n",
    "\n",
    "    # Checking if there are any NA values present. Having NA values in the 4 variables will affect the final dataset to be created\n",
    "    temp_NA_Count = pd.concat([avg_blank, avg_samples, ratio_blank_samples, is_real_feature], \n",
    "                            keys=['avg_blank', 'avg_samples', 'ratio_blank_samples', 'bg_bin'], axis = 1)\n",
    "    \n",
    "    print('No. of NA values in the following columns: ')\n",
    "    display(pd.DataFrame(temp_NA_Count.isna().sum(), columns=['NA']))\n",
    "\n",
    "    # Calculating the number of background features and features present (sum(bg_bin) equals number of features to be removed)\n",
    "    print(f\"No. of Background or noise features: {len(samples)-sum(is_real_feature)}\")\n",
    "    print(f\"No. of features after excluding noise: {sum(is_real_feature)}\")\n",
    "\n",
    "    blank_removal = samples[is_real_feature.values]\n",
    "    # save to file\n",
    "    blank_removal.to_csv(os.path.join(result_dir, \"Blanks_Removed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cbd3f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "81cbd3f6",
    "outputId": "993f7bfa-d414-45e0-d905-0f60a4726e00"
   },
   "outputs": [],
   "source": [
    "print('Dimension: ',blank_removal.shape)\n",
    "display(blank_removal.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4210007",
   "metadata": {
    "id": "b4210007",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Step 2: Imputation\n",
    "\n",
    "<p style='text-align: justify;'> For several reasons, real world datasets might have some missing values in it, in the form of NA, NANs or 0s. Eventhough the gapfilling step of MZmine fills the missing values, we still end up with some missing values or 0s in our feature table. This could be problematic for statistical analysis. </p> \n",
    "<p style='text-align: justify;'> In order to have a better dataset, we cannot simply discard those rows or columns with missing values as we will lose a chunk of our valuable data. Instead we can try imputing those missing values. Imputation involves replacing the missing values in the data with a meaningful, reasonable guess. There are several methods, such as: </p> \n",
    "  \n",
    "1) Mean imputation (replacing the missing values in a column with the mean or average of the column)  \n",
    "2) Replacing it with the most frequent value  \n",
    "3) Several other machine learning imputation methods such as k-nearest neighbors algorithm(k-NN), Hidden Markov Model(HMM)\n",
    "\n",
    "Here, we use ft and see the frquency distribution of its features with a plot. It shows where the features are present in higher number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JwFJ1lrlf_1S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "JwFJ1lrlf_1S",
    "outputId": "1597f89b-f70b-40dd-dd5f-89ae2aefa1cc"
   },
   "outputs": [],
   "source": [
    "bins, bins_label, a = [-1, 0, 1, 10], ['-1','0', \"1\", \"10\"], 2\n",
    "\n",
    "while a<=10:\n",
    "    bins_label.append(np.format_float_scientific(10**a))\n",
    "    bins.append(10**a)\n",
    "    a+=1\n",
    "\n",
    "freq_table = pd.DataFrame(bins_label)\n",
    "frequency = pd.DataFrame(np.array(np.unique(np.digitize(blank_removal.to_numpy(), bins, right=True), return_counts=True)).T).set_index(0)\n",
    "freq_table = pd.concat([freq_table,frequency], axis=1).fillna(0).drop(0)\n",
    "freq_table.columns = ['intensity', 'Frequency']\n",
    "freq_table['Log(Frequency)'] = np.log(freq_table['Frequency']+1)\n",
    "\n",
    "# get the lowest intensity (that is not zero) as a cutoff LOD value\n",
    "cutoff_LOD = round(blank_removal.replace(0, np.nan).min(numeric_only=True).min())\n",
    "\n",
    "fig = px.bar(freq_table, x=\"intensity\", y=\"Log(Frequency)\", template=\"plotly_white\",  width=600, height=400)\n",
    "\n",
    "fig.update_traces(marker_color=\"#696880\")\n",
    "fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                  title={\"text\":\"FEATURE INTENSITY - FREQUENCY PLOT\", 'x':0.5, \"font_color\":\"#3E3D53\"})\n",
    "fig.write_image(os.path.join(result_dir, \"frequency_plot.svg\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aE9z6OQZhLbn",
   "metadata": {
    "id": "aE9z6OQZhLbn"
   },
   "source": [
    "A random number between this minimum value and zero will be used for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0rBzEjIUhu-R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "0rBzEjIUhu-R",
    "outputId": "97fa5bbc-8608-4537-a285-6f6dfe0a8f36"
   },
   "outputs": [],
   "source": [
    "imputed = blank_removal.copy()\n",
    "if(input(\"Do you want to perform Imputation? - Y/N: \").upper()==\"Y\"):\n",
    "    #imputed.replace(0, np.random.randint(0, cutoff_LOD), inplace=True)\n",
    "    imputed = imputed.apply(lambda x: [np.random.randint(0, cutoff_LOD) if v == 0 else v for v in x])\n",
    "    print('Dimension: ',imputed.shape)\n",
    "    display(imputed)\n",
    "    # save to file\n",
    "    imputed.to_csv(os.path.join(result_dir, f\"Imputed_QuantTable.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tnSOkAjHArKs",
   "metadata": {
    "id": "tnSOkAjHArKs"
   },
   "source": [
    "Too many missing values is problematic for statistical analyses. Here we calculate the proportion of missing values (coded as the value of the cutoff_LOD) and display the proportions in a histogram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce521719",
   "metadata": {},
   "source": [
    "TODO move plot up before imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06048f4f-1a77-4c34-ae24-830267817986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of missing values per feature in a histogram\n",
    "n_zeros = imputed.T.apply(lambda x: sum(x<=cutoff_LOD))\n",
    "\n",
    "fig = px.histogram(n_zeros, template=\"plotly_white\",  \n",
    "                   width=600, height=400)\n",
    "\n",
    "fig.update_traces(marker_color=\"#696880\")\n",
    "fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                  title={\"text\":\"MISSING VALUES PER FEATURE\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "                  xaxis_title=\"number of missing values\", yaxis_title=\"count\", showlegend=False)\n",
    "fig.write_image(os.path.join(result_dir, \"number_of_missing_values_per_feature.svg\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a2dd7",
   "metadata": {
    "id": "f06a2dd7"
   },
   "source": [
    "# Step 3 Normalization\n",
    "The following code performs sample-centric (column-wise) normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OJ0p875Aj_F9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "OJ0p875Aj_F9",
    "outputId": "61f12e83-9c4c-41e0-9f23-70d0ec0ed2ec"
   },
   "outputs": [],
   "source": [
    "normalized = imputed.copy()\n",
    "if(input(\"Do you want to perform Normalization? - Y/N: \").upper()==\"Y\"):\n",
    "    # Dividing each element of a particular column with its column sum\n",
    "    normalized = normalized.apply(lambda x: x/np.sum(x), axis=0)\n",
    "    \n",
    "    # save to file\n",
    "    normalized.to_csv(os.path.join(result_dir, \"Normalised_Quant_table.csv\"))\n",
    "    \n",
    "    print('Dimension: ', normalized.shape)\n",
    "    display(normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w7rYinojzPVx",
   "metadata": {
    "id": "w7rYinojzPVx"
   },
   "source": [
    "# Step 4: Transposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bwSzUDgomTnm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "bwSzUDgomTnm",
    "outputId": "be6c23da-a59c-4cfb-cba2-5f62d792c5ae"
   },
   "outputs": [],
   "source": [
    "# transposing the imputed table before scaling\n",
    "transposed = imputed.T\n",
    "print(f'Imputed feature table rows/columns: {transposed.shape}')\n",
    "display(transposed.head(3))\n",
    "# put the rows in the feature table and metadata in the same order\n",
    "transposed.sort_index(inplace=True)\n",
    "md_samples.sort_index(inplace=True)\n",
    "try:\n",
    "    print(md_samples.index == transposed.index) # should be all True\n",
    "except:\n",
    "    print(\"WARNING: Sample names in feature and metadata table are NOT the same!\")\n",
    "transposed.to_csv(os.path.join(result_dir, \"Imputed_QuantTable_transposed.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c183c7-cdfb-4d60-a02e-7bd880987104",
   "metadata": {},
   "source": [
    "# Step 5: Scaling\n",
    "For statistics normalization should happen across the complete dataframe via scaling and centering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L3U41ypivQ4Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "L3U41ypivQ4Z",
    "outputId": "589177a2-2ae1-42e7-8bca-0e9ec51d1703"
   },
   "outputs": [],
   "source": [
    "print(f\"Proportion of the dataset that consists of missing values (coded as {cutoff_LOD}): {((transposed<=cutoff_LOD).to_numpy()).mean()}\")\n",
    "print(f\"\\nMetabolites with measurements in at least 50 % of the samples: {(((n_zeros/transposed.shape[0])<=0.5).to_numpy()).mean()}\")\n",
    "\n",
    "# Deselect metabolites with more than 50 % missing values. This helps to get rid of features that are present in too few samples to conduct proper statistical tests\n",
    "data_filtered = transposed[transposed.columns[(n_zeros/transposed.shape[0])<0.5]]\n",
    "\n",
    "# scale filtered data\n",
    "scaled = pd.DataFrame(StandardScaler().fit_transform(data_filtered), index=data_filtered.index, columns=data_filtered.columns)\n",
    "scaled.to_csv(os.path.join(result_dir, \"Imputed_Scaled_QuantTable.csv\"))\n",
    "\n",
    "# Merge feature table and metadata to one dataframe:\n",
    "# \"how=inner\" performs an inner join (only the filenames that appear in md_samples and data are kept)\n",
    "data = pd.merge(md_samples, scaled, left_index=True, right_index=True, how=\"inner\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DRHowqAkApBe",
   "metadata": {
    "id": "DRHowqAkApBe"
   },
   "source": [
    "# Univariate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LKPTigNdTGyN",
   "metadata": {
    "id": "LKPTigNdTGyN"
   },
   "source": [
    "**Run ANOVA** <br>\n",
    "\n",
    "We now use the function anova from the pingouin library to run the ANOVA. Since one ANOVA is being run for each metabolite feature, we run the analyses in a loop and save the output for each feature in a list called anova_out.<br>\n",
    "\n",
    "The vector a indicates which columns in the dataset are features (i.e. from column 5 to the last column of the data frame). <br>\n",
    "\n",
    "We can run a for loop to pass each feature column into the first argument of the aov function, while the second argument, time point, is constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MDCZdylZywGX",
   "metadata": {
    "id": "MDCZdylZywGX"
   },
   "outputs": [],
   "source": [
    "# select an attribute to perform ANOVA\n",
    "anova_attribute = 'ATTRIBUTE_Sample_Area'\n",
    "\n",
    "def gen_anova_data(df, columns, groups_col):\n",
    "    for col in columns:\n",
    "        result = pg.anova(data=df, dv=col, between=groups_col, detailed=True).set_index('Source')\n",
    "        p = result.loc[groups_col, 'p-unc']\n",
    "        f = result.loc[groups_col, 'F']\n",
    "        yield col, p, f\n",
    "\n",
    "dtypes = [('metabolite', 'U100'), ('p', 'f'), ('F', 'f')]\n",
    "anova = pd.DataFrame(np.fromiter(gen_anova_data(data, scaled.columns, anova_attribute), dtype=dtypes))\n",
    "anova"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OKKiBVxxTndL",
   "metadata": {
    "id": "OKKiBVxxTndL"
   },
   "source": [
    "The following is of interest:\n",
    "*   Feature ID (column 'metabolite')\n",
    "*   p-value for ANOVA\n",
    "*   p-value after taking multiple tests into consideration\n",
    "*   F-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xRLmgVnq5MOI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 720
    },
    "id": "xRLmgVnq5MOI",
    "outputId": "d52e23fa-b234-4a40-d6ee-7ee5a936871e"
   },
   "outputs": [],
   "source": [
    "# add Bonferroni corrected p-values for multiple testing correction\n",
    "if 'p_bonferroni' not in anova.columns:\n",
    "    anova.insert(2, 'p_bonferroni', pg.multicomp(anova['p'], method='bonf')[1])\n",
    "# add significance\n",
    "if 'significant' not in anova.columns:\n",
    "    anova.insert(3, 'significant', anova['p_bonferroni'] < 0.05)\n",
    "# sort by p-value\n",
    "anova.sort_values('p', inplace=True)\n",
    "# save ANOVA table\n",
    "anova.to_csv(os.path.join(result_dir, 'ANOVA_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P9OMCkdmVrNY",
   "metadata": {
    "id": "P9OMCkdmVrNY"
   },
   "source": [
    "**Plot ANOVA results**\n",
    "\n",
    "We will use plotly to visualize results from the ANOVA, with log(F-values) on the x-axis and -log(p) on the y-axis. Features are colored after statistical significance after multiple test correction. Since there are large differences in the F- and p-values, it is easier to plot their log.\n",
    "\n",
    "We can also display the names of some of the top features in the plot. This easily gets very cluttered if we decide to display too many names, so starting at the top 5 could be a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OwSyax7p-IPk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "OwSyax7p-IPk",
    "outputId": "e8833fb5-4b71-4b87-a6e0-d58e7158837a"
   },
   "outputs": [],
   "source": [
    "# first plot insignificant features\n",
    "fig = px.scatter(x=anova[anova['significant'] == False]['F'].apply(np.log),\n",
    "                y=anova[anova['significant'] == False]['p'].apply(lambda x: -np.log(x)),\n",
    "                template='plotly_white', width=600, height=600)\n",
    "fig.update_traces(marker_color=\"#696880\")\n",
    "\n",
    "# plot significant features\n",
    "fig.add_scatter(x=anova[anova['significant']]['F'].apply(np.log),\n",
    "                y=anova[anova['significant']]['p'].apply(lambda x: -np.log(x)),\n",
    "                mode='markers+text',\n",
    "                text=anova['metabolite'].iloc[:4],\n",
    "                textposition='top left', textfont=dict(color='#ef553b', size=7), name='significant')\n",
    "\n",
    "fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                  title={\"text\":\"ANOVA - FEATURE SIGNIFICANCE\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "                  xaxis_title=\"log(F)\", yaxis_title=\"-log(p)\", showlegend=False)\n",
    "\n",
    "# save fig as pdf\n",
    "fig.write_image(os.path.join(result_dir, \"plot_ANOVA.pdf\"), scale=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pc0VaEmlK90g",
   "metadata": {
    "id": "pc0VaEmlK90g"
   },
   "outputs": [],
   "source": [
    "# boxplots with top 4 metabolites from ANOVA\n",
    "for metabolite in anova.sort_values('p_bonferroni').iloc[:4, 0]:\n",
    "    fig = px.box(data, x=anova_attribute, y=metabolite, color=anova_attribute)\n",
    "    fig.update_layout(showlegend=False, title=metabolite, xaxis_title=\"\", yaxis_title=\"intensity\", template=\"plotly_white\", width=500)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y6E_i2aqeevQ",
   "metadata": {
    "id": "y6E_i2aqeevQ"
   },
   "source": [
    "**Tukey's post hoc test:**\n",
    "\n",
    "Features with both positive and negative time trends exist, but differences appear to the be largest between timepoint 0 and 45. Tukey's post hoc test can be used to calculate which features show statistically significant differences between these two timepoints."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c11ca11c",
   "metadata": {},
   "source": [
    "TODO: modify to compare every location with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdFyHQTwS1I1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "qdFyHQTwS1I1",
    "outputId": "4c9bc838-cc0c-443b-9b50-d00d376e628a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def gen_pairwise_tukey(df, time_points, metabolites):\n",
    "#     \"\"\" Yield results for pairwise Tukey test for all metabolites between start and end time points.\"\"\"\n",
    "#     for metabolite in metabolites:\n",
    "#         df_for_tukey = df.iloc[np.where(data['ATTRIBUTE_Sample_Area'].isin([time_points[0], time_points[-1]]))][[metabolite, 'ATTRIBUTE_Sample_Area']]\n",
    "#         tukey = pg.pairwise_tukey(df_for_tukey, dv=metabolite, between='ATTRIBUTE_Sample_Area')\n",
    "#         yield metabolite, int(metabolite.split('_')[0]), tukey['diff'], tukey['p-tukey']\n",
    "\n",
    "# dtypes = [('stats_metabolite', 'U100'), ('stats_ID', 'i'), ('stats_diff', 'f'), ('stats_p', 'f')]\n",
    "# tukey = pd.DataFrame(np.fromiter(gen_pairwise_tukey(data, time_points, anova[anova['significant']]['metabolite']), dtype=dtypes))\n",
    "# # add Bonferroni corrected p-values\n",
    "# tukey.insert(4, 'stats_p_bonferroni', pg.multicomp(tukey['stats_p'], method='bonf')[1])\n",
    "# # add significance\n",
    "# tukey.insert(5, 'stats_significant', tukey['stats_p_bonferroni'] < 0.05)\n",
    "# # sort by p-value\n",
    "# tukey.sort_values('stats_p', inplace=True)\n",
    "\n",
    "# # write output to csv file\n",
    "# tukey.to_csv(os.path.join(result_dir, 'TukeyHSD_output.csv'))\n",
    "\n",
    "# display(tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7MAZmD0Ve2LJ",
   "metadata": {
    "id": "7MAZmD0Ve2LJ"
   },
   "source": [
    "Create a volcano plot that displays -log(p) on the y-axis and group-difference on the x-axis. Again, display names of top findings in the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CFYymstHflaO",
   "metadata": {
    "id": "CFYymstHflaO"
   },
   "source": [
    "Create a volcano plot that displays -log(p) on the y-axis and group-difference on the x-axis. Again, display names of top findings in the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6uWEoGWS_bH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "i6uWEoGWS_bH",
    "outputId": "d0a1228f-66e8-4b0f-e758-0196ed39f77d"
   },
   "outputs": [],
   "source": [
    "# # create figure\n",
    "# fig = px.scatter(template='plotly_white', width=600, height=600)\n",
    "\n",
    "# # plot insignificant values\n",
    "# fig.add_trace(go.Scatter(x=tukey[tukey['stats_significant'] == False]['stats_diff'],\n",
    "#                          y=tukey[tukey['stats_significant'] == False]['stats_p'].apply(lambda x: -np.log(x)),\n",
    "#                          mode='markers', marker_color='#696880', name='insignificant'))\n",
    "\n",
    "# # plot significant values\n",
    "# fig.add_trace(go.Scatter(x=tukey[tukey['stats_significant']]['stats_diff'],\n",
    "#                          y=tukey[tukey['stats_significant']]['stats_p'].apply(lambda x: -np.log(x)),\n",
    "#                          mode='markers+text', text=anova['metabolite'].iloc[:4], textposition='top left', \n",
    "#                          textfont=dict(color='#ef553b', size=8), marker_color='#ef553b', name='significant'))\n",
    "\n",
    "# fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "#                   title={\"text\":\"TUKEY - FEATURE DIFFERENCE\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "#                   xaxis_title=\"stats_diff\", yaxis_title=\"-log(p)\")\n",
    "\n",
    "# # save image as pdf\n",
    "# fig.write_image(os.path.join(result_dir, \"TukeyHSD_group0vsgroup45.pdf\"), scale=3)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NghlJ7Tif7ml",
   "metadata": {
    "id": "NghlJ7Tif7ml"
   },
   "source": [
    "As a sanity check we can check a few of the top metabolites by plotting them in a boxplot. Just change the input argument for y to match a name in the result list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PtUUyGejf5Te",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "PtUUyGejf5Te",
    "outputId": "eb2d2896-c69e-4759-ac76-b6d82e7f3744"
   },
   "outputs": [],
   "source": [
    "# def metabolite_boxplot(metabolite):\n",
    "#     p_value = anova.set_index('metabolite')._get_value(metabolite, \"p\")\n",
    "#     df = data[['ATTRIBUTE_Sample_Area', metabolite]]\n",
    "#     title = str(metabolite)+\"<br>p-value:\"+str(p_value)\n",
    "#     fig = px.box(df, x='ATTRIBUTE_Sample_Area', y=metabolite, template='plotly_white',\n",
    "#                  width=800, height=600, points='all', color='ATTRIBUTE_Sample_Area')\n",
    "\n",
    "#     fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "#                       title={\"text\":title, 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "#                       xaxis_title=\"time point\", yaxis_title=\"intensity scales and centered\")\n",
    "#     fig.show()\n",
    "#     return None\n",
    "\n",
    "# # Add dropdown to create boxplot of metabolite selected (just for significant=True)\n",
    "# interact(metabolite_boxplot, metabolite=sorted(list(anova[\"metabolite\"][anova[\"significant\"]==True])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DlmsH5pz7Ez0",
   "metadata": {
    "id": "DlmsH5pz7Ez0"
   },
   "source": [
    "# PCoA PermANOVA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDIj3G4ugpxA",
   "metadata": {
    "id": "fDIj3G4ugpxA"
   },
   "source": [
    "Principal coordinates analysis (PCoA)\n",
    "\n",
    "Principal coordinates analysis (PCoA) is a metric multidimensional scaling (MDS) method that attempts to represent sample dissimilarities in a low-dimensional space. It converts a distance matrix consisting of pair-wise distances (dissimilarities) across samples into a 2- or 3-D graph (Gower, 2005). Different distance metrics can be used to calculate dissimilarities among samples (e.g. Euclidean, Canberra, Minkowski). Performing a principal coordinates analysis using the Euclidean distance metric is the same as performing a principal components analysis (PCA). The selection of the most appropriate metric depends on the nature of your data and assumptions made by the metric.\n",
    "\n",
    "Within the metabolomics field the Euclidean, Bray-Curtis, Jaccard or Canberra distances are most commonly used. The Jaccard distance is an unweighted metric (presence/absence) whereas Euclidean, Bray-Curtis and Canberra distances take into account relative abundances (weighted). Some metrics may be better suited for very sparse data (with many zeroes) than others. For example, the Euclidean distance metric is not recommended to be used for highly sparse data.\n",
    "\n",
    "This video tutorial by StatQuest summarizes nicely the basic principles of PCoA: https://www.youtube.com/watch?v=GEn-_dAyYME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131844f-a392-4b91-9994-a0a87d0113de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating Principal components\n",
    "n = 10\n",
    "pca = PCA(n_components=n)\n",
    "pca_df = pd.DataFrame(data = pca.fit_transform(scaled), columns = [f'PC{x}' for x in range(1, n+1)])\n",
    "pca_df.index = md_samples.index\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a scree plot showing the variance of each PC in percentage:\n",
    "percent_variance = np.round(pca.explained_variance_ratio_* 100, decimals =2)\n",
    "\n",
    "fig_bar = px.bar(x=pca_df.columns, y=percent_variance, template=\"plotly_white\",  width=500, height=400)\n",
    "fig_bar.update_traces(marker_color=\"#696880\", width=0.5)\n",
    "fig_bar.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                    title={\"text\":\"PCA - VARIANCE\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "                    xaxis_title=\"principal component\", yaxis_title=\"variance (%)\")\n",
    "fig_bar.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f558f90",
   "metadata": {},
   "source": [
    "TODO make the attibute colors work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c798bf-4c4f-409d-8d4d-d963713fd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pca_scatter_plot(attribute):\n",
    "    title = f'PRINCIPLE COMPONENT ANALYSIS'\n",
    "\n",
    "    df = pd.merge(pca_df[['PC1', 'PC2']], md_samples[attribute].apply(str), left_index=True, right_index=True)\n",
    "\n",
    "    fig = px.scatter(df, x='PC1', y='PC2', template='plotly_white', width=600, height=400, color=attribute)\n",
    "\n",
    "    fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                      title={\"text\":title, 'x':0.2, \"font_color\":\"#3E3D53\"},\n",
    "                      xaxis_title=f'PC1 {round(pca.explained_variance_ratio_[0]*100, 1)}%',\n",
    "                      yaxis_title=f'PC2 {round(pca.explained_variance_ratio_[1]*100, 1)}%')\n",
    "    display(fig)\n",
    "\n",
    "interact(pca_scatter_plot, attribute=sorted(md_samples.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f762eb82",
   "metadata": {},
   "source": [
    "TODO fix the interact thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd99f21-5ff9-4766-ae37-982d0ef956b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcoa(attribute, distance_matrix):\n",
    "    # Create the distance matrix from the original data\n",
    "    distance_matrix = skbio.stats.distance.DistanceMatrix(distance.squareform(distance.pdist(scaled.values, distance_matrix)))\n",
    "    # perform PERMANOVA test\n",
    "    permanova = skbio.stats.distance.permanova(distance_matrix, md_samples[attribute])\n",
    "    permanova['R2'] = 1 - 1 / (1 + permanova['test statistic'] * permanova['number of groups'] / (permanova['sample size'] - permanova['number of groups'] - 1))\n",
    "    display(permanova)\n",
    "    # perfom PCoA\n",
    "    pcoa = skbio.stats.ordination.pcoa(distance_matrix)\n",
    "    df = pcoa.samples[['PC1', 'PC2']]\n",
    "    df = df.set_index(md_samples.index)\n",
    "    df = pd.merge(df[['PC1', 'PC2']], md_samples[attribute].apply(str), left_index=True, right_index=True)\n",
    "    \n",
    "    title = f'PRINCIPLE COORDINATE ANALYSIS'\n",
    "    fig = px.scatter(df, x='PC1', y='PC2', template='plotly_white', width=600, height=400, color=attribute)\n",
    "\n",
    "    fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                      title={\"text\":title, 'x':0.18, \"font_color\":\"#3E3D53\"},\n",
    "                      xaxis_title=f'PC1 {round(pcoa.proportion_explained[0]*100, 1)}%',\n",
    "                      yaxis_title=f'PC2 {round(pcoa.proportion_explained[1]*100, 1)}%')\n",
    "    fig.show()\n",
    "    \n",
    "    # To get a scree plot showing the variance of each PC in percentage:\n",
    "    percent_variance = np.round(pcoa.proportion_explained* 100, decimals =2)\n",
    "\n",
    "    fig = px.bar(x=[f'PC{x}' for x in range(1, len(pcoa.proportion_explained)+1)], y=percent_variance, template=\"plotly_white\",  width=500, height=400)\n",
    "    fig.update_traces(marker_color=\"#696880\", width=0.5)\n",
    "    fig.update_layout(font={\"color\":\"grey\", \"size\":12, \"family\":\"Sans\"},\n",
    "                      title={\"text\":\"PCoA - VARIANCE\", 'x':0.5, \"font_color\":\"#3E3D53\"},\n",
    "                      xaxis_title=\"principal component\", yaxis_title=\"variance (%)\")#\n",
    "    fig.show()\n",
    "\n",
    "matrices = ['canberra', 'chebyshev', 'correlation', 'cosine', 'euclidean', 'hamming', 'jaccard', 'matching', 'minkowski', 'seuclidean']\n",
    "interact(pcoa, attribute=sorted(md_samples.columns), distance_matrix=matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b774363-4d57-408b-bf42-25281f2f6017",
   "metadata": {},
   "source": [
    "# Hierarchial Clustering Algorithm:\n",
    "\n",
    "We are now ready to perform a cluter analysis. The concept behind hierarchical clustering is to repeatedly combine the two nearest clusters into a larger cluster.\n",
    "\n",
    "The first step consists of calculating the distance between every pair of observation points and stores it in a matrix;\n",
    "1. It puts every point in its own cluster;\n",
    "2. It merges the closest pairs of points according to their distances;\n",
    "3. It recomputes the distance between the new cluster and the old ones and stores them in a new distance matrix;\n",
    "4. It repeats steps 2 and 3 until all the clusters are merged into one single cluster. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d6812-7030-4c4e-b787-8b6a5c86c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_dendrogram(scaled, labels=list(scaled.index))\n",
    "fig.update_layout(width=700, height=500, template='plotly_white')\n",
    "\n",
    "# save image as pdf\n",
    "fig.write_image(os.path.join(result_dir, \"Cluster_Dendrogram.pdf\"), scale=3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad3f85-5112-486b-800a-f5618991d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SORT DATA TO CREATE HEATMAP\n",
    "\n",
    "# Compute linkage matrix from distances for hierarchical clustering\n",
    "linkage_data_ft = linkage(scaled, method='complete', metric='euclidean')\n",
    "linkage_data_samples = linkage(scaled.T, method='complete', metric='euclidean')\n",
    "\n",
    "# Create a dictionary of data structures computed to render the dendrogram. \n",
    "# We will use dict['leaves']\n",
    "cluster_samples = dendrogram(linkage_data_ft, no_plot=True)\n",
    "cluster_ft = dendrogram(linkage_data_samples, no_plot=True)\n",
    "\n",
    "# Create dataframe with sorted samples\n",
    "ord_samp = scaled.copy()\n",
    "ord_samp.reset_index(inplace=True)\n",
    "ord_samp = ord_samp.reindex(cluster_samples['leaves'])\n",
    "ord_samp.rename(columns={'index': 'Filename'}, inplace=True)\n",
    "ord_samp.set_index('Filename', inplace=True)\n",
    "\n",
    "# Create dataframe with sorted features\n",
    "ord_ft = ord_samp.T.reset_index()\n",
    "ord_ft = ord_ft.reindex(cluster_ft['leaves'])\n",
    "ord_ft.rename(columns={'index': 'Feature'}, inplace=True)\n",
    "ord_ft.set_index('Feature', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f60dc-7522-4f57-9bb0-b6f8a196cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "fig = px.imshow(ord_ft,y=list(ord_ft.index), x=list(ord_ft.columns), text_auto=True, aspect=\"auto\",\n",
    "               color_continuous_scale='PuOr_r', range_color=[-3,3])\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=700,\n",
    "    height=800)\n",
    "\n",
    "fig.update_yaxes(visible=False)\n",
    "fig.update_xaxes(tickangle = 35)\n",
    "\n",
    "# save image as pdf\n",
    "fig.write_image(os.path.join(result_dir, \"Heatmap.pdf\"), scale=3)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "statistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "572e6a6a0d902086d9fab211e1f6ea25118f94d6a17d1c4885801f9b3fb3838f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
