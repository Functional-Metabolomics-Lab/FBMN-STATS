{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "To easily find the parameters on the jupyter notebook on the header bar select view -> cell toolbar -> tags. all the paramters will have the tag \"parameter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download GNPS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary modules\n",
    "import networkx as nx\n",
    "from gnpsdata import taskresult\n",
    "import os\n",
    "from gnpsdata import workflow_fbmn\n",
    "import pandas as pd\n",
    "from qiime2 import Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "import pingouin as pg\n",
    "import skbio # Don't import on Windows!!\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact\n",
    "from pynmranalysis.normalization import PQN_normalization\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# GNPS task id\n",
    "task = \"cf6e14abf5604f47b28b467a513d3532\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading raw data from GNPS\n",
    "def download_graphml(task, output_file):\n",
    "    taskresult.download_task_resultfile(task, \"gnps_molecular_network_graphml/\", output_file)\n",
    "\n",
    "def get_graphml_network(task):\n",
    "    taskresult.download_task_resultfile(task, \"gnps_molecular_network_graphml/\", \"temp.graphml\")\n",
    "\n",
    "    G = nx.read_graphml(\"temp.graphml\")\n",
    "\n",
    "    return G\n",
    "\n",
    "def download_quantification(task, output_file):\n",
    "    taskresult.download_task_resultfile(task, \"quantification_table/\", output_file)\n",
    "\n",
    "def download_metadata(task, output_file):\n",
    "    taskresult.download_task_resultfile(task, \"metadata_merged/\", output_file)\n",
    "\n",
    "def download_mgf(task, output_file):\n",
    "    taskresult.download_task_resultfile(task, \"spectra_reformatted/\", output_file)\n",
    "    \n",
    "# Qiime2 Data\n",
    "def download_qiime2(task, output_file):\n",
    "    taskresult.download_task_resultfile(task, \"qiime2_output/qiime2_table.qza\", output_file)\n",
    "\n",
    "def download_qiime2_manifest(task, output_file):\n",
    "    taskresult.download_task_resultfile(task, \"qiime2_output/qiime2_manifest.tsv\", output_file)\n",
    "\n",
    "def download_qiime2_metadata(task, output_file):\n",
    "    taskresult.download_task_resultfile(task, \"qiime2_output/qiime2_metadata.tsv\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download quantification and manifest\n",
    "os.makedirs(\"../output_QIIME2_Notebook\", exist_ok=True)\n",
    "download_quantification(task, \"../output_QIIME2_Notebook/quant.csv\")\n",
    "download_qiime2_manifest(task, \"../output_QIIME2_Notebook/manifest.csv\")\n",
    "# Downloading metadata\n",
    "workflow_fbmn.download_metadata(task, \"../output_QIIME2_Notebook/unprocessed_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Metadata and Manifest Column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read metadata file\n",
    "metadata = pd.read_csv(\"../output_QIIME2_Notebook/unprocessed_metadata.tsv\", sep = \"\\t\", index_col=False)\n",
    "#rename 1st column to \"#sample id\n",
    "metadata = metadata.rename(columns={\"filename\":\"sample id\"})\n",
    "#convert back to .tsv\n",
    "metadata.to_csv('../output_QIIME2_Notebook/metadata.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings for cleaner output, comment out for debugging\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blank Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# When cutoff is low, more noise (or background) detected; With higher cutoff, less background detected, thus more features observed\n",
    "cutoff = 0.1\n",
    "\n",
    "condition = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get folder with data files\n",
    "result_dir = \"../output_QIIME2_Notebook/\"\n",
    "#Read quant.csv and metadata .tsv\n",
    "ft = pd.read_csv(\"../output_QIIME2_Notebook/quant.csv\")\n",
    "md = pd.read_csv(\"../output_QIIME2_Notebook/metadata.tsv\", sep = \"\\t\").set_index(\"sample id\")\n",
    "\n",
    "\n",
    "\n",
    "def inside_levels(df):\n",
    "    # get all the columns (equals all attributes) -> will be number of rows\n",
    "    levels = []\n",
    "    types = []\n",
    "    count = []\n",
    "    for col in df.columns:\n",
    "        types.append(type(df[col][0]))\n",
    "        levels.append(sorted(set(df[col].dropna())))\n",
    "        tmp = df[col].value_counts()\n",
    "        count.append([tmp[levels[-1][i]] for i in range(len(levels[-1]))])\n",
    "    return pd.DataFrame({\"ATTRIBUTES\": df.columns, \"LEVELS\": levels, \"COUNT\":count, \"TYPES\": types}, index=range(1, len(levels)+1))\n",
    "new_md = md.copy() #storing the files under different names to preserve the original files\n",
    "# remove the (front & tail) spaces, if any present, from the rownames of md\n",
    "new_md.index = [name.strip() for name in md.index]\n",
    "# for each col in new_md\n",
    "# 1) removing the spaces (if any)\n",
    "# 2) replace the spaces (in the middle) to underscore\n",
    "# 3) converting them all to UPPERCASE\n",
    "for col in new_md.columns:\n",
    "    if new_md[col].dtype == str:\n",
    "        new_md[col] = [item.strip().replace(\" \", \"_\").upper() for item in new_md[col]]\n",
    "\n",
    "new_ft = ft.copy() #storing the files under different names to preserve the original files\n",
    "# changing the index in feature table to contain m/z and RT information\n",
    "new_ft.index = [f\"{id}_{round(mz, 3)}_{round(rt, 3)}\" for id, mz, rt in zip(ft[\"row ID\"], ft[\"row m/z\"], ft[\"row retention time\"])]\n",
    "new_ft.index.name = \"CustomIndex\"\n",
    "# drop all columns that are not mzML or mzXML file names\n",
    "new_ft.drop(columns=[col for col in new_ft.columns if \".mz\" not in col], inplace=True)\n",
    "# remove \" Peak area\" from column names\n",
    "new_ft.rename(columns={col: col.replace(\" Peak area\", \"\").strip() for col in new_ft.columns}, inplace=True)\n",
    "\n",
    "if sorted(new_ft.columns) != sorted(new_md.index):\n",
    "    # print the md rows / ft column which are not in ft columns / md rows and remove them\n",
    "    ft_cols_not_in_md = [col for col in new_ft.columns if col not in new_md.index]\n",
    "    new_ft.drop(columns=ft_cols_not_in_md, inplace=True)\n",
    "    md_rows_not_in_ft = [row for row in new_md.index if row not in new_ft.columns]\n",
    "    new_md.drop(md_rows_not_in_ft, inplace=True)\n",
    "\n",
    "new_ft = new_ft.reindex(sorted(new_ft.columns), axis=1) #ordering the ft by its column names\n",
    "new_md.sort_index(inplace=True) #ordering the md by its row names\n",
    "list(new_ft.columns) == list(new_md.index)\n",
    "data = new_md\n",
    "df = pd.DataFrame({\"LEVELS\": inside_levels(data).iloc[condition-1][\"LEVELS\"]})\n",
    "df.index = [*range(1, len(df)+1)]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Among the shown levels of an attribute, select the one to remove\n",
    "blank_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Splitting the data into blanks and samples based on the metadata\n",
    "md_blank = data[data[inside_levels(data)['ATTRIBUTES'][condition]] == df['LEVELS'][blank_id]]\n",
    "blank = new_ft[list(md_blank.index)]\n",
    "md_samples = data[data[inside_levels(data)['ATTRIBUTES'][condition]] != df['LEVELS'][blank_id]]\n",
    "samples = new_ft[list(md_samples.index)]\n",
    "\n",
    "blank_removal = samples.copy()\n",
    "\n",
    "# Getting mean for every feature in blank and Samples\n",
    "avg_blank = blank.mean(axis=1, skipna=False) # set skipna = False do not exclude NA/null values when computing the result.\n",
    "avg_samples = samples.mean(axis=1, skipna=False)\n",
    "\n",
    "# Getting the ratio of blank vs samples\n",
    "ratio_blank_samples = (avg_blank+1)/(avg_samples+1)\n",
    "\n",
    "# Create an array with boolean values: True (is a real feature, ratio<cutoff) / False (is a blank, background, noise feature, ratio>cutoff)\n",
    "is_real_feature = (ratio_blank_samples<cutoff)\n",
    "blank_removal = samples[is_real_feature.values]\n",
    "imputation_samples = blank_removal.copy()\n",
    "\n",
    "# save to file\n",
    "entry_id = []\n",
    "entry_mz = []\n",
    "entry_time = []\n",
    "for entryCol in blank_removal.index:\n",
    "    entry = entryCol.split(\"_\")\n",
    "    entry_id.append(entry[0])\n",
    "    entry_mz.append(entry[1])\n",
    "    entry_time.append(entry[2])\n",
    "blank_removal.insert(0,\"#OTU ID\",entry_id,True)\n",
    "blank_removal.to_csv(os.path.join(result_dir, \"Blanks_Removed.tsv\"), sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lowest intensity (that is not zero) as a cutoff LOD value\n",
    "cutoff_LOD = round(imputation_samples.replace(0, np.nan).min(numeric_only=True).min())\n",
    "\n",
    "imputation_samples = imputation_samples.apply(lambda x: [np.random.randint(1, cutoff_LOD) if v == 0 else v for v in x])\n",
    "imputed = imputation_samples.copy()\n",
    "\n",
    "entry_id = []\n",
    "entry_mz = []\n",
    "entry_time = []\n",
    "for entryCol in imputed.index:\n",
    "    entry = entryCol.split(\"_\")\n",
    "    entry_id.append(entry[0])\n",
    "    entry_mz.append(entry[1])\n",
    "    entry_time.append(entry[2])\n",
    "imputed.insert(0,\"#OTU ID\",entry_id,True)\n",
    "# save to file\n",
    "imputed.to_csv(os.path.join(result_dir, \"Imputed_QuantTable.tsv\"), sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set normalization_method to 1 for sample centric normalization or 2 for Probabilistic Quotient Normalization\n",
    "normalization_method = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = imputation_samples.copy()\n",
    "# Dividing each element of a particular column with its column sum\n",
    "if normalization_method == 1:\n",
    "    normalized = normalized.apply(lambda x: x/np.sum(x), axis=0)\n",
    "else:\n",
    "    normalized = PQN_normalization(normalized ,ref_norm = \"median\" , verbose=False) \n",
    "normalized_samples = normalized.copy()\n",
    "entry_id = []\n",
    "entry_mz = []\n",
    "entry_time = []\n",
    "for entryCol in normalized_samples.index:\n",
    "    entry = entryCol.split(\"_\")\n",
    "    entry_id.append(entry[0])\n",
    "    entry_mz.append(entry[1])\n",
    "    entry_time.append(entry[2])\n",
    "normalized_samples.insert(0,\"#OTU ID\",entry_id,True)\n",
    "normalized_samples.to_csv(os.path.join(result_dir, \"Normalised_Quant_table.tsv\"), sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposing the imputed table before scaling\n",
    "transposed = imputation_samples.T\n",
    "# put the rows in the feature table and metadata in the same order\n",
    "transposed.sort_index(inplace=True)\n",
    "md_samples.sort_index(inplace=True)\n",
    "\n",
    "if (md_samples.index == transposed.index).all():\n",
    "    pass\n",
    "else:\n",
    "    print(\"WARNING: Sample names in feature and metadata table are NOT the same!\")\n",
    "transposed.to_csv(os.path.join(result_dir, \"Imputed_QuantTable_transposed.csv\"))\n",
    "\n",
    "# scale filtered data\n",
    "scaled = pd.DataFrame(StandardScaler().fit_transform(transposed), index=transposed.index, columns=transposed.columns)\n",
    "scaled = scaled.T\n",
    "entry_id = []\n",
    "entry_mz = []\n",
    "entry_time = []\n",
    "for entryCol in scaled.index:\n",
    "    entry = entryCol.split(\"_\")\n",
    "    entry_id.append(entry[0])\n",
    "    entry_mz.append(entry[1])\n",
    "    entry_time.append(entry[2])\n",
    "scaled.insert(0,\"#OTU ID\",entry_id,True)\n",
    "scaled.to_csv(os.path.join(result_dir, \"Imputed_Scaled_QuantTable.tsv\"), sep = \"\\t\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Into Qiime2\n",
    "## Convert .tsv to .biom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! biom convert \\\n",
    "  -i ../output_QIIME2_Notebook/Normalised_Quant_table.tsv \\\n",
    "  -o ../output_QIIME2_Notebook/quant.biom --to-hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! qiime tools import \\\n",
    "  --input-path ../output_QIIME2_Notebook/quant.biom \\\n",
    "  --type 'FeatureTable[Frequency]' \\\n",
    "  --input-format BIOMV210Format \\\n",
    "  --output-path ../output_QIIME2_Notebook/qiime_table.qza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Metadata and Normalized Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_scaled = scaled.transpose()\n",
    "\n",
    "Data = pd.merge(md_samples, transposed_scaled, left_index=True, right_index=True, how=\"inner\")\n",
    "Data.index.name = 'sample_name'\n",
    "Data.to_csv(os.path.join(result_dir, \"merged_metadata.tsv\"), sep = \"\\t\", index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Longitudinal ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "p_formula = 'ATTRIBUTE_Year~ATTRIBUTE_Sample_Area+ATTRIBUTE_Latitude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! qiime longitudinal anova \\\n",
    "  --m-metadata-file ../output_QIIME2_Notebook/metadata.tsv \\\n",
    "  --p-formula $p_formula \\\n",
    "  --p-sstype 'I' \\\n",
    "  --o-visualization ../output_QIIME2_Notebook/metadata.qzv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.load('../output_QIIME2_Notebook/metadata.qzv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "p_metric = 'canberra_adkins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! qiime diversity beta \\\n",
    "  --i-table ../output_QIIME2_Notebook/qiime_table.qza \\\n",
    "  --p-metric $p_metric \\\n",
    "  --o-distance-matrix ../output_QIIME2_Notebook/distance_matrix.qza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Coordinate Analysis (PCoA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! qiime diversity pcoa \\\n",
    "  --i-distance-matrix ../output_QIIME2_Notebook/distance_matrix.qza \\\n",
    "  --o-pcoa ../output_QIIME2_Notebook/pcoa.qza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emperor plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! qiime emperor plot \\\n",
    "  --i-pcoa ../output_QIIME2_Notebook/pcoa.qza \\\n",
    "  --m-metadata-file ../output_QIIME2_Notebook/metadata.tsv \\\n",
    "  --o-visualization ../output_QIIME2_Notebook/emperor_plot \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.load('../output_QIIME2_Notebook/emperor_plot.qzv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Data/Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "metadata_column = 'ATTRIBUTE_Sample_Area'\n",
    "estimator = 'RandomForestClassifier'\n",
    "n_estimators = 500\n",
    "random_state = 123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! qiime sample-classifier classify-samples \\\n",
    "  --i-table ../output_QIIME2_Notebook/qiime_table.qza \\\n",
    "  --m-metadata-file ../output_QIIME2_Notebook/metadata.tsv \\\n",
    "  --m-metadata-column $metadata_column \\\n",
    "  --p-optimize-feature-selection \\\n",
    "  --p-parameter-tuning \\\n",
    "  --p-estimator $estimator \\\n",
    "  --p-n-estimators $n_estimators \\\n",
    "  --p-random-state $random_state \\\n",
    "  --o-accuracy-results ../output_QIIME2_Notebook/accuracy_results.qzv \\\n",
    "  --o-feature-importance ../output_QIIME2_Notebook/feature_importance.qza \\\n",
    "  --o-heatmap ../output_QIIME2_Notebook/heatmap.qzv \\\n",
    "  --o-model-summary ../output_QIIME2_Notebook/model_summary.qzv \\\n",
    "  --o-predictions ../output_QIIME2_Notebook/predictions.qza \\\n",
    "  --o-probabilities ../output_QIIME2_Notebook/probabilities.qza \\\n",
    "  --o-sample-estimator ../output_QIIME2_Notebook/sample_estimator.qza \\\n",
    "  --o-test-targets ../output_QIIME2_Notebook/test_targets.qza \\\n",
    "  --o-training-targets ../output_QIIME2_Notebook/training_targets.qza \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.load('../output_QIIME2_Notebook/heatmap.qzv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PermANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "metadata_column_permanova = 'ATTRIBUTE_Sample_Area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! qiime diversity beta-group-significance \\\n",
    "  --i-distance-matrix ../output_QIIME2_Notebook/distance_matrix.qza \\\n",
    "  --m-metadata-file ../output_QIIME2_Notebook/metadata.tsv \\\n",
    "  --m-metadata-column $metadata_column_permanova \\\n",
    "  --o-visualization ../output_QIIME2_Notebook/permanova.qzv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualization.load('../output_QIIME2_Notebook/permanova.qzv')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
